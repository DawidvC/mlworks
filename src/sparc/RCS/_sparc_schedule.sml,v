head	1.42;
access;
symbols
	MLW_daveb_inline_1_4_99:1.42.3
	MLWorks_21c0_1999_03_25:1.42
	MLWorks_20c1_1998_08_20:1.42
	MLWorks_20c0_1998_08_04:1.42
	MLWorks_20b2c2_1998_06_19:1.42
	MLWorks_20b2_Windows_1998_06_12:1.42
	MLWorks_20b1c1_1998_05_07:1.42
	MLWorks_20b0_1998_04_07:1.42
	MLWorks_20b0_1998_03_20:1.42
	MLWorks_20m2_1998_02_16:1.42
	MLWorks_20m1_1997_10_23:1.42
	MLWorks_11r1:1.40.1.1.1.1.1
	MLWorks_workspace_97:1.42.2
	MLWorks_dt_wizard:1.42.1
	MLWorks_11c0_1997_09_09:1.40.1.1.1.1
	MLWorks_10r3:1.40.1.1.3
	MLWorks_10r2_551:1.40.1.1.2
	MLWorks_11:1.40.1.1.1
	MLWorks_1_0_r2c2_1997_07_28:1.40.1.1
	MLWorks_20m0_1997_06_20:1.42
	MLWorks_1_0_r2c2_1997_06_14:1.40.1.1
	MLWorks_1_0_r2c1_released_1997_05_23:1.40.1.1
	MLWorks_1_0_r2c1_1997_05_12:1.40.1
	MLWorks_BugFix_1997_04_24:1.41
	MLWorks_1_0_r2_Win32_1997_04_11:1.40
	MLWorks_1_0_r2_Unix_1997_04_04:1.40
	MLWorks_1_0_1_ULTRASPARC_1997_02_24:1.39.6.1.1
	MLWorks_gui_1996_12_18:1.39.7
	MLWorks_1_0_Win32_1996_12_17:1.39.6
	MLWorks_1_0_Irix_1996_11_28_released_1996_12_17:1.39.4.1.1.1
	MLWorks_1_0_Unix_1996_11_14_released_1996_12_17:1.39.4.1
	MLWorks_1_0_Irix_1996_11_28:1.39.4.1.1
	MLWorks_1_0_Win32_1996_11_22:1.39.5
	MLWorks_1_0_Unix_1996_11_14:1.39.4
	MLWorks_Open_Beta2_1996_10_11:1.39.3
	MLWorks_License_dev:1.39.2
	MLWorks_1_open_beta_1996_09_13:1.39.1
	MLWorks_Open_Beta_1996_08_22:1.39
	MLWorks_Beta_1996_07_02:1.39
	MLWorks_Beta_1996_06_07:1.39
	MLWorks_Beta_1996_06_06:1.39
	MLWorks_Beta_1996_06_05:1.39
	MLWorks_Beta_1996_06_03:1.39
	MLWorks_Beta_1996_05_31:1.39
	MLWorks_Beta_1996_05_30:1.39
	ML_beta_release_12/08/94:1.35
	ML_beta_release_03/08/94:1.35
	ML_revised_beta_release_25/05/94:1.34
	ML_final_beta_release_02/03/94:1.33
	mlworks-28-01-1994:1.33
	Release:1.33
	mlworks-beta-01-09-1993:1.33
	MLWorks-1-0-4-29/01/1993:1.30
	MLWorks-1-0-3-21/12/1992:1.28
	MLWorks-1-0-2-15/12/1992:1.28
	MLWorks-1-0-1-04/12/1992:1.27
	checkpoint_17_08_92:1.21;
locks; strict;
comment	@ * @;


1.42
date	97.05.01.13.18.35;	author jont;	state Exp;
branches
	1.42.1.1
	1.42.2.1
	1.42.3.1;
next	1.41;

1.41
date	97.04.21.15.49.28;	author jont;	state Exp;
branches;
next	1.40;

1.40
date	97.02.04.17.29.53;	author matthew;	state Exp;
branches
	1.40.1.1;
next	1.39;

1.39
date	95.12.22.16.31.24;	author jont;	state Exp;
branches
	1.39.1.1
	1.39.2.1
	1.39.3.1
	1.39.4.1
	1.39.5.1
	1.39.6.1
	1.39.7.1;
next	1.38;

1.38
date	95.12.22.13.04.35;	author jont;	state Exp;
branches;
next	1.37;

1.37
date	95.07.28.09.22.27;	author matthew;	state Exp;
branches;
next	1.36;

1.36
date	94.10.13.11.22.48;	author matthew;	state Exp;
branches;
next	1.35;

1.35
date	94.07.22.16.13.48;	author nickh;	state Exp;
branches;
next	1.34;

1.34
date	94.03.09.15.54.40;	author jont;	state Exp;
branches;
next	1.33;

1.33
date	93.07.29.15.30.08;	author nosa;	state Exp;
branches
	1.33.1.1;
next	1.32;

1.32
date	93.05.18.18.58.49;	author jont;	state Exp;
branches;
next	1.31;

1.31
date	93.02.25.15.03.02;	author jont;	state Exp;
branches;
next	1.30;

1.30
date	93.01.27.14.34.43;	author jont;	state Exp;
branches;
next	1.29;

1.29
date	93.01.20.17.53.35;	author jont;	state Exp;
branches;
next	1.28;

1.28
date	92.12.08.11.18.37;	author clive;	state Exp;
branches;
next	1.27;

1.27
date	92.12.03.11.28.37;	author clive;	state Exp;
branches;
next	1.26;

1.26
date	92.12.01.20.51.33;	author jont;	state Exp;
branches;
next	1.25;

1.25
date	92.10.30.12.34.53;	author jont;	state Exp;
branches;
next	1.24;

1.24
date	92.10.05.10.09.28;	author clive;	state Exp;
branches;
next	1.23;

1.23
date	92.09.16.10.17.56;	author clive;	state Exp;
branches;
next	1.22;

1.22
date	92.08.26.16.14.27;	author jont;	state Exp;
branches;
next	1.21;

1.21
date	92.08.10.10.35.15;	author davidt;	state Exp;
branches;
next	1.20;

1.20
date	92.07.30.19.05.09;	author jont;	state Exp;
branches;
next	1.19;

1.19
date	92.05.06.18.23.00;	author jont;	state Exp;
branches;
next	1.18;

1.18
date	92.03.05.11.47.43;	author jont;	state Exp;
branches;
next	1.17;

1.17
date	92.02.18.19.26.00;	author jont;	state Exp;
branches;
next	1.16;

1.16
date	92.02.17.17.32.51;	author richard;	state Exp;
branches;
next	1.15;

1.15
date	92.01.23.16.55.09;	author clive;	state Exp;
branches;
next	1.14;

1.14
date	91.12.05.13.02.23;	author jont;	state Exp;
branches;
next	1.13;

1.13
date	91.12.04.19.33.34;	author jont;	state Exp;
branches;
next	1.12;

1.12
date	91.12.02.20.23.43;	author jont;	state Exp;
branches;
next	1.11;

1.11
date	91.11.27.18.53.03;	author jont;	state Exp;
branches;
next	1.10;

1.10
date	91.11.25.19.00.47;	author jont;	state Exp;
branches;
next	1.9;

1.9
date	91.11.20.16.59.27;	author jont;	state Exp;
branches;
next	1.8;

1.8
date	91.11.18.17.45.26;	author jont;	state Exp;
branches;
next	1.7;

1.7
date	91.11.08.16.09.39;	author jont;	state Exp;
branches;
next	1.6;

1.6
date	91.10.29.12.44.25;	author jont;	state Exp;
branches;
next	1.5;

1.5
date	91.10.28.15.10.00;	author jont;	state Exp;
branches;
next	1.4;

1.4
date	91.10.25.15.56.52;	author jont;	state Exp;
branches;
next	1.3;

1.3
date	91.10.24.14.39.06;	author jont;	state Exp;
branches;
next	1.2;

1.2
date	91.10.23.17.45.59;	author jont;	state Exp;
branches;
next	1.1;

1.1
date	91.10.22.18.10.42;	author jont;	state Exp;
branches;
next	;

1.33.1.1
date	93.07.29.15.30.08;	author jont;	state Exp;
branches;
next	;

1.39.1.1
date	96.09.13.11.26.38;	author hope;	state Exp;
branches;
next	;

1.39.2.1
date	96.10.07.16.17.09;	author hope;	state Exp;
branches;
next	;

1.39.3.1
date	96.10.17.11.36.53;	author hope;	state Exp;
branches;
next	;

1.39.4.1
date	96.11.14.13.04.45;	author hope;	state Exp;
branches
	1.39.4.1.1.1;
next	;

1.39.4.1.1.1
date	96.11.28.15.14.12;	author hope;	state Exp;
branches;
next	;

1.39.5.1
date	96.11.22.18.22.13;	author hope;	state Exp;
branches;
next	;

1.39.6.1
date	96.12.17.18.00.13;	author hope;	state Exp;
branches
	1.39.6.1.1.1;
next	;

1.39.6.1.1.1
date	97.02.24.11.51.52;	author hope;	state Exp;
branches;
next	;

1.39.7.1
date	96.12.18.09.55.34;	author hope;	state Exp;
branches;
next	;

1.40.1.1
date	97.05.12.10.50.15;	author hope;	state Exp;
branches
	1.40.1.1.1.1
	1.40.1.1.2.1
	1.40.1.1.3.1;
next	;

1.40.1.1.1.1
date	97.07.28.18.32.28;	author daveb;	state Exp;
branches
	1.40.1.1.1.1.1.1;
next	;

1.40.1.1.1.1.1.1
date	97.10.07.11.58.36;	author jkbrook;	state Exp;
branches;
next	;

1.40.1.1.2.1
date	97.09.08.17.25.44;	author daveb;	state Exp;
branches;
next	;

1.40.1.1.3.1
date	97.09.09.14.22.22;	author daveb;	state Exp;
branches;
next	;

1.42.1.1
date	97.09.10.19.41.27;	author brucem;	state Exp;
branches;
next	;

1.42.2.1
date	97.09.11.21.09.01;	author daveb;	state Exp;
branches;
next	;

1.42.3.1
date	99.04.01.18.07.50;	author daveb;	state Exp;
branches;
next	;


desc
@Instruction scheduling into delay slots
@


1.42
log
@[Bug #30088]
Get rid of MLWorks.Option
@
text
@(* _sparc_schedule.sml the functor *)
(*
$Log: _sparc_schedule.sml,v $
 * Revision 1.41  1997/04/21  15:49:28  jont
 * [Bug #20001]
 * Remove debugging comments when filling load delays
 *
 * Revision 1.40  1997/02/04  17:29:53  matthew
 * Adding treatment of load delays
 *
 * Revision 1.39  1995/12/22  16:31:24  jont
 * Remove references to utils/option
 *
Revision 1.38  1995/12/22  13:04:35  jont
Add extra field to procedure_parameters to contain old (pre register allocation)
spill sizes. This is for the i386, where spill assignment is done in the backend

Revision 1.37  1995/07/28  09:22:27  matthew
Putting sources registers for various instructions in correct order

Revision 1.36  1994/10/13  11:22:48  matthew
Use pervasive Option.option for return values in NewMap

Revision 1.35  1994/07/22  16:13:48  nickh
Prevent rescheduling of allocation sequence.
(Also add comments and abstract common code to a function).

Revision 1.34  1994/03/09  15:54:40  jont
Adding load offset instructions

Revision 1.33  1993/07/29  15:30:08  nosa
structure Option.

Revision 1.32  1993/05/18  18:58:49  jont
Removed integer parameter

Revision 1.31  1993/02/25  15:03:02  jont
Modified to use new variable size hashsets

Revision 1.30  1993/01/27  14:34:43  jont
Improved scheduling of short blocks. Improved procedure level scheduling
by removing chained branchs first as well as after

Revision 1.29  1993/01/20  17:53:35  jont
Made selection of 3 instruction blocks more selective, to allow
scheduling of short blocks which tail or call but not those which branch

Revision 1.28  1992/12/08  11:18:37  clive
Changed the type of nop used for tracing to store it being moved by the scheduler

Revision 1.27  1992/12/03  11:28:37  clive
Changed reschedule block to ignore the small blocks (really to
get the new stack overflow check optimised)

Revision 1.26  1992/12/01  20:51:33  jont
Modified to use new improved hashset signature

Revision 1.25  1992/10/30  12:34:53  jont
Modified to use MirTypes.Map for the tag lookups

Revision 1.24  1992/10/05  10:09:28  clive
Change to NewMap.empty which now takes < and = functions instead of the single-function

Revision 1.23  1992/09/16  10:17:56  clive
Removed some handles of hashtable lookup exceptions

Revision 1.22  1992/08/26  16:14:27  jont
Removed some redundant structures and sharing

Revision 1.21  1992/08/10  10:35:15  davidt
Now uses NewMap instead of BalancedTree structure.
Array is now pervasive.

Revision 1.20  1992/07/30  19:05:09  jont
Fixed problem with delay opcodes disappearing from between fcmp
and fb

Revision 1.19  1992/05/06  18:23:00  jont
Modified the inter block rescheduler to use hash sets and
balanced trees instead of sets and association lists

Revision 1.18  1992/03/05  11:47:43  jont
Reded intrablock shceduling to be much more efficient

Revision 1.17  1992/02/18  19:26:00  jont
Improved speed of rescheduling by about a factor of ten by
careful use of set primitives

Revision 1.16  1992/02/17  17:32:51  richard
Some efficiency changes by jont re-checked because of accidental
deletion.

Revision 1.16  1992/02/17  16:39:29  richard
Some attempts at improving efficiency by jont.  This revision
was accidentally deleted so I rechecked it.

Revision 1.18  1992/02/14  19:31:47  jont
Some attempts at improving rescheduling efficiency

Copyright (c) 1991 Harlequin Ltd.
*)

require "../utils/_hashset";
require "../utils/crash";
require "../utils/lists";
require "sparc_assembly";
require "sparc_schedule";

functor Sparc_Schedule(
  structure Crash : CRASH
  structure Lists : LISTS
  structure Sparc_Assembly : SPARC_ASSEMBLY
  ) : SPARC_SCHEDULE =
struct
  structure Sparc_Assembly = Sparc_Assembly
  structure MirTypes = Sparc_Assembly.MirTypes
  structure Set = MirTypes.Set
  structure MachTypes = Sparc_Assembly.Sparc_Opcodes.MachTypes
  structure NewMap = MirTypes.Map
  structure HashSet = HashSet(
    structure Crash = Crash
    structure Lists = Lists
    type element = MirTypes.tag
    val eq = MirTypes.equal_tag
    val hash = MirTypes.int_of_tag
      )


  (* limit on the number of instructions to look at for a slot filler *)
  val schedule_limit = 20

  val hashset_size = 32

  fun refs_for_opcode_list(done, []) = done
  | refs_for_opcode_list(done, (_, SOME tag, _) :: rest) =
    refs_for_opcode_list(HashSet.add_member(done, tag), rest)
  | refs_for_opcode_list(done, _ :: rest) =  refs_for_opcode_list(done, rest)

  fun refs_for_block (done, (tag, opcode_list)) =
    refs_for_opcode_list(done, opcode_list)

  fun refs_all (tag, block_list) =
    Lists.reducel
    refs_for_block
    (HashSet.add_member(HashSet.empty_set hashset_size, tag), block_list)

  val nop_code = Sparc_Assembly.nop_code
  val nop = (nop_code, NONE, "")

  fun passes_from_sets((first_i_defines, first_i_uses, first_r_defines, first_r_uses),
		       (second_i_defines, second_i_uses, second_r_defines, second_r_uses)) =
    (* Return true if the first instruction can safely pass the second *)
    not (Set.intersect (first_i_defines, second_i_defines)) andalso
    not (Set.intersect (first_i_defines, second_i_uses)) andalso
    not (Set.intersect (first_i_uses, second_i_defines)) andalso
    not (Set.intersect (first_r_defines, second_r_defines)) andalso
    not (Set.intersect (first_r_defines, second_r_uses)) andalso
    not (Set.intersect (first_r_uses, second_r_defines))

  fun convert_branch_annul(Sparc_Assembly.BRANCH_ANNUL
			   (branch, disp), tag_opt, comment) =
    (Sparc_Assembly.BRANCH(branch, disp), tag_opt, comment)
    | convert_branch_annul(Sparc_Assembly.FBRANCH_ANNUL
			   (branch, disp), tag_opt, comment) =
      (Sparc_Assembly.FBRANCH(branch, disp), tag_opt, comment)
    | convert_branch_annul opcode = opcode

  fun has_delay(Sparc_Assembly.BRANCH _) = true
    | has_delay(Sparc_Assembly.BRANCH_ANNUL _) = true
    | has_delay(Sparc_Assembly.FBRANCH _) = true
    | has_delay(Sparc_Assembly.FBRANCH_ANNUL _) = true
    | has_delay(Sparc_Assembly.Call _) = true
    | has_delay(Sparc_Assembly.JUMP_AND_LINK _) = true
    | has_delay _ = false


(* remove_doubled_instrs turns this sequence:
		bcc,a	dest
		instr		\these two instrs
		instr		/are identical
   into this:
		bcc	dest
		instr
*)
  
  fun remove_doubled_instrs(tag, opcode_list) =
    let
      fun remove(done, []) = rev done
      | remove(done, [x]) = rev(x :: done)
      | remove(done, [x, y]) = rev(y :: x :: done)
      | remove(done,
	       (opc as (Sparc_Assembly.BRANCH_ANNUL(opcode, i),
			SOME tag, comment)) ::
	       (other as
		(op1 as (opc1, opt1, _)) ::
		(op2 as (opc2, opt2, _)) :: rest)) =
	(case opcode of
	   Sparc_Assembly.BA =>
             (* Got rid of the error message as it was given for computed gotos -
                we really ought to check that it is a computed goto at this point *)
	      remove(opc :: done, other)
	 | _ =>
	     if opc1 = opc2 andalso opt1 = opt2 then
	       remove(op1 :: (Sparc_Assembly.BRANCH(opcode, i),
			      SOME tag, comment) :: done, rest)
	     else
	       remove(op1 :: opc :: done, op2 :: rest))
	| remove(done, x :: rest) = remove(x :: done, rest)
	  
    in
      (tag, remove([], opcode_list))
    end

  fun get_tag_and_instr(tag, opcode :: _) = (tag, opcode)
    | get_tag_and_instr(_, _) = Crash.impossible"Empty block"


(* find_baa_block returns true on a block which is just a ba,a *)
(* instruction (and an optional nop *)

  fun find_baa_block(_, []) = Crash.impossible"Empty block"
    | find_baa_block(_, (Sparc_Assembly.BRANCH_ANNUL(Sparc_Assembly.BA, _),
			 _, _) :: rest) =
      (case rest of
	 [] => true
       | [(Sparc_Assembly.ARITHMETIC_AND_LOGICAL(Sparc_Assembly.AND,
						 MachTypes.G0,
						 MachTypes.G0,
						 Sparc_Assembly.IMM 0),
	   _, _)] => true
       | _ => Crash.impossible"Unreachable instructions after BAA")
    | find_baa_block _ = false

  (* 'get_transitive_dest table ([],tag)' uses the table to look for the
   ultimate destination of the branch at tag. *)

  fun get_transitive_dest baa_tags_and_dests =
    let
      fun g_t_d(visited, tag) =
	(case NewMap.tryApply'(baa_tags_and_dests, tag) of
	   SOME dest_tag =>
	     (case NewMap.tryApply'(baa_tags_and_dests, dest_tag) of
		SOME _ =>
		  if not(Lists.member(tag, visited)) then
		    g_t_d(tag :: visited, dest_tag)
		  else dest_tag
	      | NONE => dest_tag)
	 | NONE => Crash.impossible"get_transitive_dest failure")
    in
      g_t_d
    end

  (* replace_chained_branch table replaces branches to ba,a branches
     with branches to the destination *)

  fun replace_chained_branch baa_tags_and_dests =
    fn (opcode as (Sparc_Assembly.BRANCH(branch, _),
		   SOME tag, comment)) =>
    (case NewMap.tryApply'(baa_tags_and_dests, tag) of
       SOME dest_tag => 
	 (Sparc_Assembly.BRANCH(branch, 0), SOME dest_tag, comment)
     | NONE => opcode)
    | (opcode as (Sparc_Assembly.BRANCH_ANNUL(branch,_),
		  SOME tag, comment)) =>
      (case NewMap.tryApply'(baa_tags_and_dests, tag) of
	 SOME dest_tag =>
	   (Sparc_Assembly.BRANCH_ANNUL(branch, 0), SOME dest_tag, comment)
       | NONE => opcode)
    |  opcode => opcode


  fun replace_chained_branches block_list =
    let
      (* baa_blocks is the set of blocks which are simply ba,a branches *)

      val baa_blocks = Lists.filterp find_baa_block block_list

      (* baa_tags_and_dests is a map from tags of baa_blocks to the
         destinations of their branches *)

      val baa_tags_and_dests =
	Lists.reducel
	(fn (tree,
	     (tag,
	      (Sparc_Assembly.BRANCH_ANNUL(Sparc_Assembly.BA, _),
	       SOME dest_tag, _) :: _)) =>
	 NewMap.define(tree, tag, dest_tag)
	     | _ => Crash.impossible"Bad baa block")
	(NewMap.empty, baa_blocks)

      (* "get_transitive_dest' ([],tag)" finds the tag of the ultimate
       destination of a chain of baa blocks starting at "tag" *)

      val get_transitive_dest' = get_transitive_dest baa_tags_and_dests

      (* baa_tags_and_dests is a map from tags of baa blocks to their
         ultimate destinations *)

      val baa_tags_and_dests =
	Lists.reducel
	(fn (tree, (tag, _)) =>
	 NewMap.define(tree, tag, get_transitive_dest'([], tag)))
	(NewMap.empty, baa_blocks)

      (* replace_chained_branch' op replaces any branch to a baa block
         with a branch to the ultimate destination. *)

      val replace_chained_branch' = replace_chained_branch baa_tags_and_dests
    in
      (* return the block list with branches fixed as above *)
      map
      (fn (tag, opcode_list) =>
       (tag, map replace_chained_branch' opcode_list))
      block_list
    end

(* has_delay' x returns true if the instruction cannot be moved
(i.e. if it has a delay slot or is one of a couple of special
instructions) *)

  fun has_delay'(Sparc_Assembly.LOAD_OFFSET _) = true
    | has_delay'(Sparc_Assembly.TAGGED_ARITHMETIC (_, MachTypes.G1,_,_)) = true
    | has_delay' x = has_delay x

  fun is_fcmp op1 =
    case op1 of
      Sparc_Assembly.FUNARY (funary,_,_) =>
        (case funary of
           Sparc_Assembly.FCMPS => true
         | Sparc_Assembly.FCMPD => true
         | Sparc_Assembly.FCMPX => true
         | _ => false)
    | _ => false

  fun is_fbranch op1 =
    case op1 of
      Sparc_Assembly.FBRANCH _ => true
    | Sparc_Assembly.FBRANCH_ANNUL _ => true
    | _ => true

  (* This function indicates if we should attempt to separate two instructions *)
  (* The SPARC architecture manual suggests separating loads and instructions *)
  (* use the the result of the load, and any two store instructions *)
  (* Return true if we should attempt to separate the two instructions *)
  fun try_separate (op1,op2) =
    (* op2 follows op1 *)
    not (has_delay op1) andalso 
    let
      fun is_store_op (Sparc_Assembly.LOAD_AND_STORE(ls,_,_,_)) =
        (case ls of
           Sparc_Assembly.STB => true
         | Sparc_Assembly.STH => true
         | Sparc_Assembly.ST => true
         | Sparc_Assembly.STD => true
         | _ => false)
        | is_store_op (Sparc_Assembly.LOAD_AND_STORE_FLOAT (ls,_,_,_)) =
           (case ls of
              Sparc_Assembly.STF => true
            | Sparc_Assembly.STDF => true
            | _ => false)
        | is_store_op _ = false
      datatype reg = GC of MachTypes.Sparc_Reg | FLOAT of MachTypes.Sparc_Reg | NOREG
      val (load_reg,is_store) =
        case op1 of
          Sparc_Assembly.LOAD_AND_STORE (ls,reg,_,_) =>
            if is_store_op op1 then (NOREG,true)
            else (GC reg,false)
        | Sparc_Assembly.LOAD_AND_STORE_FLOAT (ls,reg,_,_) =>
            if is_store_op op1 then (NOREG,true)
            else (FLOAT reg,false)
        | _ => (NOREG,false)
    in
      case load_reg of
        NOREG =>
          (is_store andalso is_store_op op2) orelse
          (is_fcmp op1 andalso is_fbranch op2)
      | GC r => 
          let
            val (i_def, i_use, r_def, r_use) =
              Sparc_Assembly.defines_and_uses op2
          in
            Set.is_member (r,i_use)
          end
      | FLOAT r => 
          let
            val (i_def, i_use, r_def, r_use) =
              Sparc_Assembly.defines_and_uses op2
          in
            Set.is_member (r,r_use)
          end
    end

  fun reschedule_proc(tag, block_list) =
    let
      (* Attempt to move instructions from the starts of blocks into the delay slots *)
      (* of instructions referencing them from other blocks *)
      (* Mustn't move LOAD_OFFSET instructions *)
      (* which may expand into more than one opcode *)

      val block_list = replace_chained_branches block_list

      (* all_tags_and_instrs is a list of the tags and leading
         instructions of basic blocks *)

      val all_tags_and_instrs =	map get_tag_and_instr block_list

      (* ok_tags_and_instrs is a list of tags and leading instructions
       of basic blocks for which those leading instructions can be
       moved into delay slots *)

      val ok_tags_and_instrs =
	Lists.filterp
	(fn (_, (x, _, _)) => not(has_delay' x))
	all_tags_and_instrs

      (* "get_ref_tags_for_block (hs,instr_list)" looks through
       instr_list for annulled branches with nop in the delay slot,
       and adds their destinations to the hashset hs *)
         	
      fun get_ref_tags_for_block(done, []) = done
	| get_ref_tags_for_block(done,
				 (Sparc_Assembly.BRANCH_ANNUL(branch, _),
				  SOME tag, _) :: rest) =
	  (case (branch, rest) of
	     (Sparc_Assembly.BA, _) =>
	       get_ref_tags_for_block(done, rest)
	     | (_, (Sparc_Assembly.ARITHMETIC_AND_LOGICAL
		    (Sparc_Assembly.AND, MachTypes.G0, MachTypes.G0, Sparc_Assembly.IMM 0), _, _) :: _) =>
	       get_ref_tags_for_block(HashSet.add_member(done, tag), rest)
		   | _ => get_ref_tags_for_block(done, rest))
	| get_ref_tags_for_block(done,
				 (Sparc_Assembly.FBRANCH_ANNUL(branch, _),
				  SOME tag, _) :: rest) =
	  (case (branch, rest) of
	     (Sparc_Assembly.FBA, _) =>
	       get_ref_tags_for_block(done, rest)
	     | (_, (Sparc_Assembly.ARITHMETIC_AND_LOGICAL
		    (Sparc_Assembly.AND, MachTypes.G0, MachTypes.G0, Sparc_Assembly.IMM 0), _, _) :: _) =>
	       get_ref_tags_for_block(HashSet.add_member(done, tag), rest)
		   | _ => get_ref_tags_for_block(done, rest))
	| get_ref_tags_for_block(done, _ :: rest) =
	  get_ref_tags_for_block(done, rest)

      (* ref_tags is a hashset of all destinations of annulled
         branches with nop in the delay slot *)

      val ref_tags =
	Lists.reducel
	(fn (tags, (tag, x)) => get_ref_tags_for_block(tags, x))
	(HashSet.empty_set hashset_size, block_list)


      (* ref_tags_etc is a map, mapping tag to (instruction,newtag),
         where tag was the tag of a basic block, of which instruction
         was the first instruction, and it was relocatable, and it was
         the destination of an annulled branch with nop in the delay
         slot. The idea is that we'll stick that leading instruction
         in the delay slot *)
	
      fun filter_fun(tag, _) = HashSet.is_member(ref_tags, tag)

      val ref_tags_etc =
	Lists.reducel
	(fn (tree, (tag, instr)) =>
	 NewMap.define(tree, tag, (instr, MirTypes.new_tag())))
	(NewMap.empty,
	 Lists.filterp filter_fun ok_tags_and_instrs)

      (* new_block_list is a block list which has split each of the
       blocks we've identified as able to lose their leading
       instruction (see ref_tags_etc, above) into two blocks: one with
       the leading instruction and original tag, the other with the
       rest and the new tag. *)

      fun split_blocks(acc, []) = rev acc
      | split_blocks(_, (_, []) :: _) = Crash.impossible"Empty block"
      | split_blocks(acc, (block as (tag, opcode :: rest)) :: others) =
        (case NewMap.tryApply'(ref_tags_etc, tag) of
           SOME (instr, dest_tag) =>
	     split_blocks
             ((tag,
	       [opcode,
		(Sparc_Assembly.BRANCH_ANNUL(Sparc_Assembly.BA, 0),
		 SOME dest_tag,
		 "Split off leading instruction")]) ::
	      (dest_tag, rest) ::
	      acc,
	      others)
	 | NONE =>
             split_blocks(block :: acc,  others))

      val new_block_list = split_blocks([], block_list)

      (* do_replace ([],instr_list) fills any nop delay slots of
       annulled branches in instr_list that it can, using the
       information gathered above. *)

      fun do_replace(done, []) = rev done
	| do_replace(done, [opcode]) = rev(opcode :: done)
	| do_replace(done, op1 :: op2 :: rest) =
	  (case (op1, op2) of
	     ((Sparc_Assembly.BRANCH_ANNUL(branch, _), SOME tag,
	       bcomment),
	      (Sparc_Assembly.ARITHMETIC_AND_LOGICAL
	       (Sparc_Assembly.AND, MachTypes.G0, MachTypes.G0, Sparc_Assembly.IMM 0), _, _)) =>
	     (case NewMap.tryApply'(ref_tags_etc, tag) of
		SOME ((opcode, tag_opt, comment), new_tag) => 
		  if branch = Sparc_Assembly.BA then
		    do_replace(op2 :: op1 :: done, rest)
		  else
		    do_replace((opcode, tag_opt, comment ^
				" moved to annulled delay slot") ::
			       (Sparc_Assembly.BRANCH_ANNUL(branch, 0),
				SOME new_tag, bcomment) :: done,
			       rest)
	      | NONE =>
		  do_replace(op2 :: op1 :: done, rest))
	   | ((Sparc_Assembly.FBRANCH_ANNUL(branch, _), SOME tag,
	       bcomment),
	      (Sparc_Assembly.ARITHMETIC_AND_LOGICAL
	       (Sparc_Assembly.AND, MachTypes.G0, MachTypes.G0, Sparc_Assembly.IMM 0), _, _)) =>
	     (case NewMap.tryApply'(ref_tags_etc, tag) of
		SOME ((opcode, tag_opt, comment), new_tag) =>
		  if branch = Sparc_Assembly.FBA then
		    do_replace(op2 :: op1 :: done, rest)
		  else
		    do_replace((opcode, tag_opt, comment ^
				" moved to annulled delay slot") ::
			       (Sparc_Assembly.FBRANCH_ANNUL(branch, 0),
				SOME new_tag, bcomment) :: done,
			       rest)
	      | NONE =>
		  do_replace(op2 :: op1 :: done, rest))
	   | _ => do_replace(op1 :: done, op2 :: rest))

      (* new_new_block_list is the block list after filling delay slots *)

      fun replace_delay_slots(tag, opcode_list) =
	(tag, do_replace([], opcode_list))

      val new_new_block_list = map replace_delay_slots new_block_list
      val next_block_list = replace_chained_branches new_new_block_list
      (* remove any blocks which are not referred to by other blocks *)
      val new_ref_tags = refs_all(tag, next_block_list)
      fun use_block(tag, _) = HashSet.is_member(new_ref_tags, tag)
      val final_block_list = Lists.filterp use_block next_block_list
      (* Finally, remove doubled instructions *)
      val result as (_, blocks) = (tag, map remove_doubled_instrs final_block_list)
    in
	result
    end

(*
  val reschedule_proc = fn x => reschedule_proc(reschedule_proc x)
*)

  fun combine((i_d1, i_u1, r_d1, r_u1), (i_d2, i_u2, r_d2, r_u2)) =
    (Set.union(i_d1, i_d2), Set.union(i_u1, i_u2), Set.union(r_d1, r_d2),
     Set.union(r_u1, r_u2))

  (* This looks for "special" instructions that will terminate the search for a slot filler *)
  fun immoveable opcode =
    opcode = Sparc_Assembly.other_nop_code orelse
    (case opcode of
       Sparc_Assembly.LOAD_OFFSET _ => true
     (* don't relocate over a load_offset *)
     | Sparc_Assembly.TAGGED_ARITHMETIC (_, MachTypes.G1,_,_) => true
     | Sparc_Assembly.ARITHMETIC_AND_LOGICAL (_, MachTypes.G2,_,_) => true
     (* don't relocate over an allocation *)
     | Sparc_Assembly.FUNARY _ => true
     (* don't relocate over an FMCP -- in particular, don't move the nop *)
     | _ => false)

  (* pos is the number of instructions passed over *)
  fun search_for_moveable_instr(pos, _, next, []) = (pos, false, nop)
    | search_for_moveable_instr(pos, current_defs, next,
				(opc as (opcode, _, _)) :: rest) =
      if pos > schedule_limit orelse immoveable opcode
	then (pos,false,nop)
      else
	let
          (* check opc2 doesn't stall with the next instruction *)
          fun check_next opc2 =
            (case next of
               NONE => true
             | SOME opc1 => not (try_separate (opc1,opc2)))
	  val is_delay = case rest of
	    [] => false
	  | (opc', _, _) :: _ => has_delay opc'
	in
	  if is_delay
            then (pos, false, nop)
	  else
	    let
	      val (i_def, i_use, r_def, r_use) =
		Sparc_Assembly.defines_and_uses opcode
	      val new_defs =
		(Set.setdiff(i_def, Set.singleton MachTypes.G0), 
		 Set.setdiff(i_use, Set.singleton MachTypes.G0),
		 r_def, r_use)
	    in
              if passes_from_sets(new_defs, current_defs) 
                andalso check_next opcode then
                (pos, true, opc)
              else
                search_for_moveable_instr(pos+1, combine(new_defs, current_defs),
                                          next, rest)
	    end
	end

  fun copy_over (0, done, rest) = (done,rest)
    | copy_over (n, done, []) = Crash.impossible "copy_over: out of code"
    | copy_over (n, done, x::rest) = copy_over(n-1, x :: done, rest)

  fun drop [] = Crash.impossible "drop: out of code"
    | drop (a::b) = b

  fun traverse(done, static, []) = done
    | traverse(done, static, [x]) = x :: done
    | traverse(done, 
               static,
               (x as (opc1, _, _)) ::
	       (rest as ((y as (opc2, tag2, com2)) :: rest_of_block))) =
      if opc1 = nop_code andalso has_delay opc2 then
	(* The difficult case, we've found a delay slot we would like to fill *)
	let
	  val (pos, found, z) =
	    search_for_moveable_instr(0, Sparc_Assembly.defines_and_uses opc2,
                                      NONE,rest_of_block)
	in
	  if found then
	    (* Got a moveable instr, so move it in *)
	    let
	      val (done, rest) =
		copy_over (pos, convert_branch_annul y :: z :: done,
                                    rest_of_block)
	    in
              traverse(done, static, drop rest)
	    end
	  else 
            traverse(y :: x :: done, static, rest_of_block)
	end
      else if not static andalso try_separate (opc2,opc1)
        then
          let
            val (pos,found,z) =
              search_for_moveable_instr (0,Sparc_Assembly.defines_and_uses opc2,
                                         SOME opc2, rest_of_block)
          in
            if found then
              let
                val (done,rest) =
                  copy_over (pos, (opc2,tag2,com2) :: z :: x :: done,rest_of_block)
              in
                traverse (done,static,drop rest)
              end
            else 
              (traverse (x :: done, static, (opc2,tag2,com2) :: rest_of_block))
          end
      else
	traverse(x :: done, static, rest)

  (* This goes through the code forwards *)
  fun trav2(acc,true, l) = rev l
    | trav2(acc,static,[]) = acc
    | trav2(acc,static,[a]) = a::acc
    | trav2(acc,static,(opc1,tag1,com1)::(opc2,tag2,com2)::rest) =
    let
      fun find_filler (n,[],_) = NONE
        | find_filler (n,(opc3,tag3,com3)::rest,current_defs) =
        if n > schedule_limit orelse has_delay opc3 orelse immoveable opc3 then NONE
        else
          let
            val (i_def, i_use, r_def, r_use) =
              Sparc_Assembly.defines_and_uses opc3
            val new_defs =
              (Set.setdiff(i_def, Set.singleton MachTypes.G0), 
               Set.setdiff(i_use, Set.singleton MachTypes.G0),
               r_def, r_use)
          in
            if passes_from_sets(current_defs,new_defs) andalso
              not (try_separate (opc1,opc3)) (* Would just get another delay *)
              then SOME ((opc3,tag3,com3),n)
            else
              find_filler (n+1,rest,
                           combine(new_defs, current_defs))
          end
    in
      (* if opc2 is a branch of some sort then it's pointless looking after it *)
      (* since all there will be is the delay slot *)
      if not (has_delay opc2) andalso try_separate (opc1,opc2)
        then
          case find_filler (0,rest,Sparc_Assembly.defines_and_uses opc2) of
            NONE => trav2 ((opc1,tag1,com1)::acc,static,(opc2,tag2,com2)::rest)
          | SOME (c,n) => 
              let
                val (done,rest) = copy_over (n,(opc2,tag2,com2) :: c :: (opc1,tag1,com1) :: acc, rest)
              in
                trav2 (done,static,drop rest)
              end
      else trav2 ((opc1,tag1,com1)::acc,static,(opc2,tag2,com2)::rest)
    end

  (* Insert nops as required between fcmps and fbranches *)
  fun check_fcmp opcodelist =
    let
      fun check (op1 :: op2 :: rest,acc) =
        if is_fcmp (#1 op1) andalso is_fbranch (#1 op2)
          then check (rest,op2 :: nop :: op1 :: acc)
        else check (op2::rest,op1::acc)
        | check ([a],acc) = rev (a::acc)
        | check ([],acc) = rev acc
    in
      case opcodelist of
        (op1 as (Sparc_Assembly.FBRANCH _,_,_)) :: rest => check (rest,[op1,nop])
      | (op1 as (Sparc_Assembly.FBRANCH_ANNUL _,_,_)) :: rest => check (rest,[op1,nop])
      | _ => check (opcodelist,[])
    end

  (* The static flag indicates that we are dealing with a setup procedure or *)
  (* a functor, so needn't worry about some of the transformations *)
  fun reschedule_block
    (static, ops as [_, (Sparc_Assembly.BRANCH_ANNUL(Sparc_Assembly.BA, _),_,_), _]) = ops
    | reschedule_block (static,opcode_list) =
    let
      val l1 = if false then trav2 ([],static,opcode_list) else rev opcode_list
      val l2 = if true then traverse ([],static,l1) else rev l1
      val l3 = if true then rev (trav2 ([],static,l2)) else l2
    in
      (* Note that we _must_ do check_fcmp or we may get scheduling errors *)
      check_fcmp l3
    end

end
@


1.42.3.1
log
@branched from trunk for label MLW_daveb_inline_1_4_99
@
text
@a3 4
 * Revision 1.42  1997/05/01  13:18:35  jont
 * [Bug #30088]
 * Get rid of MLWorks.Option
 *
@


1.42.2.1
log
@branched from trunk for label MLWorks_workspace_97
@
text
@a3 4
 * Revision 1.42  1997/05/01  13:18:35  jont
 * [Bug #30088]
 * Get rid of MLWorks.Option
 *
@


1.42.1.1
log
@branched from trunk for label MLWorks_dt_wizard
@
text
@a3 4
 * Revision 1.42  1997/05/01  13:18:35  jont
 * [Bug #30088]
 * Get rid of MLWorks.Option
 *
@


1.41
log
@[Bug #20001]
Remove debugging comments when filling load delays
@
text
@d4 4
d135 1
a135 1
  | refs_for_opcode_list(done, (_, MLWorks.Option.SOME tag, _) :: rest) =
d148 1
a148 1
  val nop = (nop_code, MLWorks.Option.NONE, "")
d193 1
a193 1
			MLWorks.Option.SOME tag, comment)) ::
d205 1
a205 1
			      MLWorks.Option.SOME tag, comment) :: done, rest)
d241 1
a241 1
	   MLWorks.Option.SOME dest_tag =>
d243 1
a243 1
		MLWorks.Option.SOME _ =>
d247 2
a248 2
	      | MLWorks.Option.NONE => dest_tag)
	 | MLWorks.Option.NONE => Crash.impossible"get_transitive_dest failure")
d258 1
a258 1
		   MLWorks.Option.SOME tag, comment)) =>
d260 3
a262 3
       MLWorks.Option.SOME dest_tag => 
	 (Sparc_Assembly.BRANCH(branch, 0), MLWorks.Option.SOME dest_tag, comment)
     | MLWorks.Option.NONE => opcode)
d264 1
a264 1
		  MLWorks.Option.SOME tag, comment)) =>
d266 3
a268 3
	 MLWorks.Option.SOME dest_tag =>
	   (Sparc_Assembly.BRANCH_ANNUL(branch, 0), MLWorks.Option.SOME dest_tag, comment)
       | MLWorks.Option.NONE => opcode)
d286 1
a286 1
	       MLWorks.Option.SOME dest_tag, _) :: _)) =>
d423 1
a423 1
				  MLWorks.Option.SOME tag, _) :: rest) =
d433 1
a433 1
				  MLWorks.Option.SOME tag, _) :: rest) =
d479 1
a479 1
           MLWorks.Option.SOME (instr, dest_tag) =>
d484 1
a484 1
		 MLWorks.Option.SOME dest_tag,
d489 1
a489 1
	 | MLWorks.Option.NONE =>
d502 1
a502 1
	     ((Sparc_Assembly.BRANCH_ANNUL(branch, _), MLWorks.Option.SOME tag,
d507 1
a507 1
		MLWorks.Option.SOME ((opcode, tag_opt, comment), new_tag) => 
d514 1
a514 1
				MLWorks.Option.SOME new_tag, bcomment) :: done,
d516 1
a516 1
	      | MLWorks.Option.NONE =>
d518 1
a518 1
	   | ((Sparc_Assembly.FBRANCH_ANNUL(branch, _), MLWorks.Option.SOME tag,
d523 1
a523 1
		MLWorks.Option.SOME ((opcode, tag_opt, comment), new_tag) =>
d530 1
a530 1
				MLWorks.Option.SOME new_tag, bcomment) :: done,
d532 1
a532 1
	      | MLWorks.Option.NONE =>
@


1.40
log
@Adding treatment of load delays
@
text
@d4 3
d649 1
a649 1
                  copy_over (pos, (opc2,tag2,com2^"<filled1>") :: z :: x :: done,rest_of_block)
d654 1
a654 1
              (traverse (x :: done, static, (opc2,tag2,com2^"<unfilled1>") :: rest_of_block))
d690 1
a690 1
            NONE => trav2 ((opc1,tag1,com1^"<unfilled2>")::acc,static,(opc2,tag2,com2)::rest)
d693 1
a693 1
                val (done,rest) = copy_over (n,(opc2,tag2,com2) :: c :: (opc1,tag1,com1^"<filled2>") :: acc, rest)
@


1.40.1.1
log
@branched from 1.40
@
text
@a3 3
 * Revision 1.40  1997/02/04  17:29:53  matthew
 * Adding treatment of load delays
 *
@


1.40.1.1.3.1
log
@branched from MLWorks_1_0_r2c1_1997_05_12 for label MLWorks_10r3
@
text
@a3 3
 * Revision 1.40.1.1  1997/05/12  10:50:15  hope
 * branched from 1.40
 *
@


1.40.1.1.2.1
log
@branched from MLWorks_1_0_r2c1_1997_05_12 for label MLWorks_10r2_551
@
text
@a3 3
 * Revision 1.40.1.1  1997/05/12  10:50:15  hope
 * branched from 1.40
 *
@


1.40.1.1.1.1
log
@branched from MLWorks_1_0_r2c1_1997_05_12 for label MLWorks_11
@
text
@a3 3
 * Revision 1.40.1.1  1997/05/12  10:50:15  hope
 * branched from 1.40
 *
@


1.40.1.1.1.1.1.1
log
@branched from MLWorks_11 for label MLWorks_11r1
@
text
@a3 3
 * Revision 1.40.1.1.1.1  1997/07/28  18:32:28  daveb
 * branched from MLWorks_1_0_r2c1_1997_05_12 for label MLWorks_11
 *
@


1.39
log
@Remove references to utils/option
@
text
@d4 3
a117 1
    val size = 1000
a119 1
  val new_tag = MirTypes.new_tag
d121 3
a123 1
  val new_tag = fn x => new_tag()
d132 1
a132 1
  fun refs_for_block(done, (tag, opcode_list)) =
d135 1
a135 1
  fun refs_all(tag, block_list) =
d143 2
a144 11
  fun sets_miss(s1, s2) =
    let
      val l1 = Set.set_to_list s1
    in
      not(Lists.exists (fn x => Set.is_member(x, s2)) l1)
    end

  fun passes_from_sets((first_i_defines, first_i_uses, first_r_defines,
			first_r_uses),
		       (second_i_defines, second_i_uses, second_r_defines,
			second_r_uses)) =
d146 6
a151 14
    sets_miss((first_i_defines, second_i_defines))
    andalso
    sets_miss((first_i_defines, second_i_uses)) andalso
    sets_miss((first_i_uses, second_i_defines)) andalso
    sets_miss((first_r_defines, second_r_defines))
    andalso
    sets_miss((first_r_defines, second_r_uses)) andalso
    sets_miss((first_r_uses, second_r_defines))
    andalso
    ((not (Set.is_member(MachTypes.cond, second_r_defines))) orelse
     (Set.empty_setp first_r_defines andalso Set.empty_setp first_r_uses))
  (* Final clause says fp instructions can't pass fp comparisions *)
  (* This is because the poxious architecture demands a non-fp *)
  (* instruction between a FCMP and a FBfcc *)
d211 2
a212 2
(* find_baa_block returns true on a block which is just a ba,a
 instruction (and an optional nop *)
d318 68
d458 1
a458 1
	 NewMap.define(tree, tag, (instr, new_tag())))
d496 1
a496 1
	       comment),
d507 1
a507 1
				MLWorks.Option.SOME new_tag, comment) :: done,
d512 1
a512 1
	       comment),
d523 1
a523 1
				MLWorks.Option.SOME new_tag, comment) :: done,
a534 1

d536 1
a536 4

	(* finally we remove any blocks which are not referred to by
	 other blocks *)

a537 1

d539 2
a540 2
      val final_block_list =
	Lists.filterp use_block next_block_list
d554 16
a569 2
  fun search_for_moveable_instr(pos, _, []) = (pos, false, nop)
    | search_for_moveable_instr(pos, current_defs,
d571 2
a572 9
      if opcode = Sparc_Assembly.other_nop_code orelse
	(case opcode of
	   Sparc_Assembly.LOAD_OFFSET _ => true
	     (* don't relocate over a load_offset *)
	 | Sparc_Assembly.ARITHMETIC_AND_LOGICAL (_, MachTypes.G2,_,_) => true
	     (* don't relocate over an allocation *)
	     | _ => false)
	then
	  (pos,false,nop)
d575 5
d584 2
a585 2
	  if is_delay (*orelse pos >= 100*) then
	    (pos, false, nop)
d595 6
a600 16
	      if pos <= 1 andalso
		(case rest of
		   (opc', _, _) :: _ =>
		     Set.is_member(MachTypes.cond,
				   #3 (Sparc_Assembly.defines_and_uses opc'))
		 | _ => false) then
		   (* Above test to ensure we don't remove the separating *)
		   (* instruction between an FCMP and an FBRANCH *)
		   search_for_moveable_instr(pos+1, combine(new_defs,
							    current_defs), rest)
	      else
		if passes_from_sets(new_defs, current_defs) then
		  (pos, true, opc)
		else
		  search_for_moveable_instr(pos+1, combine(new_defs, current_defs),
					    rest)
d604 13
a616 14
  fun copy_over(n, done, rest) =
    if n < 0 then
      Crash.impossible"copy_over negative argument"
    else
      if n = 0 then (done, rest)
      else
	case rest of
	  [] => Crash.impossible"copy_over run out of list"
	| x :: xs => copy_over(n-1, x :: done, xs)

  fun traverse(done, []) = done
    | traverse(done, [x]) = x :: done
    | traverse(done, (x as (opc1, _, _)) ::
	       (rest as ((y as (opc2, _, _)) :: rest_of_block))) =
d622 1
a622 1
				      rest_of_block)
d628 2
a629 2
		copy_over(pos, convert_branch_annul y :: z :: done,
			  rest_of_block)
d631 1
a631 3
	      case rest of
		[] => Crash.impossible"Scheduled from beyond end of block"
	      | x :: xs => traverse(done, xs)
d633 2
a634 8
	  else
	    traverse(y :: x :: done, rest_of_block)
	  (*
	   traverse(copy_over(if pos >= 2 then pos-2 else 0, y :: x :: done,
	   rest_of_block))
	   (* Copy over the number of failed instructions first *)
	   (* Making sure we don't skip any further delay instructions *)
	   *)
d636 17
d654 58
a711 1
	traverse(x :: done, rest)
d713 2
d716 11
a726 7
    (ops as [_, {1=Sparc_Assembly.BRANCH_ANNUL(Sparc_Assembly.BA, _), ...}, _]) = ops
    | reschedule_block opcode_list =
      let
        val rev_list = rev opcode_list
      in
        traverse([], rev_list)
      end
@


1.39.7.1
log
@branched from 1.39
@
text
@a3 3
 * Revision 1.39  1995/12/22  16:31:24  jont
 * Remove references to utils/option
 *
@


1.39.6.1
log
@branched from 1.39
@
text
@a3 3
 * Revision 1.39  1995/12/22  16:31:24  jont
 * Remove references to utils/option
 *
@


1.39.6.1.1.1
log
@branched from 1.39.6.1
@
text
@a3 3
 * Revision 1.39.6.1  1996/12/17  18:00:13  hope
 * branched from 1.39
 *
@


1.39.5.1
log
@branched from 1.39
@
text
@a3 3
 * Revision 1.39  1995/12/22  16:31:24  jont
 * Remove references to utils/option
 *
@


1.39.4.1
log
@branched from 1.39
@
text
@a3 3
 * Revision 1.39  1995/12/22  16:31:24  jont
 * Remove references to utils/option
 *
@


1.39.4.1.1.1
log
@branched from 1.39.4.1
@
text
@a3 3
 * Revision 1.39.4.1  1996/11/14  13:04:45  hope
 * branched from 1.39
 *
@


1.39.3.1
log
@branched from 1.39
@
text
@a3 3
 * Revision 1.39  1995/12/22  16:31:24  jont
 * Remove references to utils/option
 *
@


1.39.2.1
log
@branched from 1.39
@
text
@a3 3
 * Revision 1.39  1995/12/22  16:31:24  jont
 * Remove references to utils/option
 *
@


1.39.1.1
log
@branched from 1.39
@
text
@a3 3
 * Revision 1.39  1995/12/22  16:31:24  jont
 * Remove references to utils/option
 *
@


1.38
log
@Add extra field to procedure_parameters to contain old (pre register allocation)
spill sizes. This is for the i386, where spill assignment is done in the backend
@
text
@d4 4
a95 1
require "../utils/option";
a101 1
  structure Option : OPTION
a104 1
  structure Option = Option
@


1.37
log
@Putting sources registers for various instructions in correct order
@
text
@d4 3
d124 1
a124 1
  | refs_for_opcode_list(done, (_, MirTypes.Option.PRESENT tag, _) :: rest) =
d137 1
a137 1
  val nop = (nop_code, MirTypes.Option.ABSENT, "")
d199 1
a199 1
			MirTypes.Option.PRESENT tag, comment)) ::
d211 1
a211 1
			      MirTypes.Option.PRESENT tag, comment) :: done, rest)
d264 1
a264 1
		   MirTypes.Option.PRESENT tag, comment)) =>
d267 1
a267 1
	 (Sparc_Assembly.BRANCH(branch, 0), MirTypes.Option.PRESENT dest_tag, comment)
d270 1
a270 1
		  MirTypes.Option.PRESENT tag, comment)) =>
d273 1
a273 1
	   (Sparc_Assembly.BRANCH_ANNUL(branch, 0), MirTypes.Option.PRESENT dest_tag, comment)
d292 1
a292 1
	       MirTypes.Option.PRESENT dest_tag, _) :: _)) =>
d361 1
a361 1
				  MirTypes.Option.PRESENT tag, _) :: rest) =
d371 1
a371 1
				  MirTypes.Option.PRESENT tag, _) :: rest) =
d422 1
a422 1
		 MirTypes.Option.PRESENT dest_tag,
d440 1
a440 1
	     ((Sparc_Assembly.BRANCH_ANNUL(branch, _), MirTypes.Option.PRESENT tag,
d452 1
a452 1
				MirTypes.Option.PRESENT new_tag, comment) :: done,
d456 1
a456 1
	   | ((Sparc_Assembly.FBRANCH_ANNUL(branch, _), MirTypes.Option.PRESENT tag,
d468 1
a468 1
				MirTypes.Option.PRESENT new_tag, comment) :: done,
@


1.36
log
@Use pervasive Option.option for return values in NewMap
@
text
@d4 3
d231 2
a232 2
						 Sparc_Assembly.IMM 0,
						 MachTypes.G0),
d363 1
a363 2
		    (Sparc_Assembly.AND, MachTypes.G0, Sparc_Assembly.IMM 0,
		     MachTypes.G0), _, _) :: _) =>
d373 1
a373 2
		    (Sparc_Assembly.AND, MachTypes.G0, Sparc_Assembly.IMM 0,
		     MachTypes.G0), _, _) :: _) =>
d440 1
a440 2
	       (Sparc_Assembly.AND, MachTypes.G0, Sparc_Assembly.IMM 0,
		MachTypes.G0), _, _)) =>
d456 1
a456 2
	       (Sparc_Assembly.AND, MachTypes.G0, Sparc_Assembly.IMM 0,
		MachTypes.G0), _, _)) =>
@


1.35
log
@Prevent rescheduling of allocation sequence.
(Also add comments and abstract common code to a function).
@
text
@d4 4
d241 1
a241 1
	   NewMap.YES dest_tag =>
d243 1
a243 1
		NewMap.YES _ =>
d247 2
a248 2
	      | NewMap.NO => dest_tag)
	 | NewMap.NO => Crash.impossible"get_transitive_dest failure")
d260 1
a260 1
       NewMap.YES dest_tag => 
d262 1
a262 1
     | NewMap.NO => opcode)
d266 1
a266 1
	 NewMap.YES dest_tag =>
d268 1
a268 1
       | NewMap.NO => opcode)
d413 1
a413 1
           NewMap.YES (instr, dest_tag) =>
d423 1
a423 1
	 | NewMap.NO =>
d442 1
a442 1
		NewMap.YES ((opcode, tag_opt, comment), new_tag) => 
d451 1
a451 1
	      | NewMap.NO =>
d459 1
a459 1
		NewMap.YES ((opcode, tag_opt, comment), new_tag) =>
d468 1
a468 1
	      | NewMap.NO =>
@


1.34
log
@Adding load offset instructions
@
text
@d4 3
d172 10
d213 4
d230 3
d249 3
a266 2
  fun has_delay'(Sparc_Assembly.LOAD_OFFSET _) = true
    | has_delay' x = has_delay x
d268 1
a268 1
  fun reschedule_proc(tag, block_list) =
d270 1
a270 9
      (* Attempt to move instructions from the starts of blocks into the delay slots *)
      (* of instructions referencing them from other blocks *)
      (* Mustn't move LOAD_OFFSET instructions *)
      (* which may expand into more than one opcode *)
(*
      val _ = print("reschedule_proc with " ^
		    MLWorks.Integer.makestring(Lists.length block_list) ^ "blocks")
      val _ = output(std_out, "Reschedule proc tag " ^ MirTypes.print_tag tag ^ "\n")
*)
d274 3
d287 3
d292 3
d301 3
d305 22
d328 1
a328 5
      val block_list =
	map
	(fn (tag, opcode_list) =>
	 (tag, map replace_chained_branch' opcode_list))
	block_list
d330 3
d335 4
d344 4
a347 7
(*
      val _ = output(std_out, "ok_tags =\n")
      val _ = Lists.iterate
	(fn (tag, _) => output(std_out, MirTypes.print_tag tag ^ "\n"))
	ok_tags_and_instrs
*)
	
d374 3
d382 7
a388 6
(*
      val _ = output(std_out, "ref_tags =\n")
      val _ = output(std_out,
		     HashSet.set_print(ref_tags, 
				       (fn tag => MirTypes.print_tag tag ^ "\n")))
*)
d399 6
d424 4
d468 2
d475 1
a475 21
      val baa_blocks = Lists.filterp find_baa_block new_new_block_list

      val baa_tags_and_dests =
	Lists.reducel
	(fn (tree,
	     (tag,
	      (Sparc_Assembly.BRANCH_ANNUL(Sparc_Assembly.BA, _),
	       MirTypes.Option.PRESENT dest_tag, _) :: _)) =>
	 NewMap.define(tree, tag, dest_tag)
	     | _ => Crash.impossible"Bad baa block")
	(NewMap.empty, baa_blocks)

      val get_transitive_dest' = get_transitive_dest baa_tags_and_dests

      val baa_tags_and_dests =
	Lists.reducel
	(fn (tree, (tag, _)) =>
	 NewMap.define(tree, tag, get_transitive_dest'([], tag)))
	(NewMap.empty, baa_blocks)

      val replace_chained_branch' = replace_chained_branch baa_tags_and_dests
d477 2
a478 5
      val next_block_list =
	map
	(fn (tag, opcode_list) =>
	 (tag, map replace_chained_branch' opcode_list))
	new_new_block_list
d502 6
a507 1
	(case opcode of Sparc_Assembly.LOAD_OFFSET _ => true | _ => false)
@


1.33
log
@structure Option.
@
text
@d4 3
d156 4
a159 4
  | convert_branch_annul(Sparc_Assembly.FBRANCH_ANNUL
			 (branch, disp), tag_opt, comment) =
    (Sparc_Assembly.FBRANCH(branch, disp), tag_opt, comment)
  | convert_branch_annul opcode = opcode
d162 6
a167 6
  | has_delay(Sparc_Assembly.BRANCH_ANNUL _) = true
  | has_delay(Sparc_Assembly.FBRANCH _) = true
  | has_delay(Sparc_Assembly.FBRANCH_ANNUL _) = true
  | has_delay(Sparc_Assembly.Call _) = true
  | has_delay(Sparc_Assembly.JUMP_AND_LINK _) = true
  | has_delay _ = false
d198 1
a198 1
  | get_tag_and_instr(_, _) = Crash.impossible"Empty block"
d244 3
d249 4
d291 1
a291 1
	(fn (_, (x, _, _)) => not(has_delay x))
d302 24
a325 24
      | get_ref_tags_for_block(done,
			       (Sparc_Assembly.BRANCH_ANNUL(branch, _),
				MirTypes.Option.PRESENT tag, _) :: rest) =
	(case (branch, rest) of
	  (Sparc_Assembly.BA, _) =>
	    get_ref_tags_for_block(done, rest)
	| (_, (Sparc_Assembly.ARITHMETIC_AND_LOGICAL
	       (Sparc_Assembly.AND, MachTypes.G0, Sparc_Assembly.IMM 0,
		MachTypes.G0), _, _) :: _) =>
	  get_ref_tags_for_block(HashSet.add_member(done, tag), rest)
	| _ => get_ref_tags_for_block(done, rest))
      | get_ref_tags_for_block(done,
			       (Sparc_Assembly.FBRANCH_ANNUL(branch, _),
				MirTypes.Option.PRESENT tag, _) :: rest) =
	(case (branch, rest) of
	  (Sparc_Assembly.FBA, _) =>
	    get_ref_tags_for_block(done, rest)
	| (_, (Sparc_Assembly.ARITHMETIC_AND_LOGICAL
	       (Sparc_Assembly.AND, MachTypes.G0, Sparc_Assembly.IMM 0,
		MachTypes.G0), _, _) :: _) =>
	  get_ref_tags_for_block(HashSet.add_member(done, tag), rest)
	| _ => get_ref_tags_for_block(done, rest))
      | get_ref_tags_for_block(done, _ :: rest) =
	get_ref_tags_for_block(done, rest)
d459 42
a500 41
  | search_for_moveable_instr(pos, current_defs,
			      (opc as (opcode, _, _)) :: rest) =
    if opcode = Sparc_Assembly.other_nop_code
      then
        (pos,false,nop)
    else
      let
        val is_delay = case rest of
          [] => false
        | (opc', _, _) :: _ => has_delay opc'
      in
        if is_delay (*orelse pos >= 100*) then
          (pos, false, nop)
        else
          let
            val (i_def, i_use, r_def, r_use) =
              Sparc_Assembly.defines_and_uses opcode
            val new_defs =
              (Set.setdiff(i_def, Set.singleton MachTypes.G0), 
               Set.setdiff(i_use, Set.singleton MachTypes.G0),
               r_def, r_use)
          in
            if pos <= 1 andalso
              (case rest of
                 (opc', _, _) :: _ =>
                   Set.is_member(MachTypes.cond,
                                 #3 (Sparc_Assembly.defines_and_uses opc'))
               | _ => false) then
                 (* Above test to ensure we don't remove the separating *)
                 (* instruction between an FCMP and an FBRANCH *)
                 search_for_moveable_instr(pos+1, combine(new_defs,
                                                          current_defs), rest)
            else
              if passes_from_sets(new_defs, current_defs) then
		(pos, true, opc)
	    else
	      search_for_moveable_instr(pos+1, combine(new_defs, current_defs),
					rest)
          end
      end
    
d512 32
a543 32
  | traverse(done, [x]) = x :: done
  | traverse(done, (x as (opc1, _, _)) ::
	     (rest as ((y as (opc2, _, _)) :: rest_of_block))) =
    if opc1 = nop_code andalso has_delay opc2 then
      (* The difficult case, we've found a delay slot we would like to fill *)
      let
	val (pos, found, z) =
	  search_for_moveable_instr(0, Sparc_Assembly.defines_and_uses opc2,
				    rest_of_block)
      in
	if found then
	  (* Got a moveable instr, so move it in *)
	  let
	    val (done, rest) =
	      copy_over(pos, convert_branch_annul y :: z :: done,
			rest_of_block)
	  in
	    case rest of
	      [] => Crash.impossible"Scheduled from beyond end of block"
	    | x :: xs => traverse(done, xs)
	  end
	else
	  traverse(y :: x :: done, rest_of_block)
(*
	  traverse(copy_over(if pos >= 2 then pos-2 else 0, y :: x :: done,
			       rest_of_block))
      (* Copy over the number of failed instructions first *)
      (* Making sure we don't skip any further delay instructions *)
*)
      end
    else
      traverse(x :: done, rest)
@


1.33.1.1
log
@Fork for bug fixing
@
text
@a3 3
Revision 1.33  1993/07/29  15:30:08  nosa
structure Option.

@


1.32
log
@Removed integer parameter
@
text
@d4 3
d108 1
a108 1
  | refs_for_opcode_list(done, (_, MirTypes.PRESENT tag, _) :: rest) =
d121 1
a121 1
  val nop = (nop_code, MirTypes.ABSENT, "")
d173 1
a173 1
			MirTypes.PRESENT tag, comment)) ::
d185 1
a185 1
			      MirTypes.PRESENT tag, comment) :: done, rest)
d228 1
a228 1
		   MirTypes.PRESENT tag, comment)) =>
d231 1
a231 1
	 (Sparc_Assembly.BRANCH(branch, 0), MirTypes.PRESENT dest_tag, comment)
d234 1
a234 1
		  MirTypes.PRESENT tag, comment)) =>
d237 1
a237 1
	   (Sparc_Assembly.BRANCH_ANNUL(branch, 0), MirTypes.PRESENT dest_tag, comment)
d256 1
a256 1
	       MirTypes.PRESENT dest_tag, _) :: _)) =>
d294 1
a294 1
				MirTypes.PRESENT tag, _) :: rest) =
d305 1
a305 1
				MirTypes.PRESENT tag, _) :: rest) =
d347 1
a347 1
		 MirTypes.PRESENT dest_tag,
d361 1
a361 1
	     ((Sparc_Assembly.BRANCH_ANNUL(branch, _), MirTypes.PRESENT tag,
d374 1
a374 1
				MirTypes.PRESENT new_tag, comment) :: done,
d378 1
a378 1
	   | ((Sparc_Assembly.FBRANCH_ANNUL(branch, _), MirTypes.PRESENT tag,
d391 1
a391 1
				MirTypes.PRESENT new_tag, comment) :: done,
d409 1
a409 1
	       MirTypes.PRESENT dest_tag, _) :: _)) =>
@


1.31
log
@Modified to use new variable size hashsets
@
text
@d4 3
a72 1
require "../utils/integer";
a79 1
  structure Integer : INTEGER
d242 1
a242 1
		    Integer.makestring(Lists.length block_list) ^ "blocks")
@


1.30
log
@Improved scheduling of short blocks. Improved procedure level scheduling
by removing chained branchs first as well as after
@
text
@d4 4
d101 2
d114 1
a114 1
    (HashSet.add_member(HashSet.empty_set(), tag), block_list)
d316 1
a316 1
	(HashSet.empty_set(), block_list)
@


1.29
log
@Made selection of 3 instruction blocks more selective, to allow
scheduling of short blocks which tail or call but not those which branch
@
text
@d4 4
d142 1
a142 21
    let
      val new_branch = case branch of
	Sparc_Assembly.BAA => Sparc_Assembly.BA
      | Sparc_Assembly.BNA => Sparc_Assembly.BN
      | Sparc_Assembly.BNEA => Sparc_Assembly.BNE
      | Sparc_Assembly.BEA => Sparc_Assembly.BE
      | Sparc_Assembly.BGA => Sparc_Assembly.BG
      | Sparc_Assembly.BLEA => Sparc_Assembly.BLE
      | Sparc_Assembly.BGEA => Sparc_Assembly.BGE
      | Sparc_Assembly.BLA => Sparc_Assembly.BL
      | Sparc_Assembly.BGUA => Sparc_Assembly.BGU
      | Sparc_Assembly.BLEUA => Sparc_Assembly.BLEU
      | Sparc_Assembly.BCCA => Sparc_Assembly.BCC
      | Sparc_Assembly.BCSA => Sparc_Assembly.BCS
      | Sparc_Assembly.BPOSA => Sparc_Assembly.BPOS
      | Sparc_Assembly.BNEGA => Sparc_Assembly.BNEG
      | Sparc_Assembly.BVCA => Sparc_Assembly.BVC
      | Sparc_Assembly.BVSA => Sparc_Assembly.BVS
    in
      (Sparc_Assembly.BRANCH(new_branch, disp), tag_opt, comment)
    end
d145 1
a145 21
    let
      val new_branch = case branch of
	Sparc_Assembly.FBAA => Sparc_Assembly.FBA
      | Sparc_Assembly.FBNA => Sparc_Assembly.FBN
      | Sparc_Assembly.FBUA => Sparc_Assembly.FBU
      | Sparc_Assembly.FBGA => Sparc_Assembly.FBG
      | Sparc_Assembly.FBUGA => Sparc_Assembly.FBUG
      | Sparc_Assembly.FBLA => Sparc_Assembly.FBL
      | Sparc_Assembly.FBULA => Sparc_Assembly.FBUL
      | Sparc_Assembly.FBLGA => Sparc_Assembly.FBLG
      | Sparc_Assembly.FBNEA => Sparc_Assembly.FBNE
      | Sparc_Assembly.FBEA => Sparc_Assembly.FBE
      | Sparc_Assembly.FBUEA => Sparc_Assembly.FBUE
      | Sparc_Assembly.FBGEA => Sparc_Assembly.FBGE
      | Sparc_Assembly.FBUGEA => Sparc_Assembly.FBUGE
      | Sparc_Assembly.FBLEA => Sparc_Assembly.FBLE
      | Sparc_Assembly.FBULEA => Sparc_Assembly.FBULE
      | Sparc_Assembly.FBOA => Sparc_Assembly.FBO
    in
      (Sparc_Assembly.FBRANCH(new_branch, disp), tag_opt, comment)
    end
d162 1
a162 1
	       (opc as (Sparc_Assembly.BRANCH_ANNUL(opcode, _),
d168 1
a168 1
	   Sparc_Assembly.BAA =>
d174 2
a175 1
	       remove(op1 :: convert_branch_annul opc :: done, rest)
d187 44
d236 1
d238 29
d274 7
d286 1
a286 1
	  (Sparc_Assembly.BAA, _) =>
d297 1
a297 1
	  (Sparc_Assembly.FBAA, _) =>
d309 1
a309 1
	(fn (tags, (_, x)) => get_ref_tags_for_block(tags, x))
d312 7
d328 3
a330 3
      fun split_blocks([]) = []
      | split_blocks((_, []) :: _) = Crash.impossible"Empty block"
      | split_blocks((block as (tag, opcode :: rest)) :: others) =
d333 9
a341 7
             (tag,
              [opcode,
               (Sparc_Assembly.BRANCH_ANNUL(Sparc_Assembly.BAA, 0),
                MirTypes.PRESENT dest_tag,
                "Split off leading instruction")]) ::
             (dest_tag, rest) ::
             split_blocks others
d343 1
a343 1
             block :: split_blocks others)
d345 1
a345 1
      val new_block_list = split_blocks block_list
d347 40
d388 2
a389 43
	let
	  fun do_replace(done, []) = rev done
	  | do_replace(done, [opcode]) = rev(opcode :: done)
	  | do_replace(done, op1 :: op2 :: rest) =
	    (case (op1, op2) of
	      ((Sparc_Assembly.BRANCH_ANNUL(branch, _), MirTypes.PRESENT tag,
		comment),
	       (Sparc_Assembly.ARITHMETIC_AND_LOGICAL
		(Sparc_Assembly.AND, MachTypes.G0, Sparc_Assembly.IMM 0,
		 MachTypes.G0), _, _)) =>
              (case NewMap.tryApply'(ref_tags_etc, tag) of
                 NewMap.YES ((opcode, tag_opt, comment), new_tag) => 
                   if branch = Sparc_Assembly.BAA then
                     do_replace(op2 :: op1 :: done, rest)
                   else
                     do_replace((opcode, tag_opt, comment ^
                                 " moved to annulled delay slot") ::
                                (Sparc_Assembly.BRANCH_ANNUL(branch, 0),
                                 MirTypes.PRESENT new_tag, comment) :: done,
                                rest)
               | NewMap.NO =>
                   do_replace(op2 :: op1 :: done, rest))
	    | ((Sparc_Assembly.FBRANCH_ANNUL(branch, _), MirTypes.PRESENT tag,
		comment),
	       (Sparc_Assembly.ARITHMETIC_AND_LOGICAL
		(Sparc_Assembly.AND, MachTypes.G0, Sparc_Assembly.IMM 0,
		 MachTypes.G0), _, _)) =>
	      (case NewMap.tryApply'(ref_tags_etc, tag) of
		 NewMap.YES ((opcode, tag_opt, comment), new_tag) =>
                   if branch = Sparc_Assembly.FBAA then
                     do_replace(op2 :: op1 :: done, rest)
                   else
                     do_replace((opcode, tag_opt, comment ^
                                 " moved to annulled delay slot") ::
                                (Sparc_Assembly.FBRANCH_ANNUL(branch, 0),
                                 MirTypes.PRESENT new_tag, comment) :: done,
				rest)
               | NewMap.NO =>
                   do_replace(op2 :: op1 :: done, rest))
               | _ => do_replace(op1 :: done, op2 :: rest))
	in
	  (tag, do_replace([], opcode_list))
	end
a390 12
      fun find_baa_block(_, []) = Crash.impossible"Empty block"
      | find_baa_block(_, (Sparc_Assembly.BRANCH_ANNUL(Sparc_Assembly.BAA, _),
			   _, _) :: rest) =
	(case rest of
	  [] => true
	| [(Sparc_Assembly.ARITHMETIC_AND_LOGICAL(Sparc_Assembly.AND,
						  MachTypes.G0,
						  Sparc_Assembly.IMM 0,
						  MachTypes.G0),
	    _, _)] => true
	| _ => Crash.impossible"Unreachable instructions after BAA")
      | find_baa_block _ = false
d398 1
a398 1
	      (Sparc_Assembly.BRANCH_ANNUL(Sparc_Assembly.BAA, _),
d404 1
a404 10
      fun get_transitive_dest(visited, tag) =
        (case NewMap.tryApply'(baa_tags_and_dests, tag) of
           NewMap.YES dest_tag =>
             (case NewMap.tryApply'(baa_tags_and_dests, dest_tag) of
                NewMap.YES _ =>
                  if not(Lists.member(tag, visited)) then
                    get_transitive_dest(tag :: visited, dest_tag)
                  else dest_tag
              | NewMap.NO => dest_tag)
         | NewMap.NO => Crash.impossible"get_transitive_dest failure")
d409 1
a409 1
	 NewMap.define(tree, tag, get_transitive_dest([], tag)))
d412 2
a413 13
      fun replace_chained_branch(opcode as (Sparc_Assembly.BRANCH(branch, _),
					    MirTypes.PRESENT tag, comment)) =
	(case NewMap.tryApply'(baa_tags_and_dests, tag) of
	   NewMap.YES dest_tag => 
             (Sparc_Assembly.BRANCH(branch, 0), MirTypes.PRESENT dest_tag, comment)
         | NewMap.NO => opcode)
        | replace_chained_branch(opcode as (Sparc_Assembly.BRANCH_ANNUL(branch,_),
                                            MirTypes.PRESENT tag, comment)) =
          (case NewMap.tryApply'(baa_tags_and_dests, tag) of
             NewMap.YES dest_tag =>
               (Sparc_Assembly.BRANCH_ANNUL(branch, 0), MirTypes.PRESENT dest_tag, comment)
           | NewMap.NO => opcode)
        | replace_chained_branch opcode = opcode
d417 1
a417 1
	 (tag, map replace_chained_branch opcode_list))
d419 1
d427 1
a427 1
      result
d430 4
d525 1
a525 1
    (ops as [_, {1=Sparc_Assembly.BRANCH_ANNUL(Sparc_Assembly.BAA, _), ...}, _]) = ops
@


1.28
log
@Changed the type of nop used for tracing to store it being moved by the scheduler
@
text
@d4 3
d402 1
a402 4
      val result = (tag, map remove_doubled_instrs final_block_list)
(*
      val _ = print"reschedule_proc finished"
*)
d446 1
a446 1
                (pos, true, opc)
d497 2
a498 1
  fun reschedule_block (ops as [_,_,_]) = ops
@


1.27
log
@Changed reschedule block to ignore the small blocks (really to
get the new stack overflow check optimised)
@
text
@d4 4
d414 33
a446 29
    let
      val is_delay = case rest of
	[] => false
      | (opc', _, _) :: _ => has_delay opc'
    in
      if is_delay (*orelse pos >= 100*) then
	(pos, false, nop)
      else
	let
	  val (i_def, i_use, r_def, r_use) =
	    Sparc_Assembly.defines_and_uses opcode
	  val new_defs =
	    (Set.setdiff(i_def, Set.singleton MachTypes.G0), 
	     Set.setdiff(i_use, Set.singleton MachTypes.G0),
	     r_def, r_use)
	in
	  if pos <= 1 andalso
	    (case rest of
	       (opc', _, _) :: _ =>
		 Set.is_member(MachTypes.cond,
			       #3 (Sparc_Assembly.defines_and_uses opc'))
	     | _ => false) then
	       (* Above test to ensure we don't remove the separating *)
	       (* instruction between an FCMP and an FBRANCH *)
	       search_for_moveable_instr(pos+1, combine(new_defs,
							current_defs), rest)
	  else
	    if passes_from_sets(new_defs, current_defs) then
	      (pos, true, opc)
d450 3
a452 3
	end
    end

@


1.26
log
@Modified to use new improved hashset signature
@
text
@d4 3
d489 7
a495 6
  fun reschedule_block opcode_list =
    let
      val rev_list = rev opcode_list
    in
      traverse([], rev_list)
    end
@


1.25
log
@Modified to use MirTypes.Map for the tag lookups
@
text
@d4 3
d85 1
a85 1
    refs_for_opcode_list(HashSet.add_member(tag, done), rest)
a90 4
  fun refs_for_block_list(done, []) = done
  | refs_for_block_list(done, x :: xs) =
    refs_for_block_list(refs_for_block(done, x), xs)

d92 3
a94 2
    refs_for_block_list(HashSet.add_member(tag, HashSet.empty_set()),
			block_list)
d235 1
a235 1
	  get_ref_tags_for_block(HashSet.add_member(tag, done), rest)
d246 1
a246 1
	  get_ref_tags_for_block(HashSet.add_member(tag, done), rest)
a250 4
      fun get_ref_tags_for_block_list(done, []) = done
      | get_ref_tags_for_block_list(done, (_, x) :: xs) =
	get_ref_tags_for_block_list(get_ref_tags_for_block(done, x), xs)

d252 3
a254 2
	get_ref_tags_for_block_list(HashSet.empty_set(), block_list)
      fun filter_fun(tag, _) = HashSet.is_member(tag, ref_tags)
d256 2
d389 1
a389 1
      fun use_block(tag, _) = HashSet.is_member(tag, new_ref_tags)
@


1.24
log
@Change to NewMap.empty which now takes < and = functions instead of the single-function
@
text
@d4 3
d67 1
a67 1
  structure NewMap = MirTypes.Debugger_Types.Datatypes.NewMap
d263 1
a263 1
	(NewMap.empty (MirTypes.order_tag,MirTypes.equal_tag),
d351 1
a351 1
	(NewMap.empty (MirTypes.order_tag,MirTypes.equal_tag), baa_blocks)
d368 1
a368 1
	(NewMap.empty (MirTypes.order_tag,MirTypes.equal_tag), baa_blocks)
@


1.23
log
@Removed some handles of hashtable lookup exceptions
@
text
@d4 3
d255 1
a255 4
      fun tag_order arg =
	if MirTypes.equal_tag arg then NewMap.EQUAL
	else if MirTypes.order_tag arg then NewMap.LESS
	else NewMap.GREATER
d260 1
a260 1
	(NewMap.empty tag_order,
d348 1
a348 1
	(NewMap.empty tag_order, baa_blocks)
d365 1
a365 1
	(NewMap.empty tag_order, baa_blocks)
@


1.22
log
@Removed some redundant structures and sharing
@
text
@d4 3
d266 12
a277 12
	(let
	   val (instr, dest_tag) = NewMap.apply'(ref_tags_etc, tag)
	 in
	   (tag,
	    [opcode,
	     (Sparc_Assembly.BRANCH_ANNUL(Sparc_Assembly.BAA, 0),
	      MirTypes.PRESENT dest_tag,
	      "Split off leading instruction")]) ::
	   (dest_tag, rest) ::
	   split_blocks others
	 end handle NewMap.Undefined =>
	   block :: split_blocks others)
d279 1
d291 12
a302 13
	      (let
		 val ((opcode, tag_opt, comment), new_tag) = NewMap.apply'(ref_tags_etc, tag)
	       in
		 if branch = Sparc_Assembly.BAA then
		   do_replace(op2 :: op1 :: done, rest)
		 else
		   do_replace((opcode, tag_opt, comment ^
			       " moved to annulled delay slot") ::
			      (Sparc_Assembly.BRANCH_ANNUL(branch, 0),
			       MirTypes.PRESENT new_tag, comment) :: done,
			      rest)
	       end handle NewMap.Undefined =>
		 do_replace(op2 :: op1 :: done, rest))
d308 9
a316 10
	      (let
		 val ((opcode, tag_opt, comment), new_tag) = NewMap.apply'(ref_tags_etc, tag)
	       in
		 if branch = Sparc_Assembly.FBAA then
		   do_replace(op2 :: op1 :: done, rest)
		 else
		   do_replace((opcode, tag_opt, comment ^
			       " moved to annulled delay slot") ::
			      (Sparc_Assembly.FBRANCH_ANNUL(branch, 0),
			       MirTypes.PRESENT new_tag, comment) :: done,
d318 3
a320 3
	       end handle NewMap.Undefined =>
		 do_replace(op2 :: op1 :: done, rest))
	    | _ => do_replace(op1 :: done, op2 :: rest))
d351 9
a359 13
	let
	  val dest_tag = (NewMap.apply'(baa_tags_and_dests, tag))
	    handle NewMap.Undefined => Crash.impossible"get_transitive_dest failure"
	in
	  (let
	     val _ = NewMap.apply'(baa_tags_and_dests, dest_tag)
	   in
	     if not(Lists.member(tag, visited)) then
	       get_transitive_dest(tag :: visited, dest_tag)
	     else dest_tag
	   end handle NewMap.Undefined =>
	     dest_tag)
	end
d369 11
a379 16
	(let
	   val dest_tag = NewMap.apply'(baa_tags_and_dests, tag)
	 in
	   (Sparc_Assembly.BRANCH(branch, 0), MirTypes.PRESENT dest_tag, comment)
	 end handle NewMap.Undefined =>
	   opcode)
      | replace_chained_branch(opcode as (Sparc_Assembly.BRANCH_ANNUL(branch,
								      _),
					  MirTypes.PRESENT tag, comment)) =
	(let
	   val dest_tag = NewMap.apply'(baa_tags_and_dests, tag)
	 in
	   (Sparc_Assembly.BRANCH_ANNUL(branch, 0), MirTypes.PRESENT dest_tag, comment)
	 end handle NewMap.Undefined =>
	   opcode)
      | replace_chained_branch opcode = opcode
@


1.21
log
@Now uses NewMap instead of BalancedTree structure.
Array is now pervasive.
@
text
@d4 4
a36 1
require "../utils/newmap";
a38 1
require "../utils/set";
a41 2
require "../mir/mirtypes";
require "machtypes";
a45 1
  structure NewMap : NEWMAP
a46 1
  structure Set : SET
a49 2
  structure MirTypes : MIRTYPES
  structure MachTypes : MACHTYPES
a50 3

  sharing MachTypes = Sparc_Assembly.MachTypes
  sharing Set = Sparc_Assembly.Set
a52 1
  structure Set = Set
a53 2
  structure MirTypes = MirTypes
  structure MachTypes = MachTypes
d55 4
a59 1
    structure Array = Array
@


1.20
log
@Fixed problem with delay opcodes disappearing from between fcmp
and fb
@
text
@d4 4
d33 1
a33 1
require "../utils/balancedtree";
a35 1
require "../utils/array";
d46 1
a46 1
  structure BalancedTree : BALANCEDTREE
d257 3
a259 7
	if MirTypes.equal_tag arg then
	  BalancedTree.EQ
	else
	  if MirTypes.order_tag arg then
	    BalancedTree.LT
	  else
	    BalancedTree.GT
d263 2
a264 2
	 BalancedTree.insert(tree, tag, (instr, new_tag())))
	(BalancedTree.empty tag_order,
d270 12
a281 10
	case BalancedTree.lookup(ref_tags_etc, tag) of
	  BalancedTree.YES(instr, dest_tag) =>
	    (tag,
	     [opcode,
	      (Sparc_Assembly.BRANCH_ANNUL(Sparc_Assembly.BAA, 0),
	       MirTypes.PRESENT dest_tag,
	       "Split off leading instruction")]) ::
	    (dest_tag, rest) ::
	    split_blocks others
	| _ => block :: split_blocks others
d294 13
a306 11
	      (case BalancedTree.lookup(ref_tags_etc, tag) of
		 BalancedTree.YES((opcode, tag_opt, comment), new_tag) =>
		   if branch = Sparc_Assembly.BAA then
		     do_replace(op2 :: op1 :: done, rest)
		   else
		     do_replace((opcode, tag_opt, comment ^
				 " moved to annulled delay slot") ::
				(Sparc_Assembly.BRANCH_ANNUL(branch, 0),
				 MirTypes.PRESENT new_tag, comment) :: done,
				rest)
	       | _ => do_replace(op2 :: op1 :: done, rest))
d312 10
a321 9
	      (case BalancedTree.lookup(ref_tags_etc, tag) of
		 BalancedTree.YES((opcode, tag_opt, comment), new_tag) =>
		   if branch = Sparc_Assembly.FBAA then
		     do_replace(op2 :: op1 :: done, rest)
		   else
		     do_replace((opcode, tag_opt, comment ^
				 " moved to annulled delay slot") ::
				(Sparc_Assembly.FBRANCH_ANNUL(branch, 0),
				 MirTypes.PRESENT new_tag, comment) :: done,
d323 2
a324 1
	       | _ => do_replace(op2 :: op1 :: done, rest))
d351 1
a351 1
	 BalancedTree.insert(tree, tag, dest_tag)
d353 1
a353 1
	(BalancedTree.empty tag_order, baa_blocks)
d357 2
a358 3
	  val dest_tag = case BalancedTree.lookup(baa_tags_and_dests, tag) of
	    BalancedTree.YES tag => tag
	  | _ => Crash.impossible"get_transitive_dest failure"
d360 8
a367 6
	  case BalancedTree.lookup(baa_tags_and_dests, dest_tag) of
	    BalancedTree.YES _ =>
	      if not(Lists.member(tag, visited)) then
		get_transitive_dest(tag :: visited, dest_tag)
	      else dest_tag
	  | _ => dest_tag
d373 2
a374 2
	 BalancedTree.insert(tree, tag, get_transitive_dest([], tag)))
	(BalancedTree.empty tag_order, baa_blocks)
d378 6
a383 5
	(case BalancedTree.lookup(baa_tags_and_dests, tag) of
	   BalancedTree.YES dest_tag =>
	     (Sparc_Assembly.BRANCH(branch, 0),
	      MirTypes.PRESENT dest_tag, comment)
	 | _ => opcode)
d387 6
a392 5
	(case BalancedTree.lookup(baa_tags_and_dests, tag) of
	   BalancedTree.YES dest_tag =>
	     (Sparc_Assembly.BRANCH_ANNUL(branch, 0),
	      MirTypes.PRESENT dest_tag, comment)
	 | _ => opcode)
@


1.19
log
@Modified the inter block rescheduler to use hash sets and
balanced trees instead of sets and association lists
@
text
@d4 4
d427 1
a427 1
	  if pos = 1 andalso
@


1.18
log
@Reded intrablock shceduling to be much more efficient
@
text
@d4 3
d25 2
d28 1
d39 1
d58 9
a66 1

d73 1
a73 1
    refs_for_opcode_list(Set.add_member(tag, done), rest)
d84 2
a85 1
    refs_for_block_list(Set.singleton tag, block_list)
d226 1
a226 1
	  get_ref_tags_for_block(Set.add_member(tag, done), rest)
d237 1
a237 1
	  get_ref_tags_for_block(Set.add_member(tag, done), rest)
d246 18
a263 7
      val ref_tags = get_ref_tags_for_block_list(Set.empty_set, block_list)
      fun filter_fun(tag, _) = Set.is_member(tag, ref_tags)
      val ref_tags_and_instrs =
	Lists.filterp filter_fun ok_tags_and_instrs
      val ok_ref_tags = map #1 ref_tags_and_instrs
      val (ref_tags_and_new_tags, _) =
	Lists.number_from_by_one(ok_ref_tags, 0, new_tag)
d267 2
a268 4
	if Lists.member(tag, ok_ref_tags) then
	  let
	    val dest_tag = Lists.assoc(tag, ref_tags_and_new_tags)
	  in
d274 3
a276 4
	     (dest_tag, rest) ::
	     split_blocks others
	  end
	else block :: split_blocks others
d289 11
a299 15
	      if branch = Sparc_Assembly.BAA orelse
		(not(Lists.member(tag, ok_ref_tags))) then
		do_replace(op2 :: op1 :: done, rest)
	      else
		let
		  val new_tag = Lists.assoc(tag, ref_tags_and_new_tags)
		  val (opcode, tag_opt, comment) =
		    Lists.assoc(tag, ref_tags_and_instrs)
		in
		  do_replace((opcode, tag_opt, comment ^
			      " moved to annulled delay slot") ::
			     (Sparc_Assembly.BRANCH_ANNUL(branch, 0),
			      MirTypes.PRESENT new_tag, comment) :: done,
			     rest)
		end
d305 11
a315 15
	      if branch = Sparc_Assembly.FBAA orelse
		(not(Lists.member(tag, ok_ref_tags))) then
		do_replace(op2 :: op1 :: done, rest)
	      else
		let
		  val new_tag = Lists.assoc(tag, ref_tags_and_new_tags)
		  val (opcode, tag_opt, comment) =
		    Lists.assoc(tag, ref_tags_and_instrs)
		in
		  do_replace((opcode, tag_opt, comment ^
			      " moved to annulled delay slot") ::
			     (Sparc_Assembly.FBRANCH_ANNUL(branch, 0),
			      MirTypes.PRESENT new_tag, comment) :: done,
			     rest)
		end
d337 8
a344 7
	map
	(fn (tag,
	     (Sparc_Assembly.BRANCH_ANNUL(Sparc_Assembly.BAA, _),
	      MirTypes.PRESENT dest_tag, _) :: _) => (tag, dest_tag)
	    | _ => Crash.impossible"Bad baa block")
	baa_blocks
      val baa_tags = map #1 baa_tags_and_dests
d348 3
a350 1
	  val dest_tag = Lists.assoc(tag, baa_tags_and_dests)
d352 6
a357 4
	  if Lists.member(dest_tag, baa_tags)
	    andalso not(Lists.member(tag, visited)) then
	    get_transitive_dest(tag :: visited, dest_tag)
	  else dest_tag
d359 1
d361 5
a365 1
	map (fn tag => (tag, get_transitive_dest([], tag))) baa_tags
d368 5
a372 4
	if Lists.member(tag, baa_tags) then
	  (Sparc_Assembly.BRANCH(branch, 0),
	   MirTypes.PRESENT(Lists.assoc(tag, baa_tags_and_dests)), comment)
	else opcode
d376 5
a380 4
	if Lists.member(tag, baa_tags) then
	  (Sparc_Assembly.BRANCH_ANNUL(branch, 0),
	   MirTypes.PRESENT(Lists.assoc(tag, baa_tags_and_dests)), comment)
	else opcode
d389 1
a389 1
      fun use_block(tag, _) = Set.is_member(tag, new_ref_tags)
@


1.17
log
@Improved speed of rescheduling by about a factor of ten by
careful use of set primitives
@
text
@d4 4
a51 4
  datatype node =
    NODE of int
  and edge = EDGE of {start : node, finish : node}

d71 4
a74 1
  fun find_delay_instr opcode_list =
d76 1
a76 57
      fun delay_sub(_, []) = (0, false)
      | delay_sub(pos,
		  (opcode as Sparc_Assembly.BRANCH_ANNUL _, _, _) ::
		  rest) =
	(case rest of
	  (Sparc_Assembly.ARITHMETIC_AND_LOGICAL
	   (Sparc_Assembly.AND, MachTypes.G0, Sparc_Assembly.IMM 0,
	    MachTypes.G0), _, _) :: _ => (pos, true)
	  | _ => delay_sub(pos + 1, rest))
(*
This section removed, cos at present we can't
get anything past the preceding FCMP to do the job
      | delay_sub(pos,
		  (opcode as Sparc_Assembly.FBRANCH_ANNUL _, _, _) ::
		  rest) =
	(case rest of
	  (Sparc_Assembly.ARITHMETIC_AND_LOGICAL
	   (Sparc_Assembly.AND, MachTypes.G0, Sparc_Assembly.IMM 0,
	    MachTypes.G0), _, _) :: _ => (pos, true)
	  | _ => delay_sub(pos + 1, rest))
*)
      | delay_sub(pos,
		  (opcode as Sparc_Assembly.FUNARY(funary, _, _), _, _) ::
		  rest) =
	let
	  val is_cmp = case funary of
	    Sparc_Assembly.FCMPS => true
	  | Sparc_Assembly.FCMPD => true
	  | Sparc_Assembly.FCMPX => true
	  | _ => false
	in
	  if is_cmp then
	    (case rest of
	      (Sparc_Assembly.ARITHMETIC_AND_LOGICAL
	       (Sparc_Assembly.AND, MachTypes.G0, Sparc_Assembly.IMM 0,
		MachTypes.G0), _, _) :: _ => (pos, true)
	    | _ => delay_sub(pos + 1, rest))
	  else
	    delay_sub(pos + 1, rest)
	end
      | delay_sub(pos,
		  (opcode as Sparc_Assembly.JUMP_AND_LINK _, _, _) ::
		  rest) =
	(case rest of
	  (Sparc_Assembly.ARITHMETIC_AND_LOGICAL
	   (Sparc_Assembly.AND, MachTypes.G0, Sparc_Assembly.IMM 0,
	    MachTypes.G0), _, _) :: _ => (pos, true)
	  | _ => delay_sub(pos + 1, rest))
      | delay_sub(pos,
		  (opcode as Sparc_Assembly.Call _, _, _) ::
		  rest) =
	(case rest of
	  (Sparc_Assembly.ARITHMETIC_AND_LOGICAL
	   (Sparc_Assembly.AND, MachTypes.G0, Sparc_Assembly.IMM 0,
	    MachTypes.G0), _, _) :: _ => (pos, true)
	  | _ => delay_sub(pos + 1, rest))
      | delay_sub(pos, _ :: rest) = delay_sub(pos + 1, rest)
d78 1
a78 1
      delay_sub(0, opcode_list)
a80 120
  val nop_code =
    Sparc_Assembly.ARITHMETIC_AND_LOGICAL
    (Sparc_Assembly.AND, MachTypes.G0, Sparc_Assembly.IMM 0, MachTypes.G0)
  val nop = (nop_code, MirTypes.ABSENT, "")

(*
  fun remove_nop_from_baa opcode_list =
    (case rev opcode_list of
      (opcode as
       (Sparc_Assembly.BRANCH_ANNUL(Sparc_Assembly.BAA, _), _, _)) ::
       (Sparc_Assembly.ARITHMETIC_AND_LOGICAL
	(Sparc_Assembly.AND, MachTypes.G0, Sparc_Assembly.IMM 0,
	 MachTypes.G0), _, _) :: rest => rev(opcode :: rest)
    | _ => opcode_list)
*)

  fun must_precede(i_defined_set, i_used_set, r_defined_set, r_used_set,
		   opcode) =
    (case opcode of
      Sparc_Assembly.LOAD_AND_STORE(operation, rd, rs1, reg_or_imm) =>
	let
	  val rs2 = case reg_or_imm of
	    Sparc_Assembly.REG rs2 => rs2
	  | _ => rs1
	  val is_load = case operation of
	    Sparc_Assembly.STB => false
	  | Sparc_Assembly.STH => false
	  | Sparc_Assembly.ST => false
	  | Sparc_Assembly.STD => false
	  | _ => true
	in
	  Set.is_member(rd, i_defined_set) orelse
	  Set.is_member(rs1, i_defined_set) orelse
	  Set.is_member(rs2, i_defined_set) orelse
	  Set.is_member(MachTypes.store, i_defined_set) orelse
	  Set.is_member(MachTypes.store, r_defined_set) orelse
	  ((is_load andalso
	    Set.is_member(rd, i_used_set)) orelse
	   ((not is_load) andalso
	    (Set.is_member(MachTypes.store, i_used_set) orelse
	     Set.is_member(MachTypes.store, r_used_set))))
	end
    | Sparc_Assembly.LOAD_AND_STORE_FLOAT(operation, rd, rs1, reg_or_imm) =>
	let
	  val rs2 = case reg_or_imm of
	    Sparc_Assembly.REG rs2 => rs2
	  | _ => rs1
	  val is_load = case operation of
	    Sparc_Assembly.STF => false
	  | Sparc_Assembly.STDF => false
	  | _ => true
	in
	  Set.is_member(rd, r_defined_set) orelse
	  Set.is_member(rs1, i_defined_set) orelse
	  Set.is_member(rs2, i_defined_set) orelse
	  Set.is_member(MachTypes.store, i_defined_set) orelse
	  Set.is_member(MachTypes.store, r_defined_set) orelse
	  ((is_load andalso Set.is_member(rd, r_used_set)) orelse
	   ((not is_load) andalso
	    (Set.is_member(MachTypes.store, i_used_set) orelse
	     Set.is_member(MachTypes.store, r_used_set))))
	end
    | Sparc_Assembly.ARITHMETIC_AND_LOGICAL(_, rd, reg_or_imm, rs1) =>
	let
	  val rs2 = case reg_or_imm of
	    Sparc_Assembly.REG rs2 => rs2
	  | _ => rs1
	in
	  Set.is_member(rd, i_defined_set) orelse
	  Set.is_member(rs1, i_defined_set) orelse
	  Set.is_member(rs2, i_defined_set) orelse
	  Set.is_member(rd, i_used_set)
	end
    | Sparc_Assembly.TAGGED_ARITHMETIC(_, rd, reg_or_imm, rs1) =>
	let
	  val rs2 = case reg_or_imm of
	    Sparc_Assembly.REG rs2 => rs2
	  | _ => rs1
	in
	  Set.is_member(rd, i_defined_set) orelse
	  Set.is_member(rs1, i_defined_set) orelse
	  Set.is_member(rs2, i_defined_set) orelse
	  Set.is_member(rd, i_used_set)
	end
    | Sparc_Assembly.SetHI(_, rd, i) =>
	Set.is_member(rd, i_defined_set) orelse
	Set.is_member(rd, i_used_set)
    | Sparc_Assembly.SAVE_AND_RESTORE _ => true
    | Sparc_Assembly.BRANCH _ => true
    | Sparc_Assembly.BRANCH_ANNUL _ => true
    | Sparc_Assembly.Call _ => true
    | Sparc_Assembly.JUMP_AND_LINK _ => true
    | Sparc_Assembly.FBRANCH _ => true
    | Sparc_Assembly.FBRANCH_ANNUL _ => true
    | Sparc_Assembly.CONV_OP(conv_op, rd, rs2) =>
	Set.is_member(rd, r_defined_set) orelse
	Set.is_member(rs2, r_defined_set) orelse
	Set.is_member(rd, r_used_set)
    | Sparc_Assembly.FUNARY(funary, rd, rs2) =>
	let
	  val defines_cond = case funary of
	    Sparc_Assembly.FCMPS => true
	  | Sparc_Assembly.FCMPD => true
	  | Sparc_Assembly.FCMPX => true
	  | _ => false
	in
	  if defines_cond then
	    not(Set.empty_setp r_defined_set andalso Set.empty_setp r_used_set)
	  else
	    Set.is_member(rd, r_defined_set) orelse
	    Set.is_member(rs2, r_defined_set) orelse
	    Set.is_member(rd, r_used_set)
	end
    | Sparc_Assembly.FBINARY(fbinary, rd, rs1, rs2) =>
	Set.is_member(rd, r_defined_set) orelse
	Set.is_member(rs1, r_defined_set) orelse
	Set.is_member(rs2, r_defined_set) orelse
	Set.is_member(rd, r_used_set)
	)

d85 2
a86 1
    Set.empty_setp(Set.intersection(first_i_defines, second_i_defines))
d88 3
a90 3
    Set.empty_setp(Set.intersection(first_i_defines, second_i_uses)) andalso
    Set.empty_setp(Set.intersection(first_i_uses, second_i_defines)) andalso
    Set.empty_setp(Set.intersection(first_r_defines, second_r_defines))
d92 2
a93 2
    Set.empty_setp(Set.intersection(first_r_defines, second_r_uses)) andalso
    Set.empty_setp(Set.intersection(first_r_uses, second_r_defines))
a94 11
    (not
     ((Set.is_member(MachTypes.store, first_i_defines) andalso
       (Set.is_member(MachTypes.store, second_r_defines) orelse
	Set.is_member(MachTypes.store, second_r_uses))) orelse
      (Set.is_member(MachTypes.store, first_r_defines) andalso
       (Set.is_member(MachTypes.store, second_i_defines) orelse
	Set.is_member(MachTypes.store, second_i_uses))) orelse
      (Set.is_member(MachTypes.store, first_i_uses) andalso
       Set.is_member(MachTypes.store, second_r_defines))))
    (* Disallow interactions between real and integer store *)
    andalso
d96 1
a96 1
     (Set.empty_setp first_r_defines andalso Set.empty_setp first_r_defines))
a100 86
  fun cant_follow_from_sets((first_i_defines, first_i_uses, first_r_defines,
			     first_r_uses),
			    (second_i_defines, second_i_uses, second_r_defines,
			     second_r_uses)) =
    Set.is_member(MachTypes.cond, first_r_defines) andalso
    Set.is_member(MachTypes.cond, second_r_uses)
  (* This says we must have another instruction between a FCMP and an FBfcc *)

  fun rev_map _ ([], b_list) = b_list
  | rev_map f (a_list, b_list) =
    let
      fun map'([], b_list) = b_list
      | map'(x :: xs, b_list) =
	f x :: map'(xs, b_list)
    in
      map'(a_list, b_list)
    end

  fun make_graph(edges_so_far, []) = edges_so_far
  | make_graph(edges_so_far,
	       (op_tag_comment as (opcode, _, _), i) :: op_list) =
    let
      val (i_defines, i_uses, r_defines, r_uses) =
	Sparc_Assembly.defines_and_uses opcode
      val i_defines = Set.setdiff(i_defines, Set.singleton MachTypes.G0)
      val i_uses = Set.setdiff(i_uses, Set.singleton MachTypes.G0)
      fun newedge (_, x) = EDGE{start = NODE i, finish = NODE x}
      fun before_all() =
	rev_map
	newedge
	(op_list, edges_so_far)

      fun before_some() =
	rev_map
	newedge
	((Lists.filterp
	  (fn x => must_precede(i_defines, i_uses, r_defines, r_uses,
				#1 (#1 x)))
	  op_list), edges_so_far)
    in
      case opcode of
	Sparc_Assembly.LOAD_AND_STORE _ =>
	  make_graph(before_some(), op_list)
      | Sparc_Assembly.LOAD_AND_STORE_FLOAT _ =>
	  make_graph(before_some(), op_list)
      | Sparc_Assembly.ARITHMETIC_AND_LOGICAL(_, rd, reg_or_imm, rs1) =>
	  make_graph(before_some(), op_list)
      | Sparc_Assembly.TAGGED_ARITHMETIC(_, rd, reg_or_imm, rs1) =>
	  make_graph(before_some(), op_list)
      | Sparc_Assembly.SetHI(_, rd, i) =>
	  make_graph(before_some(), op_list)
      | Sparc_Assembly.SAVE_AND_RESTORE _ =>
	  make_graph(before_all(), op_list)
      | Sparc_Assembly.BRANCH _ =>
	  make_graph(before_all(), op_list)
      | Sparc_Assembly.BRANCH_ANNUL _ =>
	  make_graph(before_all(), op_list)
      | Sparc_Assembly.Call _ =>
	  make_graph(before_all(), op_list)
      | Sparc_Assembly.JUMP_AND_LINK _ =>
	  make_graph(before_all(), op_list)
      | Sparc_Assembly.FBRANCH _ =>
	  make_graph(before_all(), op_list)
      | Sparc_Assembly.FBRANCH_ANNUL _ =>
	  make_graph(before_all(), op_list)
      | Sparc_Assembly.CONV_OP(conv_op, rd, rs2) =>
	  make_graph(before_some(), op_list)
      | Sparc_Assembly.FUNARY(funary, rd, rs2) =>
	  make_graph(before_some(), op_list)
      | Sparc_Assembly.FBINARY(fbinary, rd, rs1, rs2) =>
	  make_graph(before_some(), op_list)
    end

  fun print_edge(EDGE{start = NODE start, finish = NODE finish}) =
    implode["Edge from node ", Integer.makestring start,
    " to ", Integer.makestring finish, "\n"]
    
  fun print_node((opcode, tag_opt, comment), i) =
    implode[Sparc_Assembly.print opcode, " ", comment, " : Node ",
	    Integer.makestring i, "\n"]

  fun print_graph(edge_list, numbered_opcode_list) =
    print(implode
	  ("Nodes\n" :: map print_node numbered_opcode_list) ^
	  implode("Edges\n" :: (map print_edge edge_list)))

d149 7
a155 126
  fun reschedule_sections(done, [], _, _) = rev done
  | reschedule_sections(done, opcode_list, edge_list, offset) =
    let
      val (delay_pos, found) = find_delay_instr opcode_list
      val real_delay_pos = offset + delay_pos
(*
      val _ =
	print
	(if found then
	   "Found suitable target at offset " ^
	   Integer.makestring real_delay_pos ^
	   " starting at offset " ^ Integer.makestring offset
	 else
	   "Failed to find target")
*)
	   
    (* Relative to list start, as are the node names *)
    in
      if found then
	let
	  fun get_edges_for_pos(done, _, []) = (done, [])
	  | get_edges_for_pos(done, pos,
			      edge_list as
			      ((edge as EDGE
			       {start = NODE start, finish = NODE finish}) ::
			       rest)) =
	    if start > pos then (done, edge_list)
	    else
	      get_edges_for_pos
	      (if start = pos then edge :: done else done,
		 pos, rest)
	  fun find_moveable_instr(_, []) = (0, false, [])
	  | find_moveable_instr(pos, edge_list) =
	    if pos >= real_delay_pos then
	      (0, false, edge_list)
	    else
	      let
		val (edges_for_pos, remaining_edges) =
		  get_edges_for_pos([], pos, edge_list)
		  fun edge_required(EDGE{finish = NODE finish, ...}) =
		    finish <= real_delay_pos
		  fun has_delay(Sparc_Assembly.BRANCH _) = true
		  | has_delay(Sparc_Assembly.BRANCH_ANNUL _) = true
		  | has_delay(Sparc_Assembly.FBRANCH _) = true
		  | has_delay(Sparc_Assembly.FBRANCH_ANNUL _) = true
		  | has_delay(Sparc_Assembly.Call _) = true
		  | has_delay(Sparc_Assembly.JUMP_AND_LINK _) = true
		  | has_delay _ = false
	      in
		case Lists.filterp edge_required edges_for_pos of
		  [EDGE{start = NODE start, finish = NODE finish}] =>
		    if start = pos andalso finish = real_delay_pos andalso
		      (pos - offset = 0 orelse
		       not(has_delay(#1 (Lists.nth(pos - offset - 1,
						   opcode_list))))) andalso
		      (* Ensure we don't move instructions from delay slots *)
		      passes_from_sets
		      (Sparc_Assembly.defines_and_uses
		       (#1 (Lists.nth(pos - offset,
				      opcode_list))),
		       Sparc_Assembly.defines_and_uses
		       (#1 (Lists.nth(delay_pos,
				      opcode_list))))
		      then
			(pos, true, remaining_edges)
		    else
		      find_moveable_instr(pos + 1, remaining_edges)
		| _ =>
		    find_moveable_instr(pos + 1, remaining_edges)
	      end
	  val (move_pos, can_move, remainder) =
	    find_moveable_instr(offset, edge_list)
(*
	  val _ =
	    if can_move then
	      print("Considering moving instruction at offset " ^
		    Integer.makestring move_pos ^
		    " past instruction at offset " ^
		    Integer.makestring real_delay_pos)
	    else
	      print"Can't find anything to move"
*)
	in
	  if can_move then
	    let
	      fun reshuffle(_, _, _, _, []) =
		Crash.impossible"reshuffle off end"
	      | reshuffle(done, pos1, pos2, op_to_move,
			  (oper as (opcode, tag_opt, comment)) :: rest) =
		(case (pos1, pos2) of
		  (0, _) => reshuffle(done, ~1, pos2 - 1,
				      (opcode, tag_opt,
				       comment ^ " moved to delay slot"), rest)
		| (_, 1) => reshuffle(convert_branch_annul oper :: done,
				      pos1 - 1, pos2 - 1, op_to_move, rest)
		| (_, 0) => (op_to_move :: done, rest)
		| _ => reshuffle(oper :: done, pos1 - 1, pos2 - 1,
				 op_to_move, rest))
	      val (done, new_opcode_list) =
		reshuffle(done, move_pos - offset, delay_pos + 1, nop,
			  opcode_list)
	    in
	      reschedule_sections(done, new_opcode_list, remainder,
				  offset + delay_pos + 2)
	    end
	  else
	    let
	      fun copy_over(done, [], i) =
		if i <= 0 then (done, [])
		else
		  Crash.impossible"Copy_over run out"
	      | copy_over(done, remainder as (opcode :: rest), n) =
		if n <= 0 then (done, remainder)
		else copy_over(opcode :: done, rest, n-1)
	      val (done', opcode_list') =
		copy_over(done, opcode_list, delay_pos + 2)
	    in
	      reschedule_sections(done', opcode_list', remainder,
				  offset + delay_pos + 2)
	    end
	end
      else
	reschedule_sections(hd opcode_list :: done, tl opcode_list, [],
			    offset + 1)
    (* Just shuffle all across and give result *)
    end
a183 15
  fun reschedule_block opcode_list =
    if length opcode_list <= 3 then
      opcode_list
    else
      let
	val (numbered_opcode_list, next) =
	  Lists.number_from_by_one(opcode_list, 0, fn x => x)
	val block_graph = rev(make_graph([], numbered_opcode_list))
(*
      val _ = print_graph(block_graph, numbered_opcode_list)
*)
      in
	reschedule_sections([], opcode_list, block_graph, 0)
      end

a186 8
  fun isnt_transfer(Sparc_Assembly.BRANCH _) = false
  | isnt_transfer(Sparc_Assembly.BRANCH_ANNUL _) = false
  | isnt_transfer(Sparc_Assembly.FBRANCH _) = false
  | isnt_transfer(Sparc_Assembly.FBRANCH_ANNUL _) = false
  | isnt_transfer(Sparc_Assembly.JUMP_AND_LINK _) = false
  | isnt_transfer(Sparc_Assembly.Call _) = false
  | isnt_transfer _ = true

d189 4
d197 1
a197 1
	(fn (_, (x, _, _)) => isnt_transfer x)
d237 3
a239 2
      fun split_block(_, []) = Crash.impossible"Empty block"
      | split_block(block as (tag, opcode :: rest)) =
d244 7
a250 5
	    [(tag, [opcode,
		    (Sparc_Assembly.BRANCH_ANNUL(Sparc_Assembly.BAA, 0),
		     MirTypes.PRESENT dest_tag,
		     "Split off leading instruction")]),
	     (dest_tag, rest)]
d252 2
a253 4
	else [block]
      val new_block_list =
	Lists.reducel op @@
	([], map split_block block_list)
d334 2
a335 2
	    andalso not(Set.is_member(tag, visited)) then
	    get_transitive_dest(Set.add_member(tag, visited), dest_tag)
d339 1
a339 1
	map (fn tag => (tag, get_transitive_dest(Set.empty_set, tag))) baa_tags
d364 4
d369 1
a369 1
      (tag, map remove_doubled_instrs final_block_list)
d372 92
@


1.16
log
@Some efficiency changes by jont re-checked because of accidental
deletion.
@
text
@d4 4
d56 4
a59 23
  fun find_all_ref_tags(tag, block_list) =
    (* This currently could include unneeded blocks *)
    (* For instance, a pair of mutually referencing blocks *)
    (* I do not believe this happens in practice *)
    (* If it does, change the algorithm *)
    let
      fun refs_for_block opcode_list =
	let
	  fun sub_fun(done, []) = done
	  | sub_fun(done, (_, MirTypes.PRESENT tag, _) :: rest) =
	    sub_fun(Set.add_member(tag, done), rest)
	  | sub_fun(done, _ :: rest) =  sub_fun(done, rest)
	in
	  sub_fun(Set.empty_set, opcode_list)
	end
    in
      Lists.reducel
      Set.union
      (Set.singleton tag,
       map
       (fn (_, x) => refs_for_block x)
       block_list)
    end
d61 10
d587 11
a599 2
      fun get_tag_and_instr(_, []) = Crash.impossible"Empty block"
      | get_tag_and_instr(tag, opcode :: rest) = (tag, opcode)
a601 7
      fun isnt_transfer(Sparc_Assembly.BRANCH _) = false
      | isnt_transfer(Sparc_Assembly.BRANCH_ANNUL _) = false
      | isnt_transfer(Sparc_Assembly.FBRANCH _) = false
      | isnt_transfer(Sparc_Assembly.FBRANCH_ANNUL _) = false
      | isnt_transfer(Sparc_Assembly.JUMP_AND_LINK _) = false
      | isnt_transfer(Sparc_Assembly.Call _) = false
      | isnt_transfer _ = true
d633 6
a638 7
      val ref_tags =
	Lists.reducel
	Set.union
	(Set.empty_set,
	 map
	 (fn (_, x) => get_ref_tags_for_block(Set.empty_set, x))
	 block_list)
d640 1
a640 3
	Lists.filterp
	(fn (x, _) => Set.is_member(x, ref_tags))
	ok_tags_and_instrs
d765 1
a765 1
      val new_ref_tags = find_all_ref_tags(tag, next_block_list)
d767 1
d769 1
a769 3
	Lists.filterp
	(fn (tag, _) => Set.is_member(tag, new_ref_tags))
	next_block_list
@


1.15
log
@Removed an error message that was given out at each computed goto -
duplicate instructions after an annulled branch
@
text
@d3 8
a10 1
$Log
d18 1
d29 1
d39 1
d48 4
d297 10
d315 1
d317 3
a319 3
	map
	(fn x => EDGE{start = NODE i, finish = NODE(#2 x)})
	op_list
d322 6
a327 6
	map
	(fn x => EDGE{start = NODE i, finish = NODE (#2 x)})
	(Lists.filterp
	 (fn x => must_precede(i_defines, i_uses, r_defines, r_uses,
			       #1 (#1 x)))
	 op_list)
d331 1
a331 1
	  make_graph(before_some() @@ edges_so_far, op_list)
d333 1
a333 1
	  make_graph(before_some() @@ edges_so_far, op_list)
d335 1
a335 1
	  make_graph(before_some() @@ edges_so_far, op_list)
d337 1
a337 1
	  make_graph(before_some() @@ edges_so_far, op_list)
d339 1
a339 1
	  make_graph(before_some() @@ edges_so_far, op_list)
d341 1
a341 1
	  make_graph(before_all() @@ edges_so_far, op_list)
d343 1
a343 1
	  make_graph(before_all() @@ edges_so_far, op_list)
d345 1
a345 1
	  make_graph(before_all() @@ edges_so_far, op_list)
d347 1
a347 1
	  make_graph(before_all() @@ edges_so_far, op_list)
d349 1
a349 1
	  make_graph(before_all() @@ edges_so_far, op_list)
d351 1
a351 1
	  make_graph(before_all() @@ edges_so_far, op_list)
d353 1
a353 1
	  make_graph(before_all() @@ edges_so_far, op_list)
d355 1
a355 1
	  make_graph(before_some() @@ edges_so_far, op_list)
d357 1
a357 1
	  make_graph(before_some() @@ edges_so_far, op_list)
d359 1
a359 1
	  make_graph(before_some() @@ edges_so_far, op_list)
d649 1
a649 1
	Lists.number_from_by_one(ok_ref_tags, 0, fn x => MirTypes.new_tag())
@


1.14
log
@Fixed bug whereby nops were faulted after BAA as unreachable
@
text
@d538 3
a540 2
	     (print"Strange, unreachable code";
	      remove(opc :: done, other))
@


1.13
log
@Added function to eliminate doubled instructions following conditional
annulled branches
@
text
@d696 5
@


1.12
log
@Stopped infinite loop when a block consists of a single self referencing
branch
@
text
@d350 48
a482 48
	      fun convert_branch_annul(Sparc_Assembly.BRANCH_ANNUL
				       (branch, disp), tag_opt, comment) =
		let
		  val new_branch = case branch of
		    Sparc_Assembly.BAA => Sparc_Assembly.BA
		  | Sparc_Assembly.BNA => Sparc_Assembly.BN
		  | Sparc_Assembly.BNEA => Sparc_Assembly.BNE
		  | Sparc_Assembly.BEA => Sparc_Assembly.BE
		  | Sparc_Assembly.BGA => Sparc_Assembly.BG
		  | Sparc_Assembly.BLEA => Sparc_Assembly.BLE
		  | Sparc_Assembly.BGEA => Sparc_Assembly.BGE
		  | Sparc_Assembly.BLA => Sparc_Assembly.BL
		  | Sparc_Assembly.BGUA => Sparc_Assembly.BGU
		  | Sparc_Assembly.BLEUA => Sparc_Assembly.BLEU
		  | Sparc_Assembly.BCCA => Sparc_Assembly.BCC
		  | Sparc_Assembly.BCSA => Sparc_Assembly.BCS
		  | Sparc_Assembly.BPOSA => Sparc_Assembly.BPOS
		  | Sparc_Assembly.BNEGA => Sparc_Assembly.BNEG
		  | Sparc_Assembly.BVCA => Sparc_Assembly.BVC
		  | Sparc_Assembly.BVSA => Sparc_Assembly.BVS
		in
		  (Sparc_Assembly.BRANCH(new_branch, disp), tag_opt, comment)
		end
	      | convert_branch_annul(Sparc_Assembly.FBRANCH_ANNUL
				     (branch, disp), tag_opt, comment) =
		let
		  val new_branch = case branch of
		    Sparc_Assembly.FBAA => Sparc_Assembly.FBA
		  | Sparc_Assembly.FBNA => Sparc_Assembly.FBN
		  | Sparc_Assembly.FBUA => Sparc_Assembly.FBU
		  | Sparc_Assembly.FBGA => Sparc_Assembly.FBG
		  | Sparc_Assembly.FBUGA => Sparc_Assembly.FBUG
		  | Sparc_Assembly.FBLA => Sparc_Assembly.FBL
		  | Sparc_Assembly.FBULA => Sparc_Assembly.FBUL
		  | Sparc_Assembly.FBLGA => Sparc_Assembly.FBLG
		  | Sparc_Assembly.FBNEA => Sparc_Assembly.FBNE
		  | Sparc_Assembly.FBEA => Sparc_Assembly.FBE
		  | Sparc_Assembly.FBUEA => Sparc_Assembly.FBUE
		  | Sparc_Assembly.FBGEA => Sparc_Assembly.FBGE
		  | Sparc_Assembly.FBUGEA => Sparc_Assembly.FBUGE
		  | Sparc_Assembly.FBLEA => Sparc_Assembly.FBLE
		  | Sparc_Assembly.FBULEA => Sparc_Assembly.FBULE
		  | Sparc_Assembly.FBOA => Sparc_Assembly.FBO
		in
		  (Sparc_Assembly.FBRANCH(new_branch, disp), tag_opt, comment)
		end
	      | convert_branch_annul opcode = opcode

d525 26
d747 1
a747 1
      (tag, final_block_list)
@


1.11
log
@Fixed bug whereby instructions in delay slots were being rescheduled
@
text
@d684 1
a684 1
      fun get_transitive_dest tag =
d688 3
a690 2
	  if Lists.member(dest_tag, baa_tags) then
	    get_transitive_dest dest_tag
d694 1
a694 1
	map (fn tag => (tag, get_transitive_dest tag)) baa_tags
@


1.10
log
@Modified such that very small block don't get internally rescheduled.
They're probably better off with the lineariser
@
text
@d391 7
d401 14
a414 12
		    if start = pos andalso finish = real_delay_pos then
		      if passes_from_sets
			(Sparc_Assembly.defines_and_uses
			 (#1 (Lists.nth(pos - offset,
					opcode_list))),
			 Sparc_Assembly.defines_and_uses
			 (#1 (Lists.nth(delay_pos,
					opcode_list))))
			then
			  (pos, true, remaining_edges)
		      else
			find_moveable_instr(pos + 1, remaining_edges)
@


1.9
log
@Moved defines_and_uses into _sparc_assembly
@
text
@d160 6
a165 2
	  ((is_load andalso Set.is_member(rd, i_used_set)) orelse
	   (not is_load) andalso Set.is_member(MachTypes.store, i_used_set))
d177 1
a177 1
	  Set.is_member(rd, i_defined_set) orelse
d181 5
a185 2
	  ((is_load andalso Set.is_member(rd, i_used_set)) orelse
	   (not is_load) andalso Set.is_member(MachTypes.store, i_used_set))
d220 3
a222 18
	let
	  val to_fp = case conv_op of
	    Sparc_Assembly.FITOS => true
	  | Sparc_Assembly.FITOD => true
	  | Sparc_Assembly.FITOX => true
	  | Sparc_Assembly.FSTOI => false
	  | Sparc_Assembly.FDTOI => false
	  | Sparc_Assembly.FXTOI => false
	in
	  if to_fp then
	    Set.is_member(rd, i_defined_set) orelse
	    Set.is_member(rs2, r_defined_set) orelse
	    Set.is_member(rd, i_used_set)
	  else
	    Set.is_member(rd, r_defined_set) orelse
	    Set.is_member(rs2, i_defined_set) orelse
	    Set.is_member(rd, r_used_set)
	end
d258 11
d517 7
a523 4
    let
      val (numbered_opcode_list, next) =
	Lists.number_from_by_one(opcode_list, 0, fn x => x)
      val block_graph = rev(make_graph([], numbered_opcode_list))
d527 1
a527 1
      val new_opcode_list =
d529 1
a529 3
    in
      new_opcode_list (* Temporary *)
    end
@


1.8
log
@Added floating point operations with delay slots to the lineariser.
There is a problem with FCMP, in that it must be followed by a
non-fp instruction, making it hard to schedule seomthing into the
delay slot of the following FBRANCH. However, we can still move things
from the target back to it.
@
text
@d26 1
d29 1
d31 1
a279 170
  fun defines_and_uses(Sparc_Assembly.LOAD_AND_STORE(operation, rd, rs1,
						     reg_or_imm)) =
    let
      val rs2 = case reg_or_imm of
	Sparc_Assembly.REG rs2 => rs2
      | _ => rs1
      val is_load = case operation of
	Sparc_Assembly.STB => false
      | Sparc_Assembly.STH => false
      | Sparc_Assembly.ST => false
      | Sparc_Assembly.STD => false
      | _ => true
      val main_uses = Set.list_to_set[rs1, rs2]
    in
      (Set.singleton(if is_load then rd else MachTypes.store),
       Set.union(main_uses,
		 Set.singleton(if is_load then MachTypes.store else rd)),
       Set.empty_set, Set.empty_set)
    end
  | defines_and_uses(Sparc_Assembly.LOAD_AND_STORE_FLOAT(operation, rd, rs1,
							 reg_or_imm)) =
    let
      val rs2 = case reg_or_imm of
	Sparc_Assembly.REG rs2 => rs2
      | _ => rs1
      val is_load = case operation of
	Sparc_Assembly.STF => false
      | Sparc_Assembly.STDF => false
      | _ => true
      val main_uses = Set.list_to_set[rs1, rs2]
    in
      (Set.empty_set,
       main_uses,
       Set.singleton(if is_load then rd else MachTypes.store),
       Set.singleton(if is_load then MachTypes.store else rd))
    end
  | defines_and_uses(Sparc_Assembly.ARITHMETIC_AND_LOGICAL(opcode, rd,
							   reg_or_imm, rs1)) =
    let
      val rs2 = case reg_or_imm of
	Sparc_Assembly.REG rs2 => rs2
      | _ => rs1
      val (defines_cond, uses_cond) =
	case opcode of
	  Sparc_Assembly.ADD => (false, false)
	| Sparc_Assembly.ADDCC => (true, false)
	| Sparc_Assembly.ADDX => (false, true)
	| Sparc_Assembly.ADDXCC => (true, true)
	| Sparc_Assembly.SUB => (false, false)
	| Sparc_Assembly.SUBCC => (true, false)
	| Sparc_Assembly.SUBX => (false, true)
	| Sparc_Assembly.SUBXCC => (true, true)
	| Sparc_Assembly.AND => (false, false)
	| Sparc_Assembly.ANDCC => (true, false)
	| Sparc_Assembly.ANDN => (false, false)
	| Sparc_Assembly.ANDNCC => (true, false)
	| Sparc_Assembly.OR => (false, false)
	| Sparc_Assembly.ORCC => (true, false)
	| Sparc_Assembly.ORN => (false, false)
	| Sparc_Assembly.ORNCC => (true, false)
	| Sparc_Assembly.XOR => (false, false)
	| Sparc_Assembly.XORCC => (true, false)
	| Sparc_Assembly.XORN => (false, false)
	| Sparc_Assembly.XORNCC => (true, false)
	| Sparc_Assembly.SLL => (false, false)
	| Sparc_Assembly.SRL => (false, false)
	| Sparc_Assembly.SRA => (false, false)

    in
      (if defines_cond then Set.list_to_set[MachTypes.cond, rd]
       else Set.singleton rd,
	 if uses_cond then Set.list_to_set[MachTypes.cond, rs1, rs2]
	 else Set.list_to_set[rs1, rs2],
       Set.empty_set, Set.empty_set)
    end
  | defines_and_uses(Sparc_Assembly.TAGGED_ARITHMETIC(_, rd, reg_or_imm,
						      rs1)) =
    let
      val rs2 = case reg_or_imm of
	Sparc_Assembly.REG rs2 => rs2
      | _ => rs1
    in
      (Set.list_to_set[MachTypes.cond, rd], Set.list_to_set[rs1, rs2],
       Set.empty_set, Set.empty_set)
    end
  | defines_and_uses(Sparc_Assembly.SetHI(_, rd, i)) =
    (Set.singleton rd, Set.empty_set,
       Set.empty_set, Set.empty_set)
  | defines_and_uses(Sparc_Assembly.SAVE_AND_RESTORE(_, rd, reg_or_imm, rs1)) =
    let
      val rs2 = case reg_or_imm of
	Sparc_Assembly.REG rs2 => rs2
      | _ => rs1
    in
      (Set.singleton rd, Set.list_to_set[rs1, rs2],
       Set.empty_set, Set.empty_set)
    end
  | defines_and_uses(Sparc_Assembly.BRANCH(branch, _)) =
    (Set.empty_set,
      (case branch of
	Sparc_Assembly.BA => Set.empty_set
      | Sparc_Assembly.BN => Set.empty_set
      | _ => Set.singleton MachTypes.cond),
	 Set.empty_set, Set.empty_set)
  | defines_and_uses(Sparc_Assembly.BRANCH_ANNUL(branch, _)) =
    (Set.empty_set,
      (case branch of
	Sparc_Assembly.BAA => Set.empty_set
      | Sparc_Assembly.BNA => Set.empty_set
      | _ => Set.singleton MachTypes.cond),
       Set.empty_set, Set.empty_set)
  | defines_and_uses(Sparc_Assembly.Call _) =
    (Set.singleton MachTypes.lr, Set.empty_set,
     Set.empty_set, Set.empty_set)
  | defines_and_uses(Sparc_Assembly.JUMP_AND_LINK(_, rd, reg_or_imm, rs1)) =
    let
      val rs2 = case reg_or_imm of
	Sparc_Assembly.REG rs2 => rs2
      | _ => rs1
    in
      (Set.singleton rd, Set.list_to_set[rs1, rs2],
       Set.empty_set, Set.empty_set)
    end
  | defines_and_uses(Sparc_Assembly.FBRANCH(fbranch, _)) =
    (Set.empty_set,
     Set.empty_set,
     Set.empty_set,
      (case fbranch of
	Sparc_Assembly.FBA => Set.empty_set
      | Sparc_Assembly.FBN => Set.empty_set
      | _ => Set.singleton MachTypes.cond))
  | defines_and_uses(Sparc_Assembly.FBRANCH_ANNUL(fbranch_annul, _)) =
    (Set.empty_set,
     Set.empty_set,
     Set.empty_set,
      (case fbranch_annul of
	Sparc_Assembly.FBAA => Set.empty_set
      | Sparc_Assembly.FBNA => Set.empty_set
      | _ => Set.singleton MachTypes.cond))
  | defines_and_uses(Sparc_Assembly.CONV_OP(conv_op, rd, rs2)) =
    let
      val to_fp = case conv_op of
	Sparc_Assembly.FITOS => true
      | Sparc_Assembly.FITOD => true
      | Sparc_Assembly.FITOX => true
      | Sparc_Assembly.FSTOI => false
      | Sparc_Assembly.FDTOI => false
      | Sparc_Assembly.FXTOI => false
    in
      if to_fp then
	(Set.empty_set, Set.singleton rs2, Set.singleton rd, Set.empty_set)
      else
	(Set.singleton rd, Set.empty_set, Set.empty_set, Set.singleton rs2)
    end
  | defines_and_uses(Sparc_Assembly.FUNARY(funary, rd, rs2)) =
    let
      val defines_cond = case funary of
	Sparc_Assembly.FCMPS => true
      | Sparc_Assembly.FCMPD => true
      | Sparc_Assembly.FCMPX => true
      | _ => false
    in
      (Set.empty_set, Set.empty_set,
       Set.singleton(if defines_cond then MachTypes.cond else rd),
       if defines_cond then Set.list_to_set[rd, rs2] else Set.singleton rs2)
    end
  | defines_and_uses(Sparc_Assembly.FBINARY(fbinary, rd, rs1, rs2)) =
    (Set.empty_set, Set.empty_set, Set.singleton rd,
     Set.list_to_set[rs1, rs2])

d284 2
a285 1
      val (i_defines, i_uses, r_defines, r_uses) = defines_and_uses opcode
d393 6
a398 4
			(defines_and_uses(#1 (Lists.nth(pos - offset,
						    opcode_list))),
			 defines_and_uses(#1 (Lists.nth(delay_pos,
							opcode_list))))
@


1.7
log
@Added scheduling of floating point instructions, plus interaction
with integer instructions
@
text
@d70 3
d74 28
d127 1
d136 1
d229 14
a242 3
	Set.is_member(rd, r_defined_set) orelse
	Set.is_member(rs2, r_defined_set) orelse
	Set.is_member(rd, r_used_set)
d254 14
a267 8
    (Set.empty_setp(Set.intersection(first_i_defines, second_i_defines))
     andalso
     Set.empty_setp(Set.intersection(first_i_defines, second_i_uses)) andalso
     Set.empty_setp(Set.intersection(first_i_uses, second_i_defines)) andalso
     Set.empty_setp(Set.intersection(first_r_defines, second_r_defines))
     andalso
     Set.empty_setp(Set.intersection(first_r_defines, second_r_uses)) andalso
     Set.empty_setp(Set.intersection(first_r_uses, second_r_defines)))
d269 8
d432 11
a442 1
    (Set.empty_set, Set.empty_set, Set.singleton rd, Set.singleton rs2)
d610 23
d699 2
d721 11
d788 20
@


1.6
log
@Commented out a lot of diagnostics
@
text
@d36 4
d105 2
a106 1
  fun must_precede(defined_set, used_set, opcode) =
d120 6
a125 6
	  Set.is_member(rd, defined_set) orelse
	  Set.is_member(rs1, defined_set) orelse
	  Set.is_member(rs2, defined_set) orelse
	  Set.is_member(MachTypes.store, defined_set) orelse
	  ((is_load andalso Set.is_member(rd, used_set)) orelse
	   (not is_load) andalso Set.is_member(MachTypes.store, used_set))
d127 17
a143 2
    | Sparc_Assembly.LOAD_AND_STORE_FLOAT =>
	Crash.unimplemented"Floating point"
d150 4
a153 4
	  Set.is_member(rd, defined_set) orelse
	  Set.is_member(rs1, defined_set) orelse
	  Set.is_member(rs2, defined_set) orelse
	  Set.is_member(rd, used_set)
d161 4
a164 4
	  Set.is_member(rd, defined_set) orelse
	  Set.is_member(rs1, defined_set) orelse
	  Set.is_member(rs2, defined_set) orelse
	  Set.is_member(rd, used_set)
d167 2
a168 2
	Set.is_member(rd, defined_set) orelse
	Set.is_member(rd, used_set)
d174 30
d206 12
a217 5
  fun passes_from_sets((first_defines, first_uses),
			 (second_defines, second_uses)) =
    (Set.empty_setp(Set.intersection(first_defines, second_defines)) andalso
     Set.empty_setp(Set.intersection(first_defines, second_uses)) andalso
     Set.empty_setp(Set.intersection(first_uses, second_defines)))
d235 2
a236 1
		 Set.singleton(if is_load then MachTypes.store else rd)))
d238 17
a254 2
  | defines_and_uses(Sparc_Assembly.LOAD_AND_STORE_FLOAT) =
    Crash.unimplemented"Floating point"
d291 2
a292 1
	 else Set.list_to_set[rs1, rs2])
d301 2
a302 1
      (Set.list_to_set[MachTypes.cond, rd], Set.list_to_set[rs1, rs2])
d305 2
a306 1
    (Set.singleton rd, Set.empty_set)
d313 2
a314 1
      (Set.singleton rd, Set.list_to_set[rs1, rs2])
d320 3
a322 1
      | _ => Set.singleton MachTypes.cond))
d327 3
a329 1
      | _ => Set.singleton MachTypes.cond))
d331 2
a332 1
    (Set.singleton MachTypes.lr, Set.empty_set)
d339 2
a340 1
      (Set.singleton rd, Set.list_to_set[rs1, rs2])
d342 36
d383 3
a385 3
      val (defines, uses) = defines_and_uses opcode
      val defines = Set.setdiff(defines, Set.singleton MachTypes.G0)
      val uses = Set.setdiff(uses, Set.singleton MachTypes.G0)
d395 2
a396 1
	 (fn x => must_precede(defines, uses, #1 (#1 x)))
d402 2
a403 2
      | Sparc_Assembly.LOAD_AND_STORE_FLOAT =>
	  Crash.unimplemented"Floating point"
d420 10
@


1.5
log
@Fixed load/store interaction bug
@
text
@d320 1
a320 1

d329 1
a329 1

d376 1
d385 1
@


1.4
log
@Added backwards instruction scheduling to take instructions into
delay slots for conditional branches from their destinations.
Also added chained branch removal which resulted from the above.
@
text
@d118 3
a120 1
	  (is_load andalso Set.is_member(rd, used_set))
d176 3
a178 3
      (if is_load then Set.singleton rd else Set.empty_set,
	 if is_load then main_uses
	 else Set.union(main_uses, Set.singleton rd))
@


1.3
log
@Finished off intra-block instruction shceduling
@
text
@d35 20
d91 1
d318 1
d327 1
d432 4
a435 1
	      fun copy_over(done, [], _) = Crash.impossible"Copy_over run out"
a443 7
	    (* Just continue looking. *)
	    (* Could be improved later to copy up to the instruction *)
	    (* requiring the delay slot *)
(*
	    reschedule_sections(hd opcode_list :: done,
				tl opcode_list, remainder, offset + 1)
*)
d465 151
@


1.2
log
@Added sequencing graph for opcodes, and started on code
to move useful instructions into delay slots.
@
text
@d67 4
d133 6
d159 2
a160 2
  | defines_and_uses(Sparc_Assembly.ARITHMETIC_AND_LOGICAL(_, rd, reg_or_imm,
							   rs1)) =
d165 26
d192 4
a195 1
      (Set.singleton rd, Set.list_to_set[rs1, rs2])
d204 1
a204 1
      (Set.singleton rd, Set.list_to_set[rs1, rs2])
d208 18
a225 6
  | defines_and_uses(Sparc_Assembly.SAVE_AND_RESTORE _) =
    (Set.empty_set, Set.empty_set)
  | defines_and_uses(Sparc_Assembly.BRANCH _) =
    (Set.empty_set, Set.empty_set)
  | defines_and_uses(Sparc_Assembly.BRANCH_ANNUL _) =
    (Set.empty_set, Set.empty_set)
d227 9
a235 3
    (Set.empty_set, Set.empty_set)
  | defines_and_uses(Sparc_Assembly.JUMP_AND_LINK _) =
    (Set.empty_set, Set.empty_set)
d296 1
a296 1
      val delay_pos = offset + delay_pos
d300 2
a301 1
	   "Found suitable target at offset " ^ Integer.makestring delay_pos ^
d323 1
a323 1
	    if pos >= delay_pos then
d329 2
d332 1
a332 1
		case edges_for_pos of
d334 10
a343 2
		    if start = pos andalso finish = delay_pos then
		      (pos, true, remaining_edges)
d356 1
a356 1
		    Integer.makestring delay_pos)
d360 66
a425 2
	  reschedule_sections(hd opcode_list :: done,
			      tl opcode_list, remainder, offset + 1)
d438 1
d440 3
a442 1
      val _ = reschedule_sections([], opcode_list, block_graph, 0)
d444 1
a444 4
      opcode_list (* Temporary *)
(*
      Crash.unimplemented"find_available_instruction"
*)
@


1.1
log
@Initial revision
@
text
@d8 3
d18 3
d31 4
d39 1
a39 1
		  (opcode as Sparc_Assembly.BRANCH_ANNUL(branch, _), _, _) ::
d46 16
d67 8
a74 2
  fun find_available_instruction(opcode_list, pos) =
    Crash.unimplemented"find_available_instruction"
d76 238
@
