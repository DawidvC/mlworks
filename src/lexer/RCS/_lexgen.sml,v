head	1.63;
access;
symbols
	MLW_daveb_inline_1_4_99:1.63.1
	MLWorks_21c0_1999_03_25:1.63
	MLWorks_20c1_1998_08_20:1.63
	MLWorks_20c0_1998_08_04:1.63
	MLWorks_20b2c2_1998_06_19:1.63
	MLWorks_20b2_Windows_1998_06_12:1.63
	MLWorks_20b1c1_1998_05_07:1.63
	MLWorks_20b0_1998_04_07:1.63
	MLWorks_20b0_1998_03_20:1.63
	MLWorks_20m2_1998_02_16:1.62
	MLWorks_20m1_1997_10_23:1.60
	MLWorks_11r1:1.59.5.1.1.1.1
	MLWorks_workspace_97:1.60.2
	MLWorks_dt_wizard:1.60.1
	MLWorks_11c0_1997_09_09:1.59.5.1.1.1
	MLWorks_10r3:1.59.5.1.3
	MLWorks_10r2_551:1.59.5.1.2
	MLWorks_11:1.59.5.1.1
	MLWorks_1_0_r2c2_1997_07_28:1.59.5.1
	MLWorks_20m0_1997_06_20:1.60
	MLWorks_1_0_r2c2_1997_06_14:1.59.5.1
	MLWorks_1_0_r2c1_released_1997_05_23:1.59.5.1
	MLWorks_1_0_r2c1_1997_05_12:1.59.5
	MLWorks_BugFix_1997_04_24:1.59
	MLWorks_1_0_r2_Win32_1997_04_11:1.59
	MLWorks_1_0_r2_Unix_1997_04_04:1.59
	MLWorks_1_0_1_ULTRASPARC_1997_02_24:1.59.3.1.1
	MLWorks_gui_1996_12_18:1.59.4
	MLWorks_1_0_Win32_1996_12_17:1.59.3
	MLWorks_1_0_Irix_1996_11_28_released_1996_12_17:1.59.1.1.1.1
	MLWorks_1_0_Unix_1996_11_14_released_1996_12_17:1.59.1.1
	MLWorks_1_0_Irix_1996_11_28:1.59.1.1.1
	MLWorks_1_0_Win32_1996_11_22:1.59.2
	MLWorks_1_0_Unix_1996_11_14:1.59.1
	MLWorks_Open_Beta2_1996_10_11:1.57.2
	MLWorks_License_dev:1.57.1
	MLWorks_1_open_beta_1996_09_13:1.55.2
	MLWorks_Open_Beta_1996_08_22:1.55
	MLWlexer_basis_io_1996:1.55.1
	MLWorks_Beta_1996_07_02:1.55
	MLWorks_Beta_1996_06_07:1.55
	MLWorks_Beta_1996_06_06:1.55
	MLWorks_Beta_1996_06_05:1.55
	MLWorks_Beta_1996_06_03:1.55
	MLWorks_Beta_1996_05_31:1.55
	MLWorks_Beta_1996_05_30:1.55
	ML_beta_release_12/08/94:1.44
	ML_beta_release_03/08/94:1.44
	ML_revised_beta_release_25/05/94:1.43
	ML_final_beta_release_02/03/94:1.42
	mlworks-28-01-1994:1.42
	Release:1.41
	mlworks-beta-01-09-1993:1.41
	MLWorks-1-0-4-29/01/1993:1.33
	MLWorks-1-0-3-21/12/1992:1.32
	MLWorks-1-0-2-15/12/1992:1.31
	MLWorks-1-0-1-04/12/1992:1.29
	checkpoint_17_08_92:1.16;
locks; strict;
comment	@ * @;


1.63
date	98.02.19.14.39.58;	author jont;	state Exp;
branches
	1.63.1.1;
next	1.62;

1.62
date	98.01.30.09.40.56;	author johnh;	state Exp;
branches;
next	1.61;

1.61
date	97.11.13.11.17.55;	author jont;	state Exp;
branches;
next	1.60;

1.60
date	97.05.28.12.05.35;	author daveb;	state Exp;
branches
	1.60.1.1
	1.60.2.1;
next	1.59;

1.59
date	96.11.06.10.55.14;	author matthew;	state Exp;
branches
	1.59.1.1
	1.59.2.1
	1.59.3.1
	1.59.4.1
	1.59.5.1;
next	1.58;

1.58
date	96.10.30.15.53.01;	author io;	state Exp;
branches;
next	1.57;

1.57
date	96.09.25.10.22.46;	author matthew;	state Exp;
branches
	1.57.1.1
	1.57.2.1;
next	1.56;

1.56
date	96.09.18.15.24.23;	author io;	state Exp;
branches;
next	1.55;

1.55
date	96.05.07.10.30.22;	author jont;	state Exp;
branches
	1.55.1.1
	1.55.2.1;
next	1.54;

1.54
date	96.04.30.17.36.53;	author jont;	state Exp;
branches;
next	1.53;

1.53
date	96.04.29.13.22.08;	author matthew;	state Exp;
branches;
next	1.52;

1.52
date	96.03.27.16.56.27;	author matthew;	state Exp;
branches;
next	1.51;

1.51
date	96.02.23.16.12.29;	author jont;	state Exp;
branches;
next	1.50;

1.50
date	95.10.10.13.14.10;	author daveb;	state Exp;
branches;
next	1.49;

1.49
date	95.03.17.12.13.29;	author matthew;	state Exp;
branches;
next	1.48;

1.48
date	95.02.23.18.20.59;	author matthew;	state Exp;
branches;
next	1.47;

1.47
date	94.10.24.13.44.43;	author matthew;	state Exp;
branches;
next	1.46;

1.46
date	94.08.25.15.12.50;	author matthew;	state Exp;
branches;
next	1.45;

1.45
date	94.08.25.15.00.03;	author matthew;	state Exp;
branches;
next	1.44;

1.44
date	94.07.22.15.10.37;	author matthew;	state Exp;
branches;
next	1.43;

1.43
date	94.03.08.14.41.50;	author daveb;	state Exp;
branches;
next	1.42;

1.42
date	93.12.23.13.25.06;	author daveb;	state Exp;
branches;
next	1.41;

1.41
date	93.08.12.13.31.22;	author jont;	state Exp;
branches
	1.41.1.1;
next	1.40;

1.40
date	93.06.16.10.42.51;	author matthew;	state Exp;
branches;
next	1.39;

1.39
date	93.06.09.16.25.14;	author matthew;	state Exp;
branches;
next	1.38;

1.38
date	93.05.18.16.15.37;	author jont;	state Exp;
branches;
next	1.37;

1.37
date	93.04.01.12.37.42;	author daveb;	state Exp;
branches;
next	1.36;

1.36
date	93.03.30.10.01.30;	author daveb;	state Exp;
branches;
next	1.35;

1.35
date	93.03.24.11.25.20;	author daveb;	state Exp;
branches;
next	1.34;

1.34
date	93.03.02.15.31.12;	author jont;	state Exp;
branches;
next	1.33;

1.33
date	93.01.15.14.10.55;	author jont;	state Exp;
branches;
next	1.32;

1.32
date	92.12.21.11.07.59;	author matthew;	state Exp;
branches;
next	1.31;

1.31
date	92.12.10.19.27.25;	author matthew;	state Exp;
branches;
next	1.30;

1.30
date	92.12.08.15.01.18;	author matthew;	state Exp;
branches;
next	1.29;

1.29
date	92.11.30.15.40.01;	author jont;	state Exp;
branches;
next	1.28;

1.28
date	92.11.20.13.47.19;	author matthew;	state Exp;
branches;
next	1.27;

1.27
date	92.11.19.14.33.10;	author matthew;	state Exp;
branches;
next	1.26;

1.26
date	92.11.17.14.25.32;	author matthew;	state Exp;
branches;
next	1.25;

1.25
date	92.11.09.18.38.22;	author daveb;	state Exp;
branches;
next	1.24;

1.24
date	92.10.30.16.38.31;	author matthew;	state Exp;
branches;
next	1.23;

1.23
date	92.10.14.11.30.52;	author richard;	state Exp;
branches;
next	1.22;

1.22
date	92.10.02.16.39.26;	author clive;	state Exp;
branches;
next	1.21;

1.21
date	92.09.04.08.36.06;	author richard;	state Exp;
branches;
next	1.20;

1.20
date	92.09.01.10.18.10;	author richard;	state Exp;
branches;
next	1.19;

1.19
date	92.08.31.17.04.01;	author richard;	state Exp;
branches;
next	1.18;

1.18
date	92.08.26.13.02.32;	author matthew;	state Exp;
branches;
next	1.17;

1.17
date	92.08.18.15.07.50;	author davidt;	state Exp;
branches;
next	1.16;

1.16
date	92.08.15.14.31.59;	author davidt;	state Exp;
branches;
next	1.15;

1.15
date	92.08.14.17.43.28;	author jont;	state Exp;
branches;
next	1.14;

1.14
date	92.08.05.14.40.42;	author jont;	state Exp;
branches;
next	1.13;

1.13
date	92.07.28.14.33.18;	author jont;	state Exp;
branches;
next	1.12;

1.12
date	92.07.28.11.49.07;	author matthew;	state Exp;
branches;
next	1.11;

1.11
date	92.05.19.17.03.29;	author clive;	state Exp;
branches;
next	1.10;

1.10
date	92.05.14.12.00.35;	author richard;	state Exp;
branches;
next	1.9;

1.9
date	92.05.06.10.35.06;	author richard;	state Exp;
branches;
next	1.8;

1.8
date	92.04.13.13.38.09;	author clive;	state Exp;
branches;
next	1.7;

1.7
date	92.04.02.12.13.07;	author matthew;	state Exp;
branches;
next	1.6;

1.6
date	92.03.23.15.27.04;	author matthew;	state Exp;
branches;
next	1.5;

1.5
date	92.03.10.12.26.02;	author matthew;	state Exp;
branches;
next	1.4;

1.4
date	92.01.31.18.23.48;	author jont;	state Exp;
branches;
next	1.3;

1.3
date	92.01.28.15.09.25;	author jont;	state Exp;
branches;
next	1.2;

1.2
date	91.10.11.13.36.25;	author davidt;	state Exp;
branches;
next	1.1;

1.1
date	91.09.06.16.48.36;	author nickh;	state Exp;
branches;
next	;

1.41.1.1
date	93.08.12.13.31.22;	author jont;	state Exp;
branches;
next	;

1.55.1.1
date	96.08.19.11.49.32;	author hope;	state Exp;
branches;
next	1.55.1.2;

1.55.1.2
date	96.08.19.12.13.33;	author davids;	state Exp;
branches;
next	1.55.1.3;

1.55.1.3
date	96.08.19.13.04.36;	author davids;	state Exp;
branches;
next	1.55.1.4;

1.55.1.4
date	96.08.20.13.28.57;	author davids;	state Exp;
branches;
next	1.55.1.5;

1.55.1.5
date	96.08.22.10.40.01;	author davids;	state Exp;
branches;
next	;

1.55.2.1
date	96.09.13.11.17.55;	author hope;	state Exp;
branches;
next	;

1.57.1.1
date	96.10.07.16.07.45;	author hope;	state Exp;
branches;
next	;

1.57.2.1
date	96.10.17.11.26.00;	author hope;	state Exp;
branches;
next	;

1.59.1.1
date	96.11.14.12.51.11;	author hope;	state Exp;
branches
	1.59.1.1.1.1;
next	;

1.59.1.1.1.1
date	96.11.28.15.02.14;	author hope;	state Exp;
branches;
next	;

1.59.2.1
date	96.11.22.18.10.27;	author hope;	state Exp;
branches;
next	;

1.59.3.1
date	96.12.17.17.49.03;	author hope;	state Exp;
branches
	1.59.3.1.1.1;
next	;

1.59.3.1.1.1
date	97.02.24.11.39.07;	author hope;	state Exp;
branches;
next	;

1.59.4.1
date	96.12.18.09.43.05;	author hope;	state Exp;
branches;
next	;

1.59.5.1
date	97.05.12.10.35.33;	author hope;	state Exp;
branches
	1.59.5.1.1.1
	1.59.5.1.2.1
	1.59.5.1.3.1;
next	;

1.59.5.1.1.1
date	97.07.28.18.20.55;	author daveb;	state Exp;
branches
	1.59.5.1.1.1.1.1;
next	;

1.59.5.1.1.1.1.1
date	97.10.07.11.46.18;	author jkbrook;	state Exp;
branches;
next	;

1.59.5.1.2.1
date	97.09.08.17.14.22;	author daveb;	state Exp;
branches;
next	;

1.59.5.1.3.1
date	97.09.09.14.10.08;	author daveb;	state Exp;
branches;
next	;

1.60.1.1
date	97.09.10.19.26.06;	author brucem;	state Exp;
branches;
next	;

1.60.2.1
date	97.09.11.20.56.12;	author daveb;	state Exp;
branches;
next	1.60.2.2;

1.60.2.2
date	97.11.20.17.08.59;	author daveb;	state Exp;
branches;
next	;

1.63.1.1
date	99.04.01.17.57.33;	author daveb;	state Exp;
branches;
next	;


desc
@Lexer generator. Here's the meat. Needs speeding up (takes 10 minutes CPU
for the ML lexer at present).
@


1.63
log
@[Bug #30341]
Fix where type ... and syntax
@
text
@(*
$Log: _lexgen.sml,v $
 * Revision 1.62  1998/01/30  09:40:56  johnh
 * [Bug #30326]
 * Merge in change from branch MLWorks_workspace_97
 *
 * Revision 1.61  1997/11/13  11:17:55  jont
 * [Bug #30089]
 * Modify TIMER (from utils) to be INTERNAL_TIMER to keep bootstrap happy
 * Revision 1.60.2.2  1997/11/20  17:08:59  daveb
 * [Bug #30326]
 *
 * Revision 1.60.2.1  1997/09/11  20:56:12  daveb
 * branched from trunk for label MLWorks_workspace_97
 *
 * Revision 1.60  1997/05/28  12:05:35  daveb
 * [Bug #30090]
 * Converted lexer to Basis IO.
 *
 * Revision 1.59  1996/11/06  10:55:14  matthew
 * [Bug #1728]
 * __integer becomes __int
 *
 * Revision 1.58  1996/10/30  15:53:01  io
 * moving String from toplevel
 *
 * Revision 1.57  1996/09/25  10:22:46  matthew
 * Take account of pushed back tokens in eof function
 *
 * Revision 1.56  1996/09/18  15:24:23  io
 * [Bug #1603]
 * convert MLWorks.ByteArray to MLWorks.Internal.ByteArray or equivalent basis functions
 *
 * Revision 1.55  1996/05/07  10:30:22  jont
 * Array moving to MLWorks.Array
 *
 * Revision 1.54  1996/04/30  17:36:53  jont
 * String functions explode, implode, chr and ord now only available from String
 * io functions and types
 * instream, oustream, open_in, open_out, close_in, close_out, input, output and end_of_stream
 * now only available from MLWorks.IO
 *
 * Revision 1.53  1996/04/29  13:22:08  matthew
 * Adding Int structure
 *
 * Revision 1.52  1996/03/27  16:56:27  matthew
 * Updating for new language definition
 *
 * Revision 1.51  1996/02/23  16:12:29  jont
 * newmap becomes map, NEWMAP becomes MAP
 *
 * Revision 1.50  1995/10/10  13:14:10  daveb
 * Removed spurious call to break.
 *
Revision 1.49  1995/03/17  12:13:29  matthew
Removing redundant requires

Revision 1.48  1995/02/23  18:20:59  matthew
Change to Option structure

Revision 1.47  1994/10/24  13:44:43  matthew
Use pervasive Option.option for return values in NewMap

Revision 1.46  1994/08/25  15:12:50  matthew
Commented out call to timer as can't run under NJ.

Revision 1.45  1994/08/25  15:00:03  matthew
Time making the lexer tables.
Bytearrays work now.

Revision 1.44  1994/07/22  15:10:37  matthew
Tried changing arrays to bytearrays.  This failed.  The code is commented out.

Revision 1.43  1994/03/08  14:41:50  daveb
Minor improvements to error messages.  First index of locations now
defined by values in Info.Location.

Revision 1.42  1993/12/23  13:25:06  daveb
Removed mkInteractiveTokenStream.

Revision 1.41  1993/08/12  13:31:22  jont
Modified to allow multiple ungets

Revision 1.40  1993/06/16  10:42:51  matthew
Added single character unGetToken function and lastToken function

Revision 1.39  1993/06/09  16:25:14  matthew
Added text_preprocess function

Revision 1.38  1993/05/18  16:15:37  jont
Removed integer parameter

Revision 1.37  1993/04/01  12:37:42  daveb
Removed the pushback facility of TokenStreams, as its never used.
Added the eof argument to mkLineTokenStream for use with the incremental
parser.

Revision 1.36  1993/03/30  10:01:30  daveb
getToken now takes a Lexerstate argument, and calls the appropriate
function for the state.  Thus we can lex comments and strings that extend
over multiple lines in the shell.
Tokenstream is now a datatype, to make type checking easier.

Revision 1.35  1993/03/24  11:25:20  daveb
lex now takes an options parameter, passed to it by getToken.

Revision 1.34  1993/03/02  15:31:12  jont
Some speed improvements

Revision 1.33  1993/01/15  14:10:55  jont
Modified to give ranges for locations, from start to end of token

Revision 1.32  1992/12/21  11:07:59  matthew
Change to allow token streams to be created with a given initial line number.

Revision 1.31  1992/12/10  19:27:25  matthew
Print non-printable characters sensibly in error messages.

Revision 1.30  1992/12/08  15:01:18  matthew
Hack to handle unclosed comments and strings

Revision 1.29  1992/11/30  15:40:01  jont
removed function involving map in favour of Lists.reducel

Revision 1.28  1992/11/20  13:47:19  matthew
Added an "unget" facility.

Revision 1.27  1992/11/19  14:33:10  matthew
Added flush_to_nl

Revision 1.26  1992/11/17  14:25:32  matthew
Changed Error structure to Info

Revision 1.25  1992/11/09  18:38:22  daveb
Added clear_eof function.

Revision 1.24  1992/10/30  16:38:31  matthew
Changed file token stream to not be interactive.

Revision 1.23  1992/10/14  11:30:52  richard
Added line number to token stream input functions.
Added mkFileTokenStream.

Revision 1.22  1992/10/02  16:39:26  clive
Change to NewMap.empty which now takes < and = functions instead of the single-function

Revision 1.21  1992/09/04  08:36:06  richard
Installed central error reporting mechanism.

Revision 1.20  1992/09/01  10:18:10  richard
Added missing require.

Revision 1.19  1992/08/31  17:04:01  richard
Replaced LexBasics error handler by proper global error handler,
and propagated more information through to the action functions
so that they can report error positions accurately.

Revision 1.18  1992/08/26  13:02:32  matthew
Added interactive slot to token streams.

Revision 1.17  1992/08/18  15:07:50  davidt
Made various changes to work with new inbuffer and ndfa signatures.

Revision 1.16  1992/08/15  14:31:59  davidt
Did a few optimisations and removed the stuff to do with the self reference.

Revision 1.15  1992/08/14  17:43:28  jont
Removed all currying from inbuffer

Revision 1.14  1992/08/05  14:40:42  jont
Removed some structures and sharing

Revision 1.13  1992/07/28  14:33:18  jont
Removed Array parameter, so it now uses pervasive Array.
Decurried numerous functions that didn't need it.

Revision 1.12  1992/07/28  11:49:07  matthew
Put in error message when EOF is encountered, and a token is being built.
This is intended for strings, but doesn't work as strings are returned as
a sequence of IGNORE tokens.

Revision 1.11  1992/05/19  17:03:29  clive
Fixed line position output from lexer

Revision 1.10  1992/05/14  12:00:35  richard
Added IGNORE token to remove recursion from lexing of comments and strings.

Revision 1.9  1992/05/06  10:35:06  richard
Changed BalancedTree to generic Map

Revision 1.8  1992/04/13  13:38:09  clive
First version of the profiler

Revision 1.7  1992/04/02  12:13:07  matthew
Changed EOF handling to allow tail recursion

Revision 1.6  1992/03/23  15:27:04  matthew
Added line numbering

Revision 1.5  1992/03/10  12:26:02  matthew
Errors signalling changed to call report_lex_error and continue rather than raise
an exception.

Revision 1.4  1992/01/31  18:23:48  jont
Removed use of myarray, replaced by array.sml

Revision 1.3  1992/01/28  15:09:25  jont
Added type information to allow our typechecker to elaborate it

Revision 1.2  1991/10/11  13:36:25  davidt
Major modifications, including a more abstract implementation of ndfa's
(more efficient as well). This generator is still pretty slow compared
to `flex' and other generators written in C but I don't want to do any
more optimisations without a profiler. Currently the subsets generated
by the subset construction are sorted using quicksort, this could actually
be done on O(n) time because we know the range of numbers we are sorting
(0 ... number of states in ndfa) and hence we could use an array sort
(or whatever its called). However, I'm not convinced this is where all
the time is being spent. Replacing balanced trees with hash tables
might well give a bigger performance win.

Revision 1.1  91/09/06  16:48:36  nickh
Initial revision

Copyright (c) 1991 Harlequin Ltd.
*)

require "../basis/__int";
require "../basis/__text_io";
require "../basis/__text_prim_io";

require "../utils/lists";
require "../utils/map";
require "../utils/mlworks_timer";
require "../basics/token";
require "ndfa";
require "lexrules";
require "lexer";

functor LexGen (structure Lists : LISTS
		structure Map : MAP
                structure Timer : INTERNAL_TIMER
		structure Token : TOKEN
		structure Ndfa : NDFA where type action = int where type state = int
		structure LexRules : LEXRULES
                sharing type LexRules.Result = Token.Token
                  
                  ) : LEXER = 
struct
  structure Token = Token
  structure RegExp = LexRules.RegExp
  structure Info = LexRules.Info
  structure InBuffer = LexRules.InBuffer
  
  type Options = LexRules.options
  type Result = LexRules.Result

  local
    val column = ref 0
  in
    fun printDot () =
      let
	val c = !column
      in
	if c = 70 then
	  (print ".\n"; column := 0)
	else
	  (print "."; column := c + 1)
      end
  end

  (* returns a counting function from m+1 *)
	
  fun counter m = let val n = ref m in fn () => (n := (!n)+1; !n) end
  
  (* insertion-sort on int lists *)
  
  fun quicksort ([], accum) = accum
    | quicksort (pivot :: rest, accum) = 
      partition (pivot, [], [], rest, accum)

  and partition (pivot, left, right, [], accum) =
    quicksort (left, pivot :: quicksort (right, accum))
    | partition (pivot, left, right, y::ys, accum) =
      if (y : int) < pivot then partition (pivot, y :: left, right, ys, accum)
      else if y > pivot then partition (pivot, left, y :: right, ys, accum)
	   else partition (pivot, left, right, ys, accum)

  val canonical = fn L => quicksort (L,[]) 

  (* first int is the node number, which we use to form subsets.  second
   int is the action number (there is a fixed list of actions), which we
   use to do comparisons. This is 0 if this node is non-accepting, and
   higher action numbers take precedence *)
      
  (* the first int is the node number, the second int is the action number *)
      
  datatype DfaNode = D of MLWorks.Internal.ByteArray.bytearray * int * int
      
  (* The int array array is the transitions, the int array is the
   actions. The initial state is always state 1, the unreachable state is
   always state 0 *)
      
  datatype Dfa = DFA of MLWorks.Internal.ByteArray.bytearray MLWorks.Internal.Array.array * int MLWorks.Internal.Array.array
      
  (*
   trans_subset returns the node-numbers of all the transitions from all
   the nodes in the subset
   *)
      
  fun loop (_, _, [], accum) = accum
    | loop (char, ndfa, state :: rest, accum) =
      loop (char, ndfa, rest, Ndfa.get_char (char, Ndfa.transitions (ndfa,state), accum))

  fun trans_subset arg (*(char, ndfa, subset)*) =
    Lists.filter(loop arg(*(char, ndfa, subset,[])*))

  (* epsclosure: returns the epsilon-closure of the subset in the ndfa *)
      
  fun epsclosure'(_, [], subset) = subset
    | epsclosure'(ndfa, state :: rest, subset) =
      let
	val new = Lists.filter (Ndfa.get_epsilon (Ndfa.transitions (ndfa, state), rest))
      in
	epsclosure'(ndfa, new, state :: subset)
      end

  fun epsclosure arg = canonical (epsclosure' arg)

  (*
   Returns the highest action number from a list of node numbers. Need
   this because when we transform to DFA, this action is the one we want
   for each subset node. If none of the nodes have an action, return 0.
  *)
      
  fun best_action(ndfa, l) =
    Lists.reducel
    (fn (m, state) =>
     let
       val an = Ndfa.action(ndfa, state)
     in
       if an > m then an else m
     end)
    (0, l)
			   
  (*
   This function will consider the empty state (corresponding to no
   NDFA states) as distinct from other states, so we should get
   termination for free by checking for that state. Alternatively, it
   would be nice to spot that state separately so that we don't have to
   record transitions to it in the list. Alternatively too, it doesn't
   matter since this DFA is only temporary.
  *)

  local      
    fun loop (n :: ns, res:int) = loop (ns, n + res)
      | loop ([], res) = res
    val total = fn L => loop (L,0) 
  in
    fun transform ndfa =
      let
	val nextnode = counter 1
	val init = epsclosure(ndfa, [Ndfa.start ndfa], [])
	val unmarkedstates = (ref []) : (int list * int) list ref
	(* state zero is empty (terminating), state 1 is initial *)
	val markedstates = (ref [D (MLWorks.Internal.ByteArray.array(256,0),0,0)]) : DfaNode list ref
	val currentstate = ref (init,1)
	(* keep a `hash-value' (the total) with each state to speed searching *)

	local
	  fun loop ([],[]) = false
	    | loop (_ ,[]) = false
	    | loop ([], _) = true
	    | loop ((h1:int)::t1,h2::t2) =
	      if h1 < h2 then true
	      else if h1 > h2 then false
		   else loop (t1,t2)
	in
	  fun ordering ((hash1:int, subset1), (hash2, subset2)) =
	    if hash1 < hash2 then true
	    else if hash1 > hash2 then false
		 else loop (subset1, subset2)
	end

	local
	  fun loop ([],[]) = true
	    | loop (_ ,[]) = false
	    | loop ([], _) = false
	    | loop ((h1:int)::t1,h2::t2) =
	      if h1 < h2 then false
	      else if h1 > h2 then false
		   else loop (t1,t2)
	in
	  fun ordering_eq ((hash1:int, subset1), (hash2, subset2)) =
	    if hash1 < hash2 then false
	    else if hash1 > hash2 then false
		 else loop (subset1, subset2)
	end

	val states = ref (Map.from_list (ordering,ordering_eq) [((0, []), 0), ((total init, init), 1)])
	
	(* Adds a state to the unmarked list and gives it a number *)
	fun addstate (t,l) =
	  let
	    val nodeno = nextnode ()
	    val actno = best_action(ndfa, l)
	  in
	    unmarkedstates := (l,nodeno) :: (!unmarkedstates);
	    states := Map.define (!states, (t,l), nodeno);
	    printDot();
	    nodeno
	  end

	(* Returns number of this state, adding if necessary *)
	fun find subset =
	  let
	    val t = total subset
	  in
	    case Map.tryApply'(!states, (t,subset)) of
	      SOME answer => answer
	    | _ => addstate(t,subset)
	  end

	(* Returns the transition table: an int array *)
	fun loop (res, c, subset, ndfa) =
	  if c < 0 then res
	  else loop(find(epsclosure(ndfa, trans_subset(c, ndfa, subset, []), [])) ::
		    res, c-1, subset, ndfa)

        (* If we are using bytearrays, better check this *)
        exception NotAByte
        fun check [] = ()
          | check (n::rest) = 
            if n < 0 orelse n > 255
              then raise NotAByte
            else check rest

 	fun transtable subset = 
          let
            val elements = (loop([], 255, subset, ndfa))
            val _ = check elements
          in
(*
            app
            (fn n => output(std_out,Int.toString n ^ ", "))
            elements;
            output(std_out,"\n");
*)
            MLWorks.Internal.ByteArray.arrayoflist elements
          end

	(* Recurses down the list of unmarked states.... *)
	fun doit () =
	  let
	    val (s,n) = !currentstate
	  in
	    (markedstates := (D(transtable s,n,best_action(ndfa, s))) :: (!markedstates);
	     (fn [] => () | (s::ss) => (unmarkedstates := ss;
					currentstate := s;
					doit ()))
	     (!unmarkedstates))
	  end
	(* I can't find a better way to format this.
	 We are applying the fnexp to !unmarkedstates *)
	val _ = doit ()
	val maxnode = nextnode () 
	val trans = MLWorks.Internal.Array.array (maxnode,MLWorks.Internal.ByteArray.array(0,0))
	val actions = MLWorks.Internal.Array.array(maxnode,0)
	fun addit (D(t,n,a)) =
	  (MLWorks.Internal.Array.update(trans,n,t); MLWorks.Internal.Array.update(actions,n,a))
	val _ = app addit (!markedstates)
      in
	DFA(trans,actions)
      end
  end
				      
  (* So now we have ndfa's, dfa's, and a function from one to the other.
   Now we need to get regexps into ndfa's, glue the ndfa's together,
   transform into a dfa, and keep the actions all the way through *)
   
  (* first a function regexp * node list * int * counter -> node list * int
     
   (adds nodes to the list, returning a new list which excludes the start
    and end nodes).  The int arguments is final node number, the int result
    is initial node number (The function generates a number for the
    initial node, and adds it to the list). This uses the constructions
    from the Dragon book (well, sort of). *)
         
  fun re2ndfa regexp ndfa =
    case regexp of
      RegExp.EPSILON =>
	Ndfa.add(ndfa, Ndfa.epsilon [Ndfa.start ndfa])
    | RegExp.NODE s =>
	let
	  fun loop (res, x) =
	    if x < 0 then res
	    else loop(Ndfa.add (res, Ndfa.single_char (MLWorks.String.ordof(s, x), Ndfa.start res)), x-1)
	in
	  loop(ndfa, size s - 1)
	end
    | RegExp.CLASS s =>
	let
	  val start = Ndfa.start ndfa
	    
	  fun loop (res, x) =
	    if x < 0 then Ndfa.mk_trans res
	    else loop ((MLWorks.String.ordof(s, x), start) :: res, x-1)
	in
	  Ndfa.add(ndfa, loop([], size s - 1))
	end
    | RegExp.BAR(s,t) =>
	let
	  val ndfa1 = re2ndfa s ndfa
	  val ndfa2 = re2ndfa t (Ndfa.set_start(ndfa1, Ndfa.start ndfa))
	  val transitions = Ndfa.epsilon [Ndfa.start ndfa1, Ndfa.start ndfa2]
	in
	  Ndfa.add (ndfa2, transitions)
	end
    | RegExp.DOT(s,t) =>
	re2ndfa s (re2ndfa t ndfa)
    | RegExp.STAR s =>
	Ndfa.add_rec (ndfa, re2ndfa s)

    (* Now a function regexp * action * node list * counter -> node list * int
     
     This adds the regexp with the given action, to the node list,
     returning the list and the initial node *)
   
    (* Now a function regexp list * counter -> node list * int list * int,
     where the int list is the list of initial nodes and the int is one
     more than the number of actions. The resulting actions start with 1 at
     the tail of the list of regexps *)
   
    fun convert_regexps [] = (Ndfa.empty, [], 1)
      | convert_regexps (regexp :: rest) =
	let
	  val (ndfa, initials, action) = convert_regexps rest
	  val ndfa' = Ndfa.add_final (ndfa, action)
	  val ndfa'' = re2ndfa regexp ndfa'
	in
	  (ndfa'', (Ndfa.start ndfa'') :: initials, action + 1)
	end
			
    (* Now a function (regexp * (string -> result)) list -> ndfa * ((string -> result) array) *)
  
    fun convert_rules rules =
      let
	val (regexps, actions) = Lists.unzip rules
	val (ndfa, initials, _) = convert_regexps regexps
	val ndfa' = Ndfa.add_start (ndfa, initials)
      in
	(ndfa', MLWorks.Internal.Array.arrayoflist (rev actions))
      end
		   
    (* last but by no means least, (regexp * (string -> result)) list -> dfa * ((string -> result) array) *)

    fun make_dfa rules =
      let
	val (ndfa, actions) = convert_rules rules
	val dfa as DFA(trans, _) = transform ndfa
      in
	print("\nDFA has " ^ Int.toString(MLWorks.Internal.Array.length trans) ^ " states\n");
	(dfa, actions)
      end

(* NJ doesn't like this as the timer functions are unimplemented *)

(*
    val make_dfa =
      fn rules => Timer.xtime ("Making DFA",true,fn () => make_dfa rules)
*)

    (* now lexing itself. lex makes a dfa-walker *)

    fun chr_to_string n =
      if n >= ord #" " andalso n < 127
          then (str o chr) n
      else
        if n < (ord #" ")
          then "^" ^ (str o chr) (n + 64)
        else
          "\\" ^ Int.toString n

    (* Change so that single tokens can be pushed back onto the stream *)
    (* The Token list ref is the list of last tokens read *)

    datatype TokenStream =
      TOKEN_STREAM of 
       {buffer : InBuffer.InBuffer, 
        source_name : string, 
        interactive : bool,
	line_and_col : (int * int) ref, 
        pushed_back : (Token.Token * Info.Location.T) list ref}
      
    fun lex (dfa as DFA (trans,action_numbers), actions)
	    ((error_info, options),
	     ts as TOKEN_STREAM {buffer, source_name = filename,...}
	    ) =
      let
	val startpoint = InBuffer.getpos buffer
        val location =
          Info.Location.POSITION (filename, InBuffer.getlinenum buffer, InBuffer.getlinepos buffer)

	fun lex1 (string,state,finishaction,finishstring,finishpoint,found_one_earlier) =
	  if InBuffer.eof buffer then
	    if found_one_earlier then
	      (InBuffer.position(buffer, finishpoint);
	       MLWorks.Internal.Array.sub (actions,finishaction-1) (location, buffer, finishstring, (error_info, options))
	      )
	    else
              (case string of
                 [] => ()
               | _ =>
                   Info.error
                   error_info
                   (Info.RECOVERABLE, location,
                    "Unexpected end of file");
               LexRules.eof)
	  else
	    let
	      val c = InBuffer.getchar buffer
	      val string = c :: string
	      val newstate = MLWorks.Internal.ByteArray.sub (MLWorks.Internal.Array.sub (trans,state), c)
	    in
	      if newstate = 0 then
		if found_one_earlier then
		  (InBuffer.position(buffer, finishpoint);
		   MLWorks.Internal.Array.sub (actions,finishaction-1) (location, buffer, finishstring, (error_info,options))
		  )
		else
		  (InBuffer.position(buffer, startpoint);
		   let
		     val discard_char = InBuffer.getchar buffer
		   in
                     Info.error
                     error_info
                     (Info.RECOVERABLE, 
                      location, 
                      "Illegal character `" ^ (chr_to_string discard_char) ^ "'");
		     lex (dfa,actions) ((error_info,options), ts)
		   end)
	      else
		let
		  val act = MLWorks.Internal.Array.sub(action_numbers,newstate)
		in
		  if act > 0 then
		    lex1(string,newstate,act,string,InBuffer.getpos buffer,true)
		  else
                    lex1(string,newstate,finishaction,finishstring,finishpoint,found_one_earlier)
		end
	    end

          fun unexpected _ =
            Info.error' error_info
			(Info.FAULT, location, "Unexpected lexical error")
      in
	lex1([], 1, ~1, [], startpoint, false)
      end

    (* lexer takes a (regexp * action) list and produces buffer -> result *)
    (* added an "interactive" boolean *)

    val lexer = (lex o make_dfa) LexRules.rules

    fun fix_input f = (!MLWorks.Internal.text_preprocess) f

    fun mkTokenStream (f, name_of_file) =
      let
	val buffer = InBuffer.mkInBuffer (fix_input f)
      in
	TOKEN_STREAM{buffer=buffer, source_name=name_of_file, interactive=false,
		     line_and_col=
   		       ref(Info.Location.first_line, Info.Location.first_col),
		     pushed_back=ref []}
      end
    
    fun mkLineTokenStream (f, name_of_file, line, eof) =
      let
	val buffer = InBuffer.mkLineInBuffer (fix_input f, line, eof)
      in
	TOKEN_STREAM{buffer=buffer, source_name=name_of_file, interactive=false,
		     line_and_col=ref(line,Info.Location.first_col),
		     pushed_back=ref []}
      end
    
    fun mkFileTokenStream (instream, name_of_file) =
      let
        val (TextPrimIO.RD {readVec, ...}, vec) =
          TextIO.StreamIO.getReader (TextIO.getInstream instream)

        fun input_fn _ = case readVec of
          NONE => raise Fail "readVec not supported.\n"
        | SOME rv => rv 4096
        (* The figure of 4096 has been empirically determined for the lexer. *)
      in
	mkTokenStream (input_fn, name_of_file)
      end
    
    (* The action taken by getToken depends on the lexer state.  IGNORE tokens
       are dealt with here, both for abstraction and for efficiency.  Other
       tokens are returned.  EOFs in comments or strings are handled
       appropriately by the parser functions in ../parser/_parser. *)
    fun getToken error_info
      (options, Token.IN_COMMENT n,
       ts as TOKEN_STREAM{buffer=b, source_name=file, ...}) =
      (case LexRules.read_comment (b, n) of
         Token.IGNORE =>
           getToken error_info (options, Token.PLAIN_STATE, ts)
       | t => t)
      | getToken error_info
	(options, Token.IN_STRING s,
	 ts as TOKEN_STREAM{buffer=b, source_name=file, line_and_col=ref (line, col),
			     ...}) =
	let
	  val result =
	    LexRules.continue_string
	    (Info.Location.EXTENT {name = file, s_line = line, s_col = col,
				   e_line = line, e_col = col},
	     b, error_info, s)
	in
	  result
	end
      | getToken error_info (options, Token.PLAIN_STATE,
			     ts as TOKEN_STREAM{buffer=b, pushed_back=list as ref ((x,_) :: xs),
                                                ...}) =
        (list := xs;
	 x)

      | getToken error_info (options, Token.PLAIN_STATE,
			     ts as TOKEN_STREAM{buffer=b, line_and_col=loc, pushed_back=ref [],
						...}) =
          let
            fun get Token.IGNORE =
	      (loc := (InBuffer.getlinenum b, InBuffer.getlinepos b);
	       get (lexer ((error_info, options), ts))
	      )
	      |get other = other
	    val result = get Token.IGNORE
          in
	    result
          end

    fun associated_filename (TOKEN_STREAM{source_name, ...}) = source_name
    fun locate (TOKEN_STREAM {buffer, source_name, pushed_back,line_and_col,...}) =
      case !pushed_back of
        (_,loc) :: _ => loc
      | _ =>
          let
            val ref (s_line, s_col) = line_and_col
          in
            Info.Location.EXTENT
            {name=source_name, s_line=s_line, s_col=s_col,
             e_line=InBuffer.getlinenum buffer, e_col=InBuffer.getlinepos buffer}
          end
    fun eof (TOKEN_STREAM{buffer=b, pushed_back, ...}) = 
      !pushed_back = [] andalso InBuffer.eof b
    fun clear_eof (TOKEN_STREAM{buffer=b, ...}) = InBuffer.clear_eof b
    fun is_interactive (TOKEN_STREAM{interactive, ...}) = interactive
    fun flush_to_nl (TOKEN_STREAM{buffer=b, ...}) = InBuffer.flush_to_nl b

    fun ungetToken (tokloc, TOKEN_STREAM{pushed_back, ...}) =
      pushed_back := tokloc :: !pushed_back

  end
@


1.63.1.1
log
@branched from trunk for label MLW_daveb_inline_1_4_99
@
text
@a2 4
 * Revision 1.63  1998/02/19  14:39:58  jont
 * [Bug #30341]
 * Fix where type ... and syntax
 *
@


1.62
log
@[Bug #30326]
Merge in change from branch MLWorks_workspace_97
@
text
@d3 4
d244 1
a244 1
		structure Ndfa : NDFA where type action = int and state = int
@


1.61
log
@[Bug #30089]
Modify TIMER (from utils) to be INTERNAL_TIMER to keep bootstrap happy
@
text
@d3 9
d230 1
a230 1
require "../utils/timer";
@


1.60
log
@[Bug #30090]
Converted lexer to Basis IO.
@
text
@d3 4
d229 1
a229 1
                structure Timer : TIMER
@


1.60.2.1
log
@branched from trunk for label MLWorks_workspace_97
@
text
@a2 4
 * Revision 1.60  1997/05/28  12:05:35  daveb
 * [Bug #30090]
 * Converted lexer to Basis IO.
 *
@


1.60.2.2
log
@[Bug #30326]
@
text
@a2 3
 * Revision 1.60.2.1  1997/09/11  20:56:12  daveb
 * branched from trunk for label MLWorks_workspace_97
 *
d221 1
a221 1
require "../utils/mlworks_timer";
@


1.60.1.1
log
@branched from trunk for label MLWorks_dt_wizard
@
text
@a2 4
 * Revision 1.60  1997/05/28  12:05:35  daveb
 * [Bug #30090]
 * Converted lexer to Basis IO.
 *
@


1.59
log
@[Bug #1728]
__integer becomes __int
@
text
@d3 4
d212 2
a237 15
  fun print s = MLWorks.IO.output (MLWorks.IO.std_out,s)

  type bytearray = MLWorks.Internal.ByteArray.bytearray
    
  val array = MLWorks.Internal.ByteArray.array
    
  (* bytearrays save a lot of space 
   * type array_type = MLWorks.Internal.ByteArray.bytearray
   * val array_array = MLWorks.Internal.ByteArray.array
   * val array_arrayoflist = MLWorks.Internal.ByteArray.arrayoflist
   *
   * I tried making this unsafe sub, but there is no speedup
   * val array_sub = ByteArray.sub
   *)

d655 2
a656 1
		     line_and_col=ref(Info.Location.first_line, Info.Location.first_col),
a668 1

d671 6
a677 1
        val buffer = InBuffer.mkInBuffer (fix_input (fn _ => MLWorks.IO.input (instream, 4096)))
d679 1
a679 3
	TOKEN_STREAM{buffer=buffer, source_name=name_of_file, interactive=false,
		     line_and_col=ref(Info.Location.first_line, Info.Location.first_col),
		     pushed_back=ref []}
@


1.59.5.1
log
@branched from 1.59
@
text
@a2 4
 * Revision 1.59  1996/11/06  10:55:14  matthew
 * [Bug #1728]
 * __integer becomes __int
 *
@


1.59.5.1.3.1
log
@branched from MLWorks_1_0_r2c1_1997_05_12 for label MLWorks_10r3
@
text
@a2 3
 * Revision 1.59.5.1  1997/05/12  10:35:33  hope
 * branched from 1.59
 *
@


1.59.5.1.2.1
log
@branched from MLWorks_1_0_r2c1_1997_05_12 for label MLWorks_10r2_551
@
text
@a2 3
 * Revision 1.59.5.1  1997/05/12  10:35:33  hope
 * branched from 1.59
 *
@


1.59.5.1.1.1
log
@branched from MLWorks_1_0_r2c1_1997_05_12 for label MLWorks_11
@
text
@a2 3
 * Revision 1.59.5.1  1997/05/12  10:35:33  hope
 * branched from 1.59
 *
@


1.59.5.1.1.1.1.1
log
@branched from MLWorks_11 for label MLWorks_11r1
@
text
@a2 3
 * Revision 1.59.5.1.1.1  1997/07/28  18:20:55  daveb
 * branched from MLWorks_1_0_r2c1_1997_05_12 for label MLWorks_11
 *
@


1.59.4.1
log
@branched from 1.59
@
text
@a2 4
 * Revision 1.59  1996/11/06  10:55:14  matthew
 * [Bug #1728]
 * __integer becomes __int
 *
@


1.59.3.1
log
@branched from 1.59
@
text
@a2 4
 * Revision 1.59  1996/11/06  10:55:14  matthew
 * [Bug #1728]
 * __integer becomes __int
 *
@


1.59.3.1.1.1
log
@branched from 1.59.3.1
@
text
@a2 3
 * Revision 1.59.3.1  1996/12/17  17:49:03  hope
 * branched from 1.59
 *
@


1.59.2.1
log
@branched from 1.59
@
text
@a2 4
 * Revision 1.59  1996/11/06  10:55:14  matthew
 * [Bug #1728]
 * __integer becomes __int
 *
@


1.59.1.1
log
@branched from 1.59
@
text
@a2 4
 * Revision 1.59  1996/11/06  10:55:14  matthew
 * [Bug #1728]
 * __integer becomes __int
 *
@


1.59.1.1.1.1
log
@branched from 1.59.1.1
@
text
@a2 3
 * Revision 1.59.1.1  1996/11/14  12:51:11  hope
 * branched from 1.59
 *
@


1.58
log
@moving String from toplevel
@
text
@d3 3
d207 1
a207 1
require "../basis/__integer";
@


1.57
log
@Take account of pushed back tokens in eof function
@
text
@d3 3
d409 1
a409 1
	      MLWorks.Option.SOME answer => answer
d433 1
a433 1
            Lists.iterate
d460 1
a460 1
	val _ = Lists.iterate addit (!markedstates)
d486 1
a486 1
	    else loop(Ndfa.add (res, Ndfa.single_char (String.ordof(s, x), Ndfa.start res)), x-1)
d496 1
a496 1
	    else loop ((String.ordof(s, x), start) :: res, x-1)
d565 2
a566 2
      if n >= String.ord " " andalso n < 127
          then String.chr n
d568 2
a569 2
        if n < (String.ord " ")
          then "^" ^ String.chr(n + 64)
@


1.57.2.1
log
@branched from 1.57
@
text
@a2 3
 * Revision 1.57  1996/09/25  10:22:46  matthew
 * Take account of pushed back tokens in eof function
 *
@


1.57.1.1
log
@branched from 1.57
@
text
@a2 3
 * Revision 1.57  1996/09/25  10:22:46  matthew
 * Take account of pushed back tokens in eof function
 *
@


1.56
log
@[Bug #1603]
convert MLWorks.ByteArray to MLWorks.Internal.ByteArray or equivalent basis functions
@
text
@d3 4
d575 5
a579 3
       {buffer : InBuffer.InBuffer, source_name : string, interactive : bool,
	line_and_col : (int * int) ref, pushed_back : Token.Token list ref,
	last_token : Token.Token ref}
d659 1
a659 2
		     pushed_back=ref ([]: Token.Token list),
		     last_token=ref Token.IGNORE}
d668 1
a668 2
		     pushed_back=ref ([]: Token.Token list),
		     last_token=ref Token.IGNORE}
d679 1
a679 2
		     pushed_back=ref ([]: Token.Token list),
		     last_token=ref Token.IGNORE}
d688 5
a692 11
       ts as TOKEN_STREAM{buffer=b, source_name=file, last_token, ...}) =
      let
	val result =
	  (case LexRules.read_comment (b, n) of
	     Token.IGNORE =>
	       getToken error_info (options, Token.PLAIN_STATE, ts)
	   | t => t)
      in
	last_token := result;
	result
      end
d696 1
a696 1
			    last_token, ...}) =
a703 1
	  last_token := result;
d707 2
a708 2
			     ts as TOKEN_STREAM{buffer=b, pushed_back=list as ref (x :: xs),
						last_token, ...}) =
a709 1
	 last_token := x;
d714 1
a714 1
						last_token, ...}) =
a722 1
	    last_token := result;
d727 13
a739 5
    fun locate (TOKEN_STREAM {buffer=b, source_name=s, line_and_col=ref(s_line, s_col),...})=
	Info.Location.EXTENT
	{name=s, s_line=s_line, s_col=s_col,
	 e_line=InBuffer.getlinenum b, e_col=InBuffer.getlinepos b}
    fun eof (TOKEN_STREAM{buffer=b, ...}) = InBuffer.eof b
d744 2
a745 2
    fun ungetToken (token, TOKEN_STREAM{pushed_back, ...}) =
      pushed_back := token :: !pushed_back
a746 2
    fun lastToken (TOKEN_STREAM{last_token = ref tok, ...}) = tok
                    
@


1.55
log
@Array moving to MLWorks.Array
@
text
@d3 3
d221 1
a221 3

  structure ByteArray = MLWorks.ByteArray

d224 12
a235 6
  (* bytearrays save a lot of space *)
  type array_type = ByteArray.bytearray
  val array_array = ByteArray.array
  val array_arrayoflist = ByteArray.arrayoflist
  (* I tried making this unsafe sub, but there is no speedup *)
  val array_sub = ByteArray.sub
d280 1
a280 1
  datatype DfaNode = D of array_type * int * int
d286 1
a286 1
  datatype Dfa = DFA of array_type MLWorks.Internal.Array.array * int MLWorks.Internal.Array.array
d348 1
a348 1
	val markedstates = (ref [D (array_array(256,0),0,0)]) : DfaNode list ref
d431 1
a431 1
            array_arrayoflist elements
d449 1
a449 1
	val trans = MLWorks.Internal.Array.array (maxnode,array_array(0,0))
d603 1
a603 1
	      val newstate = array_sub (MLWorks.Internal.Array.sub (trans,state), c)
@


1.55.2.1
log
@branched from 1.55
@
text
@a2 3
 * Revision 1.55  1996/05/07  10:30:22  jont
 * Array moving to MLWorks.Array
 *
@


1.55.1.1
log
@branched from 1.55
@
text
@a2 3
 * Revision 1.55  1996/05/07  10:30:22  jont
 * Array moving to MLWorks.Array
 *
@


1.55.1.2
log
@Changed arrays to use the basis library.
@
text
@a2 3
 * Revision 1.55.1.1  1996/08/19  11:49:32  hope
 * branched from 1.55
 *
a197 1
require "../basis/__array";
d224 2
d254 1
a254 1
  (* Quicksort on int lists *)
d282 1
a282 1
  datatype Dfa = DFA of array_type Array.array * int Array.array
d445 2
a446 2
	val trans = Array.array (maxnode,array_array(0,0))
	val actions = Array.array(maxnode,0)
d448 1
a448 2
	  (Array.update(trans,n,t);
	   Array.update(actions,n,a))
d530 1
a530 1
	(ndfa', Array.fromList (rev actions))
d540 1
a540 1
	print("\nDFA has " ^ Int.toString(Array.length trans) ^ " states\n");
d584 1
a584 1
	       Array.sub (actions,finishaction-1) (location, buffer, finishstring, (error_info, options))
d599 1
a599 1
	      val newstate = array_sub (Array.sub (trans,state), c)
d604 1
a604 1
		   Array.sub (actions,finishaction-1) (location, buffer, finishstring, (error_info,options))
d620 1
a620 1
		  val act = Array.sub(action_numbers,newstate)
@


1.55.1.3
log
@Changed to make strings use basis library String and Char structures.
@
text
@a201 2
require "../basis/__char";
require "../basis/__string";
d400 1
a400 1
	      SOME answer => answer
d478 1
a478 1
	    else loop (Ndfa.add (res, Ndfa.single_char (Char.ord (String.sub (s, x)), Ndfa.start res)), x-1)
d488 1
a488 1
	    else loop ((Char.ord (String.sub (s, x)), start) :: res, x-1)
d556 9
d618 1
a618 2
                      "Illegal character `" ^ 
		      (Char.toString (Char.chr discard_char)) ^ "'");
@


1.55.1.4
log
@Change IO to use basis library.
@
text
@a2 4
 * Revision 1.55.1.3  1996/08/19  13:04:36  davids
 * Changed arrays to use basis library Array structure.
 * String manipulations now use basis library String and Char structures.
 *
d226 1
a226 1
  structure Position = LexRules.Position
d563 1
a563 1
       {strmpos: Position.stream_pos, source_name : string, interactive : bool,
d569 1
a569 1
	     ts as TOKEN_STREAM {strmpos, source_name = filename,...}
d572 6
a577 8
	val startpoint = Position.getpos strmpos
        val location = 
	  Info.Location.POSITION (filename, Position.getlinenum strmpos,
				  Position.getlinepos strmpos)

	fun lex1 (string,state,finishaction,finishstring,
		  finishpoint,found_one_earlier) =
	  if Position.eof strmpos then
d579 2
a580 2
	      (Position.position(strmpos, finishpoint);
	       Array.sub (actions,finishaction-1) (location, strmpos, finishstring, (error_info, options))
d593 1
a593 1
	      val c = Position.getchar strmpos
d599 2
a600 2
		  (Position.position(strmpos, finishpoint);
		   Array.sub (actions,finishaction-1) (location, strmpos, finishstring, (error_info,options))
d603 1
a603 1
		  (Position.position(strmpos, startpoint);
d605 1
a605 1
		     val discard_char = Position.getchar strmpos
d620 1
a620 1
		    lex1(string,newstate,act,string,Position.getpos strmpos,true)
d633 1
a633 1
    (* lexer takes a (regexp * action) list and produces strmpos -> result *)
d638 3
a640 1
    fun mkFileTokenStream (instrm, name_of_file) =
d642 1
a642 1
        val strmpos = Position.mkStreamPos (instrm)
d644 2
a645 5
	TOKEN_STREAM{strmpos=strmpos, 
		     source_name=name_of_file,
		     interactive=false,
		     line_and_col=ref(Info.Location.first_line,
				      Info.Location.first_col),
d649 2
a650 2

    fun mkTokenStream (str, name_of_file) =
d652 1
a652 1
	val strmpos = Position.mkFromString (str)
d654 2
a655 5
	TOKEN_STREAM{strmpos=strmpos, 
		     source_name=name_of_file,
		     interactive=false,
		     line_and_col=ref(Info.Location.first_line,
				      Info.Location.first_col),
d659 1
d661 1
a661 1
    fun mkLineTokenStream (line_string, name_of_file, line_num) =
d663 2
a664 1
	val strmpos = Position.mkFromLine (line_string, line_num)
d666 2
a667 4
	TOKEN_STREAM{strmpos=strmpos, 
		     source_name=name_of_file,
		     interactive=false,
		     line_and_col=ref(line_num, Info.Location.first_col),
a670 1
	
d678 1
a678 1
       ts as TOKEN_STREAM{strmpos=b, source_name=file, last_token, ...}) =
d691 1
a691 1
	 ts as TOKEN_STREAM{strmpos=b, source_name=file, line_and_col=ref (line, col),
d704 1
a704 1
			     ts as TOKEN_STREAM{strmpos=b, pushed_back=list as ref (x :: xs),
d711 1
a711 1
			     ts as TOKEN_STREAM{strmpos=b, line_and_col=loc, pushed_back=ref [],
d715 1
a715 1
	      (loc := (Position.getlinenum b, Position.getlinepos b);
d726 1
a726 1
    fun locate (TOKEN_STREAM {strmpos=b, source_name=s, line_and_col=ref(s_line, s_col),...})=
d729 3
a731 2
	 e_line=Position.getlinenum b, e_col=Position.getlinepos b}
    fun eof (TOKEN_STREAM{strmpos=b, ...}) = Position.eof b
d733 1
a733 1
    fun flush_to_nl (TOKEN_STREAM{strmpos=b, ...}) = Position.flush_to_nl b
@


1.55.1.5
log
@Changing TextIO to a functor argument.
@
text
@a2 3
 * Revision 1.55.1.4  1996/08/20  13:28:57  davids
 * Change IO to use basis library.
 *
a242 1
  type instream = Position.instream
@


1.54
log
@String functions explode, implode, chr and ord now only available from String
io functions and types
instream, oustream, open_in, open_out, close_in, close_out, input, output and end_of_stream
now only available from MLWorks.IO
@
text
@d3 6
a229 8
(* In case bytearrays break *)
(*
  type array_type = int Array.array
  val array_array = Array.array
  val array_arrayoflist = Array.arrayoflist
  val array_sub = Array.sub
*)

d279 1
a279 1
  datatype Dfa = DFA of array_type Array.array * int Array.array
d442 2
a443 2
	val trans = Array.array (maxnode,array_array(0,0))
	val actions = Array.array(maxnode,0)
d445 1
a445 1
	  (Array.update(trans,n,t); Array.update(actions,n,a))
d527 1
a527 1
	(ndfa', Array.arrayoflist (rev actions))
d537 1
a537 1
	print("\nDFA has " ^ Int.toString(Array.length trans) ^ " states\n");
d581 1
a581 1
	       Array.sub (actions,finishaction-1) (location, buffer, finishstring, (error_info, options))
d596 1
a596 1
	      val newstate = array_sub (Array.sub (trans,state), c)
d601 1
a601 1
		   Array.sub (actions,finishaction-1) (location, buffer, finishstring, (error_info,options))
d617 1
a617 1
		  val act = Array.sub(action_numbers,newstate)
@


1.53
log
@Adding Int structure
@
text
@d3 3
d215 1
a215 1
  fun print s = output (std_out,s)
d553 2
a554 2
      if n >= ord " " andalso n < 127
          then chr n
d556 2
a557 2
        if n < (ord " ")
          then "^" ^ chr(n + 64)
d666 1
a666 1
        val buffer = InBuffer.mkInBuffer (fix_input (fn _ => input (instream, 4096)))
@


1.52
log
@Updating for new language definition
@
text
@d3 3
d185 2
d419 1
a419 1
            (fn n => output(std_out,MLWorks.Integer.makestring n ^ ", "))
d536 1
a536 1
	print("\nDFA has " ^ MLWorks.Integer.makestring(Array.length trans) ^ " states\n");
d556 1
a556 1
          "\\" ^ MLWorks.Integer.makestring n
@


1.51
log
@newmap becomes map, NEWMAP becomes MAP
@
text
@d3 3
d194 1
a194 1
		structure Ndfa : NDFA
d196 2
a197 2
		sharing type Ndfa.action = int = Ndfa.state
                and type LexRules.Result = Token.Token
@


1.50
log
@Removed spurious call to break.
@
text
@d3 3
d180 1
a180 1
require "../utils/newmap";
d188 1
a188 1
		structure Map : NEWMAP
@


1.49
log
@Removing redundant requires
@
text
@d3 3
a200 2
  val break = MLWorks.Debugger.break

a521 1
        val _ = break "make_dfa"
@


1.48
log
@Change to Option structure
@
text
@d3 3
a173 1
require "../utils/print";
a176 2
require "../main/options";
require "../main/info";
a181 1
		structure Print : PRINT
d184 1
a186 1
		structure Token : TOKEN
d200 2
d228 1
a228 1
	  (Print.print ".\n"; column := 0)
d230 1
a230 1
	  (Print.print "."; column := c + 1)
d525 1
a525 1
	Print.print("\nDFA has " ^ MLWorks.Integer.makestring(Array.length trans) ^ " states\n");
d564 2
a565 1
	fun lex1 (string,state,finishfunction,finishstring,finishpoint,found_one_earlier) =
d569 1
a569 1
	       finishfunction (location, buffer, finishstring, (error_info, options))
a584 3
(*
              val _ = output (std_out,MLWorks.Integer.makestring state ^ ":" ^ MLWorks.Integer.makestring c ^ "-" ^ MLWorks.Integer.makestring newstate ^ "\n")
*)
d589 1
a589 1
		   finishfunction (location, buffer, finishstring, (error_info,options))
d608 1
a608 1
		    lex1(string,newstate,Array.sub(actions,act-1),string,InBuffer.getpos buffer,true)
d610 1
a610 1
                    lex1(string,newstate,finishfunction,finishstring,finishpoint,found_one_earlier)
d618 1
a618 1
	lex1([], 1, unexpected, [], startpoint, false)
@


1.47
log
@Use pervasive Option.option for return values in NewMap
@
text
@d3 3
d199 2
d216 1
a216 1
  type options = LexRules.options
d468 1
a468 1
	    else loop((String.ordof(s, x), start) :: res, x-1)
d484 1
a484 1
				  
d520 1
d529 1
@


1.46
log
@Commented out call to timer as can't run under NJ.
@
text
@d3 3
d376 1
a376 1
	      Map.YES answer => answer
@


1.45
log
@Time making the lexer tables.
Bytearrays work now.
@
text
@d3 4
d519 2
d523 1
@


1.44
log
@Use this as change log message? [yn] Tried changing arrays to bytearrays.  This failed.  The code is commented out.
@
text
@d3 3
d163 1
a163 1
require "../utils/crash";
d174 1
d189 1
a189 1
(*
d193 1
a194 1
*)
d196 2
d202 1
d379 8
d390 1
d514 3
@


1.43
log
@Minor improvements to error messages.  First index of locations now
defined by values in Info.Location.
@
text
@d3 4
d183 14
d240 1
a240 1
  datatype DfaNode = D of int Array.array * int * int
d246 1
a246 1
  datatype Dfa = DFA of int Array.array Array.array * int Array.array
d308 1
a308 1
	val markedstates = (ref [D (Array.array(256,0),0,0)]) : DfaNode list ref
d372 12
a383 1
	fun transtable subset = Array.arrayoflist(loop([], 255, subset, ndfa))
d400 1
a400 1
	val trans = Array.array (maxnode,Array.array(0,0))
d546 4
a549 1
	      val state = Array.sub (Array.sub (trans,state), c)
d551 1
a551 1
	      if state = 0 then
d570 1
a570 1
		  val act = Array.sub(action_numbers,state)
d573 1
a573 1
		    lex1(string,state,Array.sub(actions,act-1),string,InBuffer.getpos buffer,true)
d575 1
a575 1
                    lex1(string,state,finishfunction,finishstring,finishpoint,found_one_earlier)
@


1.42
log
@Removed mkInteractiveTokenStream.
@
text
@d3 3
d533 1
a533 1
                      "Ignoring character \"" ^ (chr_to_string discard_char) ^ "\"");
d549 1
a549 1
			(Info.FAULT, location, "Unexpected Lexical Error")
d566 2
a567 1
		     line_and_col=ref(0,0), pushed_back=ref ([]: Token.Token list),
d576 2
a577 1
		     line_and_col=ref(line,0),pushed_back=ref ([]: Token.Token list),
d588 2
a589 1
		     line_and_col=ref(0,0),pushed_back=ref ([]: Token.Token list),
d599 1
a599 2
       ts as TOKEN_STREAM{buffer=b, source_name=file, line_and_col=ref (line, col),
			  last_token, ...}) =
@


1.41
log
@Modified to allow multiple ungets
@
text
@d3 3
a575 8
    fun mkInteractiveTokenStream (f, name_of_file) =
      let
	val buffer = InBuffer.mkInBuffer (fix_input f)
      in
	TOKEN_STREAM{buffer=buffer, source_name=name_of_file, interactive=true,
		     line_and_col=ref(0,0), pushed_back=ref ([]: Token.Token list),
		     last_token=ref Token.IGNORE}
      end
@


1.41.1.1
log
@Fork for bug fixing
@
text
@a2 3
Revision 1.41  1993/08/12  13:31:22  jont
Modified to allow multiple ungets

@


1.40
log
@Added single character unGetToken function and lastToken function
@
text
@d3 3
d476 1
a476 5
    (* The Token ref is the last token read *)
    (* The bool ref is whether it has been pushed back or not *)
    (* The default on creating a token stream is Token.IGNORE *)
    (* I'm not sure this is right as is may end up being the token *)
    (* returned after doing an unGetToken *)
d480 3
a482 1
        (InBuffer.InBuffer * string * bool * (int * int) ref * Token.Token ref * bool ref)
d486 1
a486 1
	     ts as TOKEN_STREAM (buffer, filename, _,_,_,_)
d559 3
a561 1
	TOKEN_STREAM (buffer, name_of_file, false, ref(0,0),ref Token.IGNORE,ref false)
d568 3
a570 1
	TOKEN_STREAM (buffer, name_of_file, false, ref(line,0),ref Token.IGNORE,ref false)
d577 3
a579 1
	TOKEN_STREAM (buffer, name_of_file, true, ref(0,0),ref Token.IGNORE,ref false)
d587 3
a589 1
	TOKEN_STREAM (buffer, name_of_file, false, ref(0,0),ref Token.IGNORE,ref false)
d597 13
a609 13
		 (options, Token.IN_COMMENT n,
      		  ts as TOKEN_STREAM (b, file, _, ref (line, col),tokref,_)) =
     let
       val result = 
         (case LexRules.read_comment (b, n) of
            Token.IGNORE =>
              getToken error_info (options, Token.PLAIN_STATE, ts)
          | t => t)
     in
       tokref := result;
       result
     end
       
d611 13
a623 12
		 (options, Token.IN_STRING s,
		  ts as TOKEN_STREAM (b, file, _, ref (line, col),tokref,_)) =
         let
           val result =
             LexRules.continue_string
             (Info.Location.EXTENT {name = file, s_line = line, s_col = col,
                                    e_line = line, e_col = col},
              b, error_info, s)
         in
           tokref := result;
           result
         end
d625 5
a629 3
			     ts as TOKEN_STREAM (b, _, _, loc,tokref,pushed as ref true)) =
        (pushed := false;
         !tokref)
d632 2
a633 1
			     ts as TOKEN_STREAM (b, _, _, loc,tokref,_)) =
d639 2
a640 2
            |   get other = other
            val result = get Token.IGNORE
d642 2
a643 2
            tokref := result;
            result
d646 2
a647 2
    fun associated_filename (TOKEN_STREAM (_, s, _, _,_,_)) = s
    fun locate (TOKEN_STREAM (b, s, _, ref(s_line, s_col),_,_)) =
d651 4
a654 4
    fun eof (TOKEN_STREAM (b, _, _, _,_,_)) = InBuffer.eof b
    fun clear_eof (TOKEN_STREAM (b, _, _, _,_,_)) = InBuffer.clear_eof b
    fun is_interactive (TOKEN_STREAM (_, _, interactive, _,_,_)) = interactive
    fun flush_to_nl (TOKEN_STREAM (b, _, _, _,_,_)) = InBuffer.flush_to_nl b
d656 4
a659 4
    fun unGetToken (TOKEN_STREAM (_,_,_,_,_,pushed)) =
      pushed := true
        
    fun lastToken (TOKEN_STREAM (_,_,_,_,ref tok,_)) = tok
@


1.39
log
@Added text_preprocess function
@
text
@d3 3
d472 7
d481 1
a481 1
        (InBuffer.InBuffer * string * bool * (int * int) ref)
d485 1
a485 1
	     ts as TOKEN_STREAM (buffer, filename, _, _)
d558 1
a558 1
	TOKEN_STREAM (buffer, name_of_file, false, ref(0,0))
d565 1
a565 1
	TOKEN_STREAM (buffer, name_of_file, false, ref(line,0))
d572 1
a572 1
	TOKEN_STREAM (buffer, name_of_file, true, ref(0,0))
d580 1
a580 1
	TOKEN_STREAM (buffer, name_of_file, false, ref(0,0))
d589 12
a600 5
      		  ts as TOKEN_STREAM (b, file, _, ref (line, col))) =
	  (case LexRules.read_comment (b, n) of
	     Token.IGNORE =>
	       getToken error_info (options, Token.PLAIN_STATE, ts)
	   | t => t)
d603 16
a618 5
		  ts as TOKEN_STREAM (b, file, _, ref (line, col))) =
	  LexRules.continue_string
	    (Info.Location.EXTENT {name = file, s_line = line, s_col = col,
				   e_line = line, e_col = col},
	     b, error_info, s)
d620 1
a620 1
			     ts as TOKEN_STREAM (b, _, _, loc)) =
d627 1
d629 2
a630 1
            get Token.IGNORE
d633 2
a634 2
    fun associated_filename (TOKEN_STREAM (_, s, _, _)) = s
    fun locate (TOKEN_STREAM (b, s, _, ref(s_line, s_col))) =
d638 10
a647 4
    fun eof (TOKEN_STREAM (b, _, _, _)) = InBuffer.eof b
    fun clear_eof (TOKEN_STREAM (b, _, _, _)) = InBuffer.clear_eof b
    fun is_interactive (TOKEN_STREAM (_, _, interactive, _)) = interactive
    fun flush_to_nl (TOKEN_STREAM (b, _, _, _)) = InBuffer.flush_to_nl b
@


1.38
log
@Removed integer parameter
@
text
@d3 3
d542 2
d546 1
a546 1
	val buffer = InBuffer.mkInBuffer f
d553 1
a553 1
	val buffer = InBuffer.mkLineInBuffer (f, line, eof)
d560 1
a560 1
	val buffer = InBuffer.mkInBuffer f
d568 1
a568 1
        val buffer = InBuffer.mkInBuffer (fn _ => input (instream, 4096))
@


1.37
log
@Removed the pushback facility of TokenStreams, as its never used.
Added the eof argument to mkLineTokenStream for use with the incremental
parser.
@
text
@d3 5
a138 1
require "../utils/integer";
a149 1
		structure Integer : INTEGER
d451 1
a451 3
	Print.print "\nDFA has ";
	Integer.print (Array.length trans);
	Print.print " states\n";
d464 1
a464 1
          "\\" ^ Integer.makestring n
@


1.36
log
@getToken now takes a Lexerstate argument, and calls the appropriate
function for the state.  Thus we can lex comments and strings that extend
over multiple lines in the shell.
Tokenstream is now a datatype, to make type checking easier.
@
text
@d3 6
a146 1
		structure Crash : CRASH
d467 1
a467 2
        (InBuffer.InBuffer * string * bool * (int * int) ref *
         (Token.Token * Info.Location.T) list ref)
d471 1
a471 1
	     ts as TOKEN_STREAM (buffer, filename, _, _, _)
d542 1
a542 1
	TOKEN_STREAM (buffer, name_of_file,false, ref(0,0), ref [])
d545 1
a545 1
    fun mkLineTokenStream (f, name_of_file, line) =
d547 1
a547 1
	val buffer = InBuffer.mkLineInBuffer (f,line)
d549 1
a549 1
	TOKEN_STREAM (buffer, name_of_file,false, ref(0,0),ref [])
d556 1
a556 1
	TOKEN_STREAM (buffer, name_of_file,true, ref(0,0), ref [])
d564 1
a564 1
	TOKEN_STREAM (buffer, name_of_file,false, ref(0,0), ref [])
d567 5
a571 13
    (* The first case of getToken deals with pushed tokens.   Otherwise the
       action taken depends on the lexer state.  IGNORE tokens are dealt
       with here, both for abstraction and for efficiency.  Other tokens
       are returned.  EOFs in comments or strings are handled appropriately
       by the parser functions in ../parser/_parser. *)
    fun getToken _ (options, Token.PLAIN_STATE,
		    ts as TOKEN_STREAM (_,_,_,_, s as ref ((t,_)::l))) =
	  (s := l; t)
      | getToken _ (options, _,
			  TOKEN_STREAM (_,_,_,_, s as ref ((t,_)::l))) =
	  Crash.impossible
	    "getToken: attempt to read pushed tokens while in comment or string"
      | getToken error_info
d573 1
a573 1
      		  ts as TOKEN_STREAM (b, file, _, ref (line, col), _)) =
d580 1
a580 1
		  ts as TOKEN_STREAM (b, file, _, ref (line, col), _)) =
d586 1
a586 1
			     ts as TOKEN_STREAM (b, _, _, loc, _)) =
d589 1
a589 1
	      (loc := (InBuffer.getlinenum b,InBuffer.getlinepos b);
d597 2
a598 6
    fun ungetToken (t, loc, TOKEN_STREAM (_,_,_,_, s as ref l)) =
	  s := (t,loc)::l

    fun associated_filename (TOKEN_STREAM (_, s, _, _, _)) = s
    fun locate (TOKEN_STREAM (_, _, _, _, ref ((_,l)::_))) = l
      | locate (TOKEN_STREAM (b, s, _, ref(s_line, s_col), _)) =
d602 4
a605 4
    fun eof (TOKEN_STREAM (b, _, _, _, _)) = InBuffer.eof b
    fun clear_eof (TOKEN_STREAM (b, _, _, _, _)) = InBuffer.clear_eof b
    fun is_interactive (TOKEN_STREAM (_, _, interactive, _, _)) = interactive
    fun flush_to_nl (TOKEN_STREAM (b, _, _, _, _)) = InBuffer.flush_to_nl b
@


1.35
log
@lex now takes an options parameter, passed to it by getToken.
@
text
@d3 3
d131 1
a135 1
require "inbuffer";
d141 1
a144 1
		structure InBuffer : INBUFFER
d148 1
a148 1
                and type LexRules.result = Token.Token
a151 1
  structure LexRules = LexRules
d154 1
d157 1
a157 1
  type result = LexRules.result
d368 1
a368 1
    from the Dragon book (well, sort of . *)
d460 4
a463 2
    type TokenStream =
      (InBuffer.InBuffer * string * bool * (int * int) ref * (Token.Token * Info.Location.T) list ref)
a464 2
    exception LexEof of bool * int * (string -> LexRules.result) * string * InBuffer.InBuffer

d467 1
a467 1
	     ts as (buffer, filename, _, _, _) : TokenStream
d477 1
a477 1
	       finishfunction (location, finishstring, (error_info, options))
d481 1
a481 1
                 [] => LexRules.check_end_state(error_info,location)
d486 1
a486 1
                    "Unexpected end of file -- unclosed string?");
d497 1
a497 1
		   finishfunction (location, finishstring, (error_info,options))
d532 1
a532 1
    val lexers = Array.arrayoflist(map (lex o make_dfa) LexRules.rules)
d538 1
a538 1
	(buffer, name_of_file,false, ref(0,0), ref [])
d545 1
a545 1
	(buffer, name_of_file,false, ref(0,0),ref [])
d552 1
a552 1
	(buffer, name_of_file,true, ref(0,0),ref [])
d560 1
a560 1
	(buffer, name_of_file,false, ref(0,0),ref [])
d563 44
a606 17
    fun getToken _ (_, ts:TokenStream as
			(_,_,_,_, s as ref ((t,_)::l))) = (s := l; t)
      | getToken error_info (options, ts as {1=b, 4=loc, ...}) =
      let
        fun get (Token.IGNORE l) =
	  (loc := (InBuffer.getlinenum b,InBuffer.getlinepos b);
	   get (Array.sub(lexers, l)((error_info, options), ts)))
          | get other = other
      in
        get (Token.IGNORE 0)
      end

    fun ungetToken (t,loc,(_,_,_,_,s as ref l)) = s := (t,loc)::l

    fun associated_filename({2=s, ...}:TokenStream) = s
    fun locate ({5=ref ((_,l)::_), ...}:TokenStream) = l
      | locate (b, s, _, ref(s_line, s_col), _) =
d610 4
a613 5
    fun eof ({1=b, ...}:TokenStream) = InBuffer.eof b
    fun clear_eof ({1=b, ...}:TokenStream) = InBuffer.clear_eof b
    fun is_interactive ({3=interactive, ...}:TokenStream) = interactive

    fun flush_to_nl ({1=b, ...}:TokenStream) = InBuffer.flush_to_nl b
@


1.34
log
@Some speed improvements
@
text
@d3 3
d129 1
d153 1
d462 4
a465 1
    fun lex (dfa as DFA (trans,action_numbers), actions) (opts,ts as (buffer, filename, _, _, _) : TokenStream) =
d473 3
a475 1
	      (InBuffer.position(buffer, finishpoint); finishfunction (location, finishstring,opts))
d478 1
a478 1
                 [] => LexRules.check_end_state(opts,location)
d481 1
a481 1
                   opts
d493 3
a495 1
		  (InBuffer.position(buffer, finishpoint); finishfunction (location, finishstring,opts))
d502 1
a502 1
                     opts
d506 1
a506 1
		     lex (dfa,actions) (opts,ts)
d520 2
a521 1
            Info.error' opts (Info.FAULT, location, "Unexpected Lexical Error")
d560 3
a562 3
    fun getToken options (ts:TokenStream as
			  (_,_,_,_, s as ref ((t,_)::l))) = (s := l; t)
      | getToken options (ts as {1=b, 4=loc, ...}) =
d566 1
a566 1
	   get (Array.sub(lexers, l)(options,ts)))
@


1.33
log
@Modified to give ranges for locations, from start to end of token
@
text
@d3 3
d171 3
a173 17
  fun canonical L =
    let
      fun quicksort ([], accum) = accum
	| quicksort (pivot :: rest, accum) = 
	  let
	    fun partition (left, right, []) =
	        quicksort (left, pivot :: quicksort (right, accum))
	      | partition (left, right, y::ys) =
		if (y : int) < pivot then partition (y :: left, right, ys)
		else if y > pivot then partition (left, y :: right, ys)
		     else partition (left, right, ys)
	  in
	    partition ([], [], rest)
	  end
    in
      quicksort (L,[])
    end
d175 9
d204 3
a206 8
  fun trans_subset(char, ndfa, subset) =
    let
      fun loop ([], accum) = accum
	| loop (state :: rest, accum) =
	  loop (rest, Ndfa.get_char (char, Ndfa.transitions (ndfa,state), accum))
    in
      Lists.filter(loop (subset,[]))
    end
d208 3
d213 7
a219 12
  fun epsclosure(ndfa, subset) =
    let
      fun epsclosure'([], subset) = subset
	| epsclosure'(state :: rest, subset) =
	  let
	    val new = Lists.filter (Ndfa.get_epsilon (Ndfa.transitions (ndfa, state), rest))
	  in
	    epsclosure'(new, state :: subset)
	  end
    in
      canonical (epsclosure'(subset, []))
    end
d221 2
a237 9
(*
    let
      fun best_action'( m, ((an:int)::t)) =
	best_action'(if an > m then an else m, t)
	| best_action'(m, []) = m
    in
      best_action'(0, map (fn state => Ndfa.action(ndfa,state)) l)
    end
*)
d249 3
a251 7
    fun total L =
      let
	fun loop (n :: ns, res:int) = loop (ns, n + res)
	  | loop ([], res) = res
      in
	loop (L,0)
      end
d254 8
a261 8
    let
      val nextnode = counter 1
      val init = epsclosure(ndfa, [Ndfa.start ndfa])
      val unmarkedstates = (ref []) : (int list * int) list ref
      (* state zero is empty (terminating), state 1 is initial *)
      val markedstates = (ref [D (Array.array(256,0),0,0)]) : DfaNode list ref
      val currentstate = ref (init,1)
      (* keep a `hash-value' (the total) with each state to speed searching *)
d263 8
a270 37
      local
	fun loop ([],[]) = false
	  | loop (_ ,[]) = false
	  | loop ([], _) = true
	  | loop ((h1:int)::t1,h2::t2) =
	    if h1 < h2 then true
	    else if h1 > h2 then false
		 else loop (t1,t2)
      in
	fun ordering ((hash1:int, subset1), (hash2, subset2)) =
	  if hash1 < hash2 then true
	  else if hash1 > hash2 then false
	       else loop (subset1, subset2)
      end

      local
	fun loop ([],[]) = true
	  | loop (_ ,[]) = false
	  | loop ([], _) = false
	  | loop ((h1:int)::t1,h2::t2) =
	    if h1 < h2 then false
	    else if h1 > h2 then false
		 else loop (t1,t2)
      in
	fun ordering_eq ((hash1:int, subset1), (hash2, subset2)) =
	  if hash1 < hash2 then false
	  else if hash1 > hash2 then false
	       else loop (subset1, subset2)
      end

      val states = ref (Map.from_list (ordering,ordering_eq) [((0, []), 0), ((total init, init), 1)])
	
      (* Adds a state to the unmarked list and gives it a number *)
      fun addstate (t,l) =
	let
	  val nodeno = nextnode ()
	  val actno = best_action(ndfa, l)
d272 4
a275 4
	  unmarkedstates := (l,nodeno) :: (!unmarkedstates);
	  states := Map.define (!states, (t,l), nodeno);
	  printDot();
	  nodeno
d278 8
a285 4
      (* Returns number of this state, adding if necessary *)
      fun find subset =
	let
	  val t = total subset
d287 4
a290 13
	  case Map.tryApply'(!states, (t,subset)) of
	    Map.YES answer => answer
	  | _ => addstate(t,subset)
        end

      (* Returns the transition table: an int array *)
      fun transtable subset =
	let
	  fun loop (res, c) =
	    if c < 0 then res
	    else loop(find(epsclosure(ndfa, trans_subset(c, ndfa, subset))) :: res, c-1)
	in
	  Array.arrayoflist(loop([], 255))
d293 43
a335 11
      (* Recurses down the list of unmarked states.... *)
      fun doit () =
	let
	  val (s,n) = !currentstate
	in
	  (markedstates := (D(transtable s,n,best_action(ndfa, s))) :: (!markedstates);
	   (fn [] => () | (s::ss) => (unmarkedstates := ss;
				      currentstate := s;
				      doit ()))
	   (!unmarkedstates))
	end
d350 3
a352 3
    (* So now we have ndfa's, dfa's, and a function from one to the other.
     Now we need to get regexps into ndfa's, glue the ndfa's together,
     transform into a dfa, and keep the actions all the way through *)
d354 1
a354 1
    (* first a function regexp * node list * int * counter -> node list * int
d356 5
a360 5
     (adds nodes to the list, returning a new list which excludes the start
     and end nodes).  The int arguments is final node number, the int result
     is initial node number (The function generates a number for the
     initial node, and adds it to the list). This uses the constructions
     from the Dragon book (well, sort of . *)
d362 34
a395 34
    fun re2ndfa regexp ndfa =
      case regexp of
	RegExp.EPSILON =>
	  Ndfa.add(ndfa, Ndfa.epsilon [Ndfa.start ndfa])
      | RegExp.NODE s =>
	  let
	    fun loop (res, x) =
	      if x < 0 then res
	      else loop(Ndfa.add (res, Ndfa.single_char (String.ordof(s, x), Ndfa.start res)), x-1)
	  in
	    loop(ndfa, size s - 1)
	  end
      | RegExp.CLASS s =>
	  let
	    val start = Ndfa.start ndfa

	    fun loop (res, x) =
	      if x < 0 then Ndfa.mk_trans res
	      else loop((String.ordof(s, x), start) :: res, x-1)
	  in
	    Ndfa.add(ndfa, loop([], size s - 1))
	  end
      | RegExp.BAR(s,t) =>
	  let
	    val ndfa1 = re2ndfa s ndfa
	    val ndfa2 = re2ndfa t (Ndfa.set_start(ndfa1, Ndfa.start ndfa))
	    val transitions = Ndfa.epsilon [Ndfa.start ndfa1, Ndfa.start ndfa2]
	  in
	    Ndfa.add (ndfa2, transitions)
	  end
      | RegExp.DOT(s,t) =>
	  re2ndfa s (re2ndfa t ndfa)
      | RegExp.STAR s =>
	  Ndfa.add_rec (ndfa, re2ndfa s)
@


1.32
log
@Change to allow token streams to be created with a given initial line number.
@
text
@d3 3
d474 2
a475 1
    type TokenStream = (InBuffer.InBuffer * string * bool * (Token.Token * Info.Location.T) list ref)
d479 1
a479 1
    fun lex (dfa as DFA (trans,action_numbers), actions) (opts,ts as (buffer, filename,_,_) : TokenStream) = 
d544 1
a544 1
	(buffer, name_of_file,false,ref [])
d551 1
a551 1
	(buffer, name_of_file,false,ref [])
d558 1
a558 1
	(buffer, name_of_file,true,ref [])
d566 1
a566 1
	(buffer, name_of_file,false,ref [])
a567 1

d569 3
a571 2
    fun getToken options (ts as (_,_,_,s as ref ((t,_)::l))) = (s := l; t)
      | getToken options ts =
d573 3
a575 1
        fun get (Token.IGNORE l) = get (Array.sub(lexers, l)(options,ts))
d581 1
a581 1
    fun ungetToken (t,loc,(_,_,_,s as ref l)) = s := (t,loc)::l
d583 9
a591 6
    fun associated_filename(_,s,_,_) = s
    fun locate (_,_,_,ref ((_,l)::_)) = l
      | locate (b, s, _,_) = Info.Location.POSITION (s, InBuffer.getlinenum b, InBuffer.getlinepos b)
    fun eof (b,_,_,_) = InBuffer.eof b
    fun clear_eof (b,_,_,_) = InBuffer.clear_eof b
    fun is_interactive (_,_,interactive,_) = interactive
d593 1
a593 1
    fun flush_to_nl (b,_,_,_) = InBuffer.flush_to_nl b
@


1.31
log
@Print non-printable characters sensibly in error messages.
@
text
@d3 3
d539 7
@


1.30
log
@Hack to handle unclosed comments and strings
@
text
@d3 3
d459 9
d506 3
a508 1
                     (Info.RECOVERABLE, location, "Ignoring character \"" ^ chr discard_char ^ "\"");
@


1.29
log
@removed function involving map in favour of Lists.reducel
@
text
@d3 3
d471 1
a471 1
                 [] => ()
@


1.28
log
@Added an "unget" facility.
@
text
@d3 3
d225 9
d241 1
d320 3
a322 2
	  Map.apply'(!states, (t,subset))
          handle Map.Undefined => addstate (t,subset)
@


1.27
log
@Added flush_to_nl
@
text
@d3 3
d439 1
a439 1
    type TokenStream = (InBuffer.InBuffer * string * bool)
d443 1
a443 1
    fun lex (dfa as DFA (trans,action_numbers), actions) (opts,ts as (buffer, filename, _) : TokenStream) = 
d506 1
a506 1
	(buffer, name_of_file,false)
d513 1
a513 1
	(buffer, name_of_file,true)
d521 1
a521 1
	(buffer, name_of_file,false)
d525 2
a526 1
    fun getToken options ts =
d534 1
a534 7
    fun linenum (b,_,_) = InBuffer.getlinenum b
    fun linepos (b,_,_) = InBuffer.getlinepos b
    fun associated_filename(_,s,_) = s
    fun locate (b, s, _) = Info.Location.POSITION (s, InBuffer.getlinenum b, InBuffer.getlinepos b)
    fun eof (b,_,_) = InBuffer.eof b
    fun clear_eof (b,_,_) = InBuffer.clear_eof b
    fun is_interactive (_,_,interactive) = interactive
d536 8
a543 1
    fun flush_to_nl (b,_,_) = InBuffer.flush_to_nl b
@


1.26
log
@Changed Error structure to Info
@
text
@d3 3
d537 2
@


1.25
log
@Added clear_eof function.
@
text
@d3 3
d102 1
a102 1
require "../main/error";
a115 1
                structure Error : ERROR
d118 1
a118 1
                sharing LexRules.Error = Error) : LEXER = 
d123 1
a123 1
  structure Error = Error
d437 1
a437 1
    fun lex (dfa as DFA (trans,action_numbers), actions) (ts as (buffer, filename, _) : TokenStream) = 
d441 1
a441 1
          Error.Location.POSITION (filename, InBuffer.getlinenum buffer, InBuffer.getlinepos buffer)
d445 1
a445 1
	      (InBuffer.position(buffer, finishpoint); finishfunction (location, finishstring))
d450 4
a453 3
                   Error.report
                   (Error.ERROR (Error.RECOVERABLE, location,
                                 "Unexpected end of file -- unclosed string?"));
d463 1
a463 1
		  (InBuffer.position(buffer, finishpoint); finishfunction (location, finishstring))
d469 4
a472 3
                     Error.report
                     (Error.ERROR (Error.RECOVERABLE, location, "Ignoring character \"" ^ chr discard_char ^ "\""));
		     lex (dfa,actions) ts
d486 1
a486 2
            Error.report'
            (Error.ERROR (Error.FAULT, location, "Unexpected Lexical Error"))
d519 1
a519 1
    fun getToken ts =
d521 1
a521 1
        fun get (Token.IGNORE l) = get (Array.sub(lexers, l) ts)
d530 1
a530 1
    fun locate (b, s, _) = Error.Location.POSITION (s, InBuffer.getlinenum b, InBuffer.getlinepos b)
@


1.24
log
@Changed file token stream to not be interactive.
@
text
@d3 3
d529 1
@


1.23
log
@Added line number to token stream input functions.
Added mkFileTokenStream.
@
text
@d3 4
d509 1
a509 1
	(buffer, name_of_file,true)
@


1.22
log
@Change to NewMap.empty which now takes < and = functions instead of the single-function
@
text
@d3 3
d499 9
@


1.21
log
@Installed central error reporting mechanism.
@
text
@d3 3
d241 3
a243 3
	fun loop ([],[]) = Map.EQUAL
	  | loop (_ ,[]) = Map.GREATER
	  | loop ([], _) = Map.LESS
d245 2
a246 2
	    if h1 < h2 then Map.LESS
	    else if h1 > h2 then Map.GREATER
d250 2
a251 2
	  if hash1 < hash2 then Map.LESS
	  else if hash1 > hash2 then Map.GREATER
d255 16
a270 1
      val states = ref (Map.from_list ordering [((0, []), 0), ((total init, init), 1)])
@


1.20
log
@Added missing require.
@
text
@d3 3
d108 1
d411 1
a411 1
          Error.POSITION (filename, InBuffer.getlinenum buffer, InBuffer.getlinepos buffer)
d489 2
a491 1
    fun associated_filename(_,s,_) = s
@


1.19
log
@Replaced LexBasics error handler by proper global error handler,
and propagated more information through to the action functions
so that they can report error positions accurately.
@
text
@d3 5
d83 1
@


1.18
log
@Added interactive slot to token streams.
@
text
@d3 3
d91 1
d93 2
a94 1
                and type LexRules.result = Token.Token) : LEXER = 
a97 1
  structure LexBasics = LexRules.LexBasics
a101 2
  exception LexError = LexBasics.LexError
    
d393 2
d397 1
a397 1
    fun lex (dfa as DFA (trans,action_numbers), actions) buffer = 
d400 2
d405 9
a413 5
	      (InBuffer.position(buffer, finishpoint); finishfunction(finishstring))
	    else (case string of
		    [] => ()
		  | _ => LexBasics.report_lex_error ("Unexpected end of file -- unclosed string?");
		      LexRules.eof)
d422 1
a422 1
		  (InBuffer.position(buffer, finishpoint); finishfunction(finishstring))
d428 3
a430 2
		     LexBasics.report_lex_error ("Ignoring character \"" ^ chr discard_char ^ "\"");
		     lex (dfa,actions) buffer
d438 2
a439 1
		  else lex1(string,state,finishfunction,finishstring,finishpoint,found_one_earlier)
d442 4
d447 1
a447 1
	lex1([],1,fn _ => raise LexError "Unexpected Lexical Error", [],startpoint,false)
a452 2
    type TokenStream = (InBuffer.InBuffer * string * bool)
      
d469 1
a469 1
    fun getToken (b,_,_) =
d471 1
a471 1
        fun get (Token.IGNORE l) = get (Array.sub(lexers, l) b)
@


1.17
log
@Made various changes to work with new inbuffer and ndfa signatures.
@
text
@d3 3
d435 1
d437 1
a437 1
    type TokenStream = (InBuffer.InBuffer * string)
d445 8
a452 1
	(buffer, name_of_file)
d455 1
a455 1
    fun getToken (b,_) =
d463 5
a467 4
    fun linenum (b,_) = InBuffer.getlinenum b
    fun linepos (b,_) = InBuffer.getlinepos b
    fun eof (b,_) = InBuffer.eof b
    fun associated_filename(_,s) = s
@


1.16
log
@Did a few optimisations and removed the stuff to do with the self reference.
@
text
@d3 3
d75 1
a75 1
require "lexgen";
d86 1
a86 1
                and type LexRules.result = Token.Token) : LEXGEN = 
d88 1
a110 8
  fun upto(m, n) = if m > n then [] else m :: upto(m + 1, n)
      
  val chars = map chr (upto(0, 255))

  val letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
  val digits = "0123456789"
  val printables = letters ^ digits ^ "`~!@@#$%^&*()_+-={}[]<>,./?;:'|\\\" "
      
a178 5
  (* half_trans does a trans and then a closure *)
  
  fun half_trans (ndfa, subset) char =
    epsclosure(ndfa, trans_subset(char, ndfa, subset))
      
d261 8
a268 2
      fun transtable s =
	Array.arrayoflist (map (find o (half_trans(ndfa, s))) chars)
d313 3
a315 6
	    fun loop [] = ndfa
	      | loop (char :: rest) =
		let val ndfa' = loop rest
		in
		  Ndfa.add (ndfa', Ndfa.single_char (char, Ndfa.start ndfa'))
		end
d317 1
a317 1
	    loop (explode s)
d322 4
a325 1
	    val trans = Ndfa.mk_trans (map (fn c => (c,start)) (explode s))
d327 1
a327 1
	    Ndfa.add(ndfa, trans)
d393 34
a426 41
	fun lex1 (string,state,finishfunction,finishstring,finishpoint,found_one_earlier)
	  = if InBuffer.eof buffer
              then
                if found_one_earlier
                  then (InBuffer.position(buffer, finishpoint);
                        finishfunction(finishstring))
                else (case string of
                        [] => ()
                      | _ => LexBasics.report_lex_error ("Unexpected end of file -- unclosed string?");
                          LexRules.eof)
            else
              let
                val c = InBuffer.getchar buffer
                val string = c :: string
                val state = Array.sub (Array.sub (trans,state), c)
              in
                 if state = 0
                   then
                     if found_one_earlier
                       then
                         (InBuffer.position(buffer, finishpoint);
                         (* forget input to current token.  This should be more intelligent
                          so don't make a new string each time round *)
                         InBuffer.forget(buffer, finishpoint);
                         finishfunction(finishstring))
                     else
                       (InBuffer.position(buffer, startpoint);
                        let val discard_char = InBuffer.getchar buffer
                        in
                          LexBasics.report_lex_error ("Ignoring character \"" ^ chr discard_char ^ "\"");
                          lex (dfa,actions) buffer
                        end)
                 else
                   let
                     val act = Array.sub(action_numbers,state)
                   in
                     if act > 0 then
                       lex1(string,state,Array.sub(actions,act-1),string,InBuffer.getpos buffer,true)
                     else lex1(string,state,finishfunction,finishstring,finishpoint,found_one_earlier)
                   end
              end
a432 5
  (* this is going to be some hacking to get it to look like a Mk.1
   lexer. Most of the following can be reworked when we come to fix the
   parser. Note that what we really want is a _function_ makeLexer, not a
   _functor_.... *)
      
a451 1
    fun flushTokenStream(b,_) = InBuffer.forget(b, (InBuffer.getpos b))
a453 1
    fun getbuffer (b,_) = ""
@


1.15
log
@Removed all currying from inbuffer
@
text
@d3 3
d154 4
a157 16
    (*
     trans_ndfa c a n accumulates on accum the node-numbers of all the
     transitions from the node numbered n on the string c in the automaton a.
     This list may contain duplicates.
    *)
  
    fun trans_ndfa (char, ndfa, state, accum) =
      Ndfa.get_char (char, Ndfa.transitions (ndfa,state), accum)

    fun epsilon_ndfa (ndfa, state, accum) =
      Ndfa.get_epsilon (Ndfa.transitions (ndfa, state), accum)

    (*
     trans_subset returns the node-numbers of all the transitions from all
     the nodes in the subset
    *)
d159 8
a166 8
    fun trans_subset(char, ndfa, subset) =
      let
	fun loop ([], accum) = accum
	  | loop (state :: rest, accum) =
	    loop (rest, trans_ndfa(char, ndfa, state, accum))
      in
	Lists.filter (loop (subset,[]))
      end
d168 1
a168 1
    (* epsclosure: returns the epsilon-closure of the subset in the ndfa *)
d170 12
a181 12
    fun epsclosure(ndfa, subset) =
      let
	fun epsclosure'([], subset) = subset
	  | epsclosure'(state :: rest, subset) =
	    let
	      val new = Lists.filter (epsilon_ndfa(ndfa, state, rest))
	    in
	      epsclosure'(new, state :: subset)
	    end
      in
	canonical (epsclosure'(subset, []))
      end
d183 1
a183 1
    (* half_trans does a trans and then a closure *)
d185 2
a186 2
    fun half_trans (ndfa, subset) char =
      epsclosure(ndfa, trans_subset(char, ndfa, subset))
d403 1
a403 1
                        "" => ()
d408 3
a410 3
                val c = (InBuffer.getchar buffer)
                val string = string ^ c
                val state = Array.sub (Array.sub (trans,state),ord c)
d425 1
a425 1
                          LexBasics.report_lex_error ("Ignoring character \"" ^ discard_char ^ "\"");
d438 1
a438 2
	
	lex1("",1,fn _ => raise LexError "Unexpected Lexical Error", "",startpoint,false)
d448 1
a448 1
    type TokenStream = (InBuffer.InBuffer * (int -> LexRules.result) * string)
d450 1
a450 1
    val lexers = map (lex o make_dfa) LexRules.rules
d452 1
a452 1
    fun mkTokenStream (f,name_of_file) =
d456 1
a456 1
	(buffer, fn n => Lists.nth (n,lexers) buffer,name_of_file)
d459 1
a459 1
    fun getToken (b,s,_) =
d461 1
a461 2
	val _ = LexRules.self := s
        fun get (Token.IGNORE lexer) = get (s lexer)
d467 6
a472 6
    fun flushTokenStream(b,s,_) = InBuffer.forget(b, (InBuffer.getpos b))
    fun linenum (b,s,_) = InBuffer.getlinenum b
    fun linepos (b,s,_) = InBuffer.getlinepos b
    fun getbuffer (b,s,_) = ""
    fun eof (b,s,_) = InBuffer.eof b
    fun associated_filename(_,_,s) = s
@


1.14
log
@Removed some structures and sharing
@
text
@d3 3
d409 1
a409 1
                  then (InBuffer.position buffer finishpoint;
d425 1
a425 1
                         (InBuffer.position buffer finishpoint;
d428 1
a428 1
                         InBuffer.forget buffer finishpoint;
d431 1
a431 1
                       (InBuffer.position buffer startpoint;
d478 1
a478 1
    fun flushTokenStream(b,s,_) = InBuffer.forget b (InBuffer.getpos b)
@


1.13
log
@Removed Array parameter, so it now uses pervasive Array.
Decurried numerous functions that didn't need it.
@
text
@d3 4
d62 1
a63 1
require "regexp";
a65 1
require "lexbasics";
a72 1
		structure RegExp : REGEXP
d75 3
a77 5
		structure LexBasics : LEXBASICS
		sharing LexBasics = LexRules.LexBasics
		and RegExp = LexRules.RegExp
		and type Ndfa.action = int = Ndfa.state
                and type LexRules.result = LexRules.Token.Token) : LEXGEN = 
d80 2
a81 1
  structure LexBasics = LexBasics
d469 1
a469 1
        fun get (LexRules.Token.IGNORE lexer) = get (s lexer)
d472 1
a472 1
        get (LexRules.Token.IGNORE 0)
@


1.12
log
@Put in error message when EOF is encountered, and a token is being built.
This is intended for strings, but doesn't work as strings are returned as
a sequence of IGNORE tokens.
@
text
@d3 5
a57 1
require "../utils/array";
a70 1
		structure Array : ARRAY
d100 1
a100 1
  fun upto m n = if m > n then [] else m :: upto (m + 1) n
d102 1
a102 1
  val chars = map chr (upto 0 255)
d164 1
a164 1
    fun trans_subset char ndfa subset =
d175 1
a175 1
    fun epsclosure ndfa subset =
d177 2
a178 2
	fun epsclosure' [] subset = subset
	  | epsclosure' (state :: rest) subset =
d182 1
a182 1
	      epsclosure' new (state :: subset)
d185 1
a185 1
	canonical (epsclosure' subset [])
d190 2
a191 2
    fun half_trans ndfa subset char =
      epsclosure ndfa (trans_subset char ndfa subset)
d199 1
a199 1
  fun best_action ndfa l =
d201 3
a203 2
      fun best_action' m ((an:int)::t) = best_action' (if an > m then an else m) t
	| best_action' m [] = m
d205 1
a205 1
      ((best_action' 0) o (map (fn state => Ndfa.action(ndfa,state)))) l
d229 1
a229 1
      val init = epsclosure ndfa [Ndfa.start ndfa]
d257 1
a257 1
	  val actno = best_action ndfa l
d270 1
a270 1
	  Map.apply (!states) (t,subset)
d276 1
a276 1
	Array.arrayoflist (map (find o (half_trans ndfa s)) chars)
d283 1
a283 1
	  (markedstates := (D(transtable s,n,best_action ndfa s)) :: (!markedstates);
a381 15
	       
(*
    local
      fun printarray printfn array =
	(Lists.iterate (fn n => printfn (Array.sub (array,n)))
	 (upto 0 ((Array.length array)-1));
	 Print.print "\n")
    in
      fun printdfa (DFA (trans,actions)) =
	(Print.print "transitions : ";
	 printarray (printarray Integer.print) trans;
	 Print.print "actions : ";
	 printarray Integer.print actions)
    end
*)
@


1.11
log
@Fixed line position output from lexer
@
text
@d3 3
d418 4
a421 1
                else LexRules.eof
@


1.10
log
@Added IGNORE token to remove recursion from lexing of comments and strings.
@
text
@d3 3
a480 1
    fun getpos (b,s,_) = InBuffer.getpos b
@


1.9
log
@Changed BalancedTree to generic Map
@
text
@d3 3
d67 2
a68 1
		and type Ndfa.action = int = Ndfa.state) : LEXGEN = 
d147 3
d171 1
a171 1
	      val new = Lists.filter (trans_ndfa("", ndfa, state, rest))
d470 2
d473 1
a473 1
	s(0)
@


1.8
log
@First version of the profiler
@
text
@d3 3
d43 1
a43 1
require "../utils/balancedtree";
d55 1
a55 1
		structure BalancedTree : BALANCEDTREE
d220 3
a222 3
	fun loop ([],[]) = BalancedTree.EQ
	  | loop (_ ,[]) = BalancedTree.GT
	  | loop ([], _) = BalancedTree.LT
d224 2
a225 2
	    if h1 < h2 then BalancedTree.LT
	    else if h1 > h2 then BalancedTree.GT
d229 2
a230 2
	  if hash1 < hash2 then BalancedTree.LT
	  else if hash1 > hash2 then BalancedTree.GT
d234 1
a234 4
      val states = ref (BalancedTree.empty ordering)
      val _ = states := BalancedTree.insert
	(BalancedTree.insert (!states,(0,[]),0),
	 (total init,init),1)
d243 1
a243 1
	  states := BalancedTree.insert (!states,(t,l),nodeno);
d253 3
a255 4
	  case BalancedTree.lookup (!states,(t,subset)) of
	    BalancedTree.YES(num) => num
	  | BalancedTree.NO => addstate (t,subset)
	end
@


1.7
log
@Changed EOF handling to allow tail recursion
@
text
@d3 3
d450 1
a450 1
    type TokenStream = (InBuffer.InBuffer * (int -> LexRules.result))
d454 1
a454 1
    fun mkTokenStream f =
d458 1
a458 1
	(buffer, fn n => Lists.nth (n,lexers) buffer)
d461 1
a461 1
    fun getToken (b,s) =
d468 7
a474 6
    fun flushTokenStream(b,s) = InBuffer.forget b (InBuffer.getpos b)
    fun linenum (b,s) = InBuffer.getlinenum b
    fun getpos (b,s) = InBuffer.getpos b
    fun linepos (b,s) = InBuffer.getlinepos b
    fun getbuffer (b,s) = ""
    fun eof (b,s) = InBuffer.eof b
@


1.6
log
@Added line numbering
@
text
@d3 3
d392 2
a396 1
	  
d398 37
a434 32
	  = (let
	       val c = InBuffer.getchar buffer								 
	       val string = string ^ c
	       val state = Array.sub (Array.sub (trans,state),ord c)
	     in
	       if state = 0
		 then
		   if found_one_earlier
		     then
		       (InBuffer.position buffer finishpoint;
                        (* forget input to current token.  This should be more intelligent
                         so don't make a new string each time round *)
                        InBuffer.forget buffer finishpoint;
			finishfunction(finishstring))
		   else
		     (InBuffer.position buffer startpoint;
                      let val discard_char = InBuffer.getchar buffer in
                        LexBasics.report_lex_error ("Ignoring character \"" ^ discard_char ^ "\"");
                        lex (dfa,actions) buffer
                      end)
	       else
		 let
		   val act = Array.sub(action_numbers,state)
		 in
		   if act > 0 then
                     lex1(string,state,Array.sub(actions,act-1),string,InBuffer.getpos buffer,true)
		   else lex1(string,state,finishfunction,finishstring,finishpoint,found_one_earlier)
		 end
	     end) handle InBuffer.Eof => if found_one_earlier
                                           then (InBuffer.position buffer finishpoint;
                                                 finishfunction(finishstring))
					 else LexRules.eof
a437 1
	
@


1.5
log
@Errors signalling changed to call report_lex_error and continue rather than raise
an exception.
@
text
@d3 4
d449 1
a449 1
			
d458 1
a458 1
    fun linenum _ = 0
d460 1
a460 1
    fun linepos _ = 0
@


1.4
log
@Removed use of myarray, replaced by array.sml
@
text
@d3 3
d385 1
a385 1
    fun lex (DFA (trans,action_numbers), actions) buffer = 
d400 3
d406 4
a409 1
		      raise LexError "No lexeme found")
d414 2
a415 1
		   if act > 0 then lex1(string,state,Array.sub(actions,act-1),string,InBuffer.getpos buffer,true)
d418 3
a420 2
	     end) handle InBuffer.Eof => if found_one_earlier then (InBuffer.position buffer finishpoint;
								    finishfunction(finishstring))
d424 1
a424 1
	lex1("",1,fn _ => raise LexError "", "",startpoint,false)
@


1.3
log
@Added type information to allow our typechecker to elaborate it
@
text
@d3 3
d23 1
d28 1
a30 1
require "myarray";
@


1.2
log
@Major modifications, including a more abstract implementation of ndfa's
(more efficient as well). This generator is still pretty slow compared
to `flex' and other generators written in C but I don't want to do any
more optimisations without a profiler. Currently the subsets generated
by the subset construction are sorted using quicksort, this could actually
be done on O(n) time because we know the range of numbers we are sorting
(0 ... number of states in ndfa) and hence we could use an array sort
(or whatever its called). However, I'm not convinced this is where all
the time is being spent. Replacing balanced trees with hash tables
might well give a bigger performance win.
@
text
@d2 13
a14 1
$Log:	_lexgen.sml,v $
d165 1
a165 1
      fun best_action' m (an::t) = best_action' (if an > m then an else m) t
d183 1
a183 1
	fun loop (n :: ns, res) = loop (ns, n + res)
d203 1
a203 1
	  | loop (h1::t1,h2::t2) =
d208 1
a208 1
	fun ordering ((hash1, subset1), (hash2, subset2)) =
a443 1

@


1.1
log
@Initial revision
@
text
@d2 4
a5 1
$Log$
d8 5
d20 6
a25 1
functor LexGen (structure RegExp : REGEXP
d31 5
a35 2
		and RegExp = LexRules.RegExp) : LEXGEN = 
  struct
d37 1
a37 2
    structure LexRules = LexRules
    structure LexBasics = LexBasics
d39 1
a39 3
    type result = LexRules.result

    exception LexError = LexBasics.LexError
d41 13
a53 1
    (* start the file with a lot of simple functions which can probably be lifted from elsewhere *)
d55 1
a55 11
    exception Nth

    fun stringofintlist [] = ""
      | stringofintlist (n::ns) = makestring(n:int) ^ " " ^ stringofintlist ns

    fun nth 0 l = raise Nth
      | nth _ [] = raise Nth
      | nth 1 (x::xs) = x
      | nth n (x::xs) = nth (n-1) xs

    fun upto m n = if m>n then [] else m :: upto (m+1) n
d57 1
a57 1
    val chars = map chr (upto 0 255)
d59 3
a61 3
    val letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
    val digits = "0123456789"
    val printables = letters ^ digits ^ "`~!@@#$%^&*()_+-={}[]<>,./?;:'|\\\" "
d63 1
a63 5
    (* `reduce' ? *)
      
    fun forall (h::t) f = (f h;
			   forall t f)
      | forall [] f = []
d65 1
a65 14
    (* simple zip/unzip *)
	
    exception Zip
    
    fun zip ((p::ps),(q::qs)) = (p,q)::zip (ps,qs)
      | zip ([],[]) = []
      | zip _ = raise Zip
	
    fun unzip ((p,q)::xs) = let val (ps,qs) = unzip xs in (p::ps,q::qs) end
      | unzip [] = ([],[])
	
    (* returns a counting function from m+1 *)
	
    fun counter m = let val n = ref m in fn () => (n := (!n)+1; !n) end
d67 1
a67 1
    (* returns only those elements satisfying the predicate *)
d69 22
a90 48
    fun filter f [] = []
      | filter f (x::xs) = if f x then x::(filter f xs) else filter f xs
	
    (* concatenates a list of lists *)
	
    fun concat (h::t) = h@@(concat t)
      | concat [] = []
	
    (* member function on equality-type lists *)
	
    fun mem x = let fun mem' (h::t) = (x = h) orelse mem' t
		      | mem' [] = false
		in
		  mem'
		end
	      
    (* duplicate-removing function on equality-type lists *)
  
    val remove_duplicates = let
			      fun remove_duplicates' a (h::t) = if mem h a
								  then
								    remove_duplicates' a t
								else
								  remove_duplicates' (h::a) t
				| remove_duplicates' a [] = a
			    in
			      remove_duplicates' []
			    end
			  
    (* insertion-sort on int lists *)
  
    val insertion_sort = let
			   fun insert x (l as (h::t)) = if (x > h)
							  then
							    h :: (insert x t)
							else
							  x::l
			     | insert x [] = [x : int]
			       
			   fun sort' a (h::t) = sort' (insert h a) t
			     | sort' a [] = a
			 in
			   sort' []
			 end
		       
    (* nice_list returns a sorted, duplicate-free list *)
  
    val nice_list = insertion_sort o remove_duplicates
d92 1
a92 1
    (* Now for the real stuff *)
d94 1
a94 1
    datatype NdfaNode = N of ((string * int) list * int * int)
d96 3
a98 4
    (* first int is the node number, which we use to form subsets.  second
     int is the action number (there is a fixed list of actions), which we
     use to do comparisons. This is 0 if this node is non-accepting, and
     higher action numbers take precedence *)
d100 1
a100 1
    datatype Ndfa = NDFA of (NdfaNode list * int)
d102 5
a106 26
    (* the int is the starting node *)
      
    datatype DfaNode = D of int Array.array * int * int
      
    (* the first int is the node number, the second int is the action number *)
      
    datatype Dfa = DFA of int Array.array Array.array * int Array.array
      
    (* The int array array is the transitions, the int array is the
     actions. The initial state is always state 1, the unreachable state is
     always state 0 *)
      
    fun nodeno (N(_,n,_)) = n
      
    (* extracts the node numbered by the second argument from the Ndfa given by the first *)
      
    exception GetNode
    
    fun getnode (NDFA(l,s)) n = let
				  fun getnode' (h::t) = if nodeno(h) = n then h else getnode' t
				    | getnode' [] = raise GetNode
				in
				  getnode' l
				end
			      
    (* trans returns the node-numbers of all the transitions from the node on the string *)
d108 7
a114 10
    fun trans char (N(l,_,_)) = let
				  fun trans' ((s,n)::t) = if (s = char) then n::trans' t else trans' t
				    | trans' [] = [];
				in
				  trans' l
				end
			      
    (* trans_ndfa c a n returns the node-numbers of all the transitions from the node numbered n on the string c in the automaton a *)
  
    fun trans_ndfa char ndfa = (trans char) o (getnode ndfa)
d116 9
a124 4
    (* trans_subset returns the node-numbers of all the transitions from all the nodes in the subset *)
      
    fun trans_subset char ndfa subset = remove_duplicates (concat (map (trans_ndfa char ndfa) subset))
      
d127 14
a140 12
    fun epsclosure ndfa subset = let
				   fun epsclosure' [] a = a
				     | epsclosure' (n::ns) a = let
								 val new = filter (fn x => not (mem x a)) (trans_ndfa "" ndfa n)
							       in
								 epsclosure' (new@@ns) (new@@a)
							       end
				 in
				   epsclosure' subset subset
				 end
			       
    (* half-trans does a trans and then a closure *)
d142 2
a143 1
    fun half_trans ndfa subset char = nice_list (epsclosure ndfa (trans_subset char ndfa subset))
d145 5
a149 4
    (* full_trans returns the entire subset accessible with that
     transformation. half_trans is all that we actually use in the subset
     construction, since we always deal with epsilon-closed subsets so the
     initial closure is redundant *)
d151 7
a157 17
    fun full_trans ndfa subset char = half_trans ndfa (epsclosure ndfa subset) char
      
    (* returns a list of Nodes when given a list of node numbers *)
      
    fun nodes_subset ndfa = map (getnode ndfa)
      
    (* returns the highest action number from a list of node numbers. Need
     this because when we transform to DFA, this action is the one we want
     for each subset node. If none of the nodes have an action, return 0.
     *)
      
    fun best_action ndfa l = let
			       fun best_action' m (N(_,_,an)::t) = best_action' (if an > m then an else m) t
				 | best_action' m [] = m
			     in
			       ((best_action' 0) o (nodes_subset ndfa)) l
			     end
d159 98
a256 65
    (* epsilon-closure of the initial node *)
  
    fun initial (ndfa as NDFA (_,i)) = epsclosure ndfa [i]
      
    (* this function will consider the empty state (corresponding to no
     NDFA states) as distinct from other states, so we should get
     termination for free by checking for that state. Alternatively, it
     would be nice to spot that state separately so that we don't have to
     record transitions to it in the list. Alternatively too, it doesn't
     matter since this DFA is only temporary. *)
      
    fun transform (ndfa as NDFA(l,i)) = let
					  fun total (n::ns) = n + total ns
					    | total [] = 0
					  val nextnode = counter 1
					  val init = initial ndfa
					  val unmarkedstates = (ref []) : (int list * int) list ref
					  (* state zero is empty (terminating), state 1 is initial *)
					  val markedstates = (ref [D (Array.array(256,0),0,0)]) : DfaNode list ref
					  val currentstate = ref (init,1)
					  (* keep a `hash-value' (the total) with each state to speed searching *)
					  val states = ref [(total init,init,1),(0,[],0)]
					  (*  adds a state to the unmarked list and gives it a number *)
					  fun addstate l = let
							     val nodeno = nextnode ()
							     val actno = best_action ndfa l
							   in
							     (unmarkedstates := (l,nodeno) :: (!unmarkedstates);
							      states := (total l,l,nodeno) :: (!states);
							      print ("Adding state "^makestring(nodeno)^" : "^ stringofintlist l);
							      nodeno)
							   end
					  (* returns number of this state, adding if necessary *)
					  fun find l = let
							 val t = total l
							 fun find' ((t',s,n)::r) = if (t = t') andalso (s = l) then n else find' r
							   | find' [] = addstate l
						       in
							 (find' (!states))
						       end
					  (* returns the transition table: an int array *)
					  fun transtable s = Array.arrayoflist (map (find o (half_trans ndfa s)) chars)
					  (* recurses down the list of unmarked states.... *)
					  fun doit () = let
							  val (s,n)= !currentstate
							in
							  (print ("Examining state " ^ makestring n);
							   markedstates := (D(transtable s,n,best_action ndfa s)) :: (!markedstates);
							   (fn [] => () | (s::ss) => (unmarkedstates := ss;
										      currentstate := s;
										      doit ()))
							   (!unmarkedstates))
							end
					  (* I can't find a better way to format this.
					   We are applying the fnexp to !unmarkedstates *)
					  val _ = doit ()
					  val maxnode = nextnode () 
					  val trans = Array.array (maxnode,Array.array(0,0))
					  val actions = Array.array(maxnode,0)
					  fun addit (D(t,n,a)) = (Array.update(trans,n,t);
								  Array.update(actions,n,a))
					  val _ = forall (!markedstates) addit
					in
					  DFA(trans,actions)
					end
d270 34
a303 35
    fun re2ndfa RegExp.EPSILON l f c = let val i = c () in (N([("",f)],i,0) :: l,i) end
      | re2ndfa (RegExp.NODE s) l f c = (fn []      => re2ndfa RegExp.EPSILON l f c
                                          | (n::[]) => let
							 val i = c() in (N([(n,f)],i,0) :: l, i) end
					  | (n::ns) => let
							 val (l',i') = re2ndfa (RegExp.NODE (implode ns)) l f c
							 val i = c()
						       in
							 (N([(n,i')],i,0) :: l',i)
						       end) (explode s)
      | re2ndfa (RegExp.CLASS s) l f c = let val i = c() in (N(map (fn c => (c,f)) (explode s),i,0) :: l,i) end
      | re2ndfa (RegExp.BAR(s,t)) l f c = let
					    val i = c ()
					    val f' = c()
					    val (l',i') = re2ndfa s l f' c
					    val f'' = c()
					    val (l'',i'') = re2ndfa t l' f'' c
					  in
					    (N([("",i'),("",i'')],i,0) ::
					     N([("",f)],f',0) :: 
					     N([("",f)],f'',0) :: l'',i)
					  end
      | re2ndfa (RegExp.DOT(s,t)) l f c = let
					    val (l',i') = re2ndfa t l f c
					  in
					    re2ndfa s l' i' c
					  end
      | re2ndfa (RegExp.STAR s) l f c = let
					  val i = c ()
					  val f' = c ()
					  val (l',i') = re2ndfa s l f' c
					in
					  (N([("",i'),("",f)],f',0) ::
					   N([("",i'),("",f)],i,0) :: l' , i)
					end
a309 7
    fun bar re a l c = let
			 val f = c ()
			 val (l',i) = re2ndfa re l f c
		       in
			 (N([],f,a) :: l', i)
		       end
		     
d315 9
a323 7
    fun baz [] c = ([],[],1)
      | baz (re::res) c = let
			    val (nl,il,a) = baz res c
			    val (nl',i) = bar re a nl c
			  in
			    (nl',i::il,a+1)
			  end
d327 8
a334 9
    fun qux l = let
		  val (res,acts) = unzip l
		  val c = counter 0
		  val (nl,il,_) = baz res c
		  val i = c()
		  val _ = print ("NDFA has" ^ makestring i ^ "states")
		in
		  (NDFA(N(map (fn n => ("",n)) il,i,0)::nl,i),Array.arrayoflist (rev acts))
		end
d338 14
a351 3
    fun printarray printfun a = (forall (upto 0 ((Array.length a)-1)) (fn n => (printfun (Array.sub (a,n));
										NewJersey.print " "));
				 NewJersey.print "\n")
d353 10
a362 12
    fun printdfa (DFA (trans,actions)) = (print "transitions : ";
					  printarray (printarray NewJersey.print) trans;
					  print "actions : ";
					  printarray NewJersey.print actions)
		   
    fun quux l = let
		   val (n,a) = qux l
		   val dfa = transform n
		 in
		   (printdfa dfa;
		    (dfa,a : (string -> result) Array.array))
		 end
a402 2
    val lexer = lex o quux

a407 4
    val lexers = map lexer LexRules.rules

    fun mkself buffer = fn n => ((nth n lexers) buffer)
		    
d410 8
a417 5
    fun mkTokenStream f = let
			    val b = InBuffer.mkInBuffer f
			  in
			    (b,mkself b)
			  end
d419 6
a424 5
    fun getToken (b,s) = let
			   val _ = LexRules.self := s
			 in
			   s(1)
			 end
d432 1
a432 1
		       
@
